# 2022 CCS 关注内容
## 人工智能的特性
1. is your explanation stable? a robustness evaluation framework for feature attribution.<br>
*关键点*：用特征归因理解神经网络的可解释性，即某个决策和关键特征有关系。当前大部分算法旨在提升模型的忠实度。<br>
*问题*：现实环境中存在许多随机噪声，会干扰图像分类任务的特征归因；解释算法容易受到对抗性攻击，为恶意干扰的输入生成相同的解释。<br>
*方案*：提出了用于特征归因的中值检验（MeTFA），在理论上量化不确定性并提高解释算法的稳定性，适用于任何特征归因方法。<br>
*功能*：（1）检查一个特征重要或不重要，并生成显著映射以可视化结果。（2）计算特征归因分数的置信区间，并生成平滑映射以提高解释的稳定性。<br>
*优点*：（1）改善了解释的视觉质量，显著降低了不稳定性，同时保持了原始方法的忠实度。**[万金油强化]**（2）两个典型应用来展示MeTFA在应用中的潜力。**[定量评估忠实度与防御效果]**
2. AI/ML for Network Security: The Emperor has no Clothes<br>
*问题*：基于机器学习（ML）的网络流量检测方案过于黑盒，网络运营商不愿信任并将部署它们。因为这些模型容易出现欠规范化问题（**未能以足够详细的方式指定模型**），导致模型表现出意想不到的糟糕行为。<br>
*痛点*：开发可解释的ML解决方案（例如决策树），帮助解释黑盒模型的决策过程。合成高保真度又易于理解的可解释模型是具有挑战性的。<br>
*方案*：合成高保真度和低复杂度的决策树，帮助网络运营商判断他们的ML模型是否欠缺规范。Trustee将现有ML模型和训练数据集作为输入，生成高保真度、易于解释的决策树以及相关信任报告作为输出。<br>
*优点*：利用已发布的、完全可复制的ML模型，来识别模型欠规范化的三个常见实例，即快捷学习的证据、伪相关性的存在以及对分布外异常样本的脆弱性。<br>
*PS*：把人工智能的规范化作为整体的任务，利用决策树做一个模型级别的检测。作者直接把赛道提高一个维度，属于首发性工作。

## 人工智能&隐私保护
1. Are Attribute Inference Attacks Just Imputation?<br>
模型可能会暴露其训练数据中的敏感信息。在属性推断攻击中，攻击者对一些训练记录有部分了解，并可以访问基于这些记录训练的模型，从而推断这些记录中敏感特征的未知值。我们研究了一种称为敏感值推断的属性推断的细粒度变体，其中攻击者的目标是从候选集中高度自信地识别出具有特定敏感值的未知属性的某些记录。我们明确地比较了属性推断与数据插补，后者在各种关于训练数据的假设下捕获了训练分布统计数据，以及攻击者可以获得的训练数据。我们的主要结论是：（1）先前的属性推断方法并未揭示模型中关于训练数据的更多信息，这些信息可以通过无需访问训练模型但具有与训练属性推断攻击所需相同的基本分布知识的攻击者推断出来；（2）黑盒属性推断攻击很少学到无法在没有模型的情况下学到的东西；但（3）白盒攻击（我们在论文中介绍和评估），可以可靠地识别出具有敏感值属性的一些记录，而在没有访问模型的情况下不会预测这些记录。此外，我们表明，诸如差分隐私训练和从训练中删除易受攻击记录等建议的防御措施，并未减轻这种隐私风险。

