# 2022 CCS 关注内容
## 人工智能的特性
1. is your explanation stable? a robustness evaluation framework for feature attribution.<br>
*关键点*：用特征归因理解神经网络的可解释性，即某个决策和关键特征有关系。当前大部分算法旨在提升模型的忠实度。<br>
*问题*：现实环境中存在许多随机噪声，会干扰图像分类任务的特征归因；解释算法容易受到对抗性攻击，为恶意干扰的输入生成相同的解释。<br>
*方案*：提出了用于特征归因的中值检验（MeTFA），在理论上量化不确定性并提高解释算法的稳定性，适用于任何特征归因方法。<br>
*功能*：（1）检查一个特征重要或不重要，并生成显著映射以可视化结果。（2）计算特征归因分数的置信区间，并生成平滑映射以提高解释的稳定性。<br>
*优点*：（1）改善了解释的视觉质量，显著降低了不稳定性，同时保持了原始方法的忠实度。**[万金油强化]**（2）两个典型应用来展示MeTFA在应用中的潜力。**[定量评估忠实度与防御效果]**
2. AI/ML for Network Security: The Emperor has no Clothes<br>
*问题*：基于机器学习（ML）的网络流量检测方案过于黑盒，网络运营商不愿信任并将部署它们。因为这些模型容易出现欠规范化问题（**未能以足够详细的方式指定模型**），导致模型表现出意想不到的糟糕行为。<br>
*痛点*：开发可解释的ML解决方案（例如决策树），帮助解释黑盒模型的决策过程。合成高保真度又易于理解的可解释模型是具有挑战性的。<br>
*方案*：合成高保真度和低复杂度的决策树，帮助网络运营商判断他们的ML模型是否欠缺规范。Trustee将现有ML模型和训练数据集作为输入，生成高保真度、易于解释的决策树以及相关信任报告作为输出。<br>
*优点*：利用已发布的、完全可复制的ML模型，来识别模型欠规范化的三个常见实例，即快捷学习的证据、伪相关性的存在以及对分布外异常样本的脆弱性。<br>
*PS*：把人工智能的规范化作为整体的任务，利用决策树做一个模型级别的检测。作者直接把赛道提高一个维度，属于首发性工作。

## 人工智能&安全攻击
1. FenceSitter: Black-box, Content-Agnostic, and Synchronization-Free Enrollment-Phase Attacks on Speaker Recognition Systems<br>
声纹识别系统（SRS）基于语音识别为合法用户提供访问权限。最近的研究表明，SRS 可以在训练阶段（后门攻击）和识别阶段（规避攻击）被绕过。在本文中，我们通过提出一种命名为FenceSitter的注册阶段攻击范式，探讨了SRS的新攻击面。在此范式中，当合法用户注册到SRS时，攻击者使用不可察觉的对抗性环境声音对SRS进行投毒。SRS提取的被污染的语音识别允许攻击者和合法用户在所有未来的识别阶段访问系统。为了实现这种攻击，我们将精心设计的连续对抗扰动插入到无辜的环境声音中。由于在长序列环境声音载体上计算对抗扰动是难以处理的，我们通过内容脱敏和物理实现来优化对抗性片段。此外，通过基于自然演化策略的梯度估计，使攻击在黑盒设置下可用。我们在英文和中文语音数据集上进行了大量实验，包括密集识别（CSI）、开放识别（OSI）和说话者验证（SV）任务。在各种数字和物理条件下的结果验证了FenceSitter的有效性和鲁棒性。通过现场注册实验和用户研究，我们进一步验证了FenceSitter的实用性。我们的工作揭示了SRS在注册阶段的安全漏洞，这可能会激发未来研究如何提高SRS的安全性。

2. Harnessing Perceptual Adversarial Patches for Crowd Counting<br>
人群计数已广泛应用于安全关键场景中估计人数，但在物理世界中（如对抗性贴片）容易受到对抗性示例的影响。尽管有害，对抗性示例也有助于评估和更好地理解模型的鲁棒性。然而，现有的人群计数对抗性示例生成方法在不同的黑盒模型之间缺乏强烈的可迁移性，限制了它们在现实世界系统中的实用性。受到攻击可迁移性与模型不变特征正相关的事实的启发，本文提出了一种基于模型共享感知特征定制人群计数场景对抗性扰动的感知对抗性贴片（PAP）生成框架。具体而言，我们手工设计一种自适应的人群密度加权方法，以捕获不同模型间的不变尺度感知特征，并利用密度引导注意力捕获模型共享的位置感知。它们都被证明可以提高我们的对抗性贴片的攻击可迁移性。大量实验表明，我们的PAP在数字和物理世界中均能达到最先进的攻击性能，并大幅超过先前的建议（最多+685.7 MAE和+699.5 MSE）。此外，我们通过实证研究证明，使用我们的PAP进行对抗性训练可以提高原始模型在缓解人群计数场景中的若干实际挑战方面的性能，包括数据集间的泛化能力（最多-376.0 MAE和-354.9 MSE）以及对复杂背景的鲁棒性（最多-10.3 MAE和-16.4 MSE）。

3. Identifying a Training-Set Attack's Target Using Renormalized Influence Estimation<br>
针对性训练集攻击通过向训练集注入恶意样本，使得训练后的模型对一个或多个特定测试样本进行错误标记。本研究提出了目标识别任务，以判断特定测试样本是否为训练集攻击的目标。目标识别可以与对抗性实例识别结合，找到（并移除）攻击实例，减轻攻击对其他预测的影响。我们不是关注单一的攻击方法或数据模态，而是基于影响力估计，量化每个训练实例对模型预测的贡献。我们证明了现有影响力估计器在实际表现不佳的原因通常是由于过度依赖训练实例和具有较大损失的迭代。我们的重标准化影响力估计器修复了这个弱点；它们在识别对抗性和非对抗性设置中的有影响力的训练样本群体方面表现远超原始估计器，甚至在没有干净数据误报的情况下找到高达100%的对抗性训练实例。目标识别任务则简化为检测具有异常影响力值的测试实例。我们在文本、视觉和语音等各个数据领域以及针对专门优化对抗性实例以规避我们方法的灰盒自适应攻击者的情况下，展示了我们方法在后门和投毒攻击中的有效性。

4. LoneNeuron: A Highly-Effective Feature-Domain Neural Trojan Using Invisible and Polymorphic Watermarks<br>
深度神经网络（DNNs）在现实世界应用中的广泛采用引发了越来越多的安全担忧。潜藏在预训练神经网络中的神经木马是针对DNN模型供应链的有害攻击。当某些隐蔽的触发器出现在输入中时，它们会生成错误的输出。尽管数据中毒攻击在文献中已被广泛研究，但代码中毒和模型中毒的后门直到最近才开始引起关注。我们提出了一种新颖的模型中毒神经木马，名为LoneNeuron，它对特征域模式作出反应，这些模式会转化为隐形的、样本特异性的和多态性的像素域水印。具有高攻击特异性的LoneNeuron实现了100%的攻击成功率，同时不影响主任务性能。通过LoneNeuron独特的水印多态性属性，同一特征域触发器在像素域中被解析为多个水印，这进一步提高了水印的随机性、隐蔽性和抵抗木马检测的能力。大量实验表明，LoneNeuron可以逃过最先进的木马检测器。LoneNeuron还是针对视觉变换器（ViTs）的第一种有效的后门攻击。

5. Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models<br>
神经文本排序模型取得了显著的进步，并越来越多地应用于实践中。然而，它们也继承了一般神经模型的对抗性漏洞，这些漏洞在先前的研究中已被发现但仍未被充分探讨。此外，这些固有的对抗性漏洞可能会被黑帽搜索引擎优化（SEO）利用，以击败更好地保护搜索引擎。在本研究中，我们提出了一种针对黑盒神经通道排序模型的模仿对抗攻击。首先，我们展示目标通道排序模型可以通过枚举关键查询/候选项来透明化和模仿，然后训练一个排序模仿模型。利用排序模仿模型，我们可以精心操纵排序结果，并将操纵攻击转移到目标排序模型。为此，我们提出了一种创新的基于梯度的攻击方法，通过成对的目标函数生成对抗性触发器，这些触发器只需要很少的令牌就能引起预谋的混乱。为了使触发器伪装，我们将下一句预测损失和语言模型流利性约束添加到目标函数中。通道排序的实验结果证明了排序模仿攻击模型和对抗触发器对各种SOTA神经排序模型的有效性。此外，各种缓解分析和人类评价显示了在面对潜在的缓解方法时伪装的有效性。为了激励其他学者进一步研究这个新颖且重要的问题，我们将实验数据和代码公开提供。

6. Perception-Aware Attack: Creating Adversarial Music via Reverse-Engineering Human Perception<br>
以往的对抗性音频攻击主要集中在通过在原始信号上添加微小的噪声样扰动来确保攻击音频信号分类器的有效性。目前尚不清楚攻击者是否能够创建既能被人类感知到的又具有攻击效果的音频信号扰动。在这项工作中，我们将针对音乐信号的对抗性攻击制定为一种新的感知感知攻击框架，将人类研究整合到对抗性攻击设计中。具体而言，我们邀请人类参与者根据原始和扰动音乐信号对的配对来评估他们感知到的偏差，并通过回归分析对人类感知过程进行反向工程，以预测给定扰动信号的人类感知偏差。然后将感知感知攻击制定为一个优化问题，该问题通过最小化来自回归人类感知模型的感知偏差预测来找到最佳扰动信号。实验结果表明，与之前针对YouTube版权检测器的工作相比，该攻击产生的对抗性音乐具有明显更好的感知质量。


## 人工智能&隐私保护
1. Are Attribute Inference Attacks Just Imputation?<br>
*问题*：模型会暴露其训练数据中的敏感信息。在属性推断攻击中，攻击者可以接触某些训练数据的一部分并访问训练后的模型，从而推断这些记录中敏感特征的未知值。<br>
*方案*：研究了一种属性推断的细粒度变体，敏感值推断。攻击者的目标是从候选集中识别出[具有特定敏感值的未知属性的]某些记录。此方案明确地比较了属性推断与数据插补，后者在各种关于训练数据的假设下捕获了训练的统计分布。<br>
*结论*：（1）先前的属性推断方法并未揭示模型中关于训练数据的更多信息。攻击者甚至不需要访问模型，与训练属性推理攻击所需的底层分布知识相同即可。（2）没有模型就学不到的东西，黑盒推理攻击很少学到。（3）论文中的白盒攻击可以识别出具有敏感值属性的一些记录[没有访问模型的情况下不会预测这些记录]。<br>
*PS*：差分隐私训练和删除易受攻击的记录等防御措施，并未减轻这种隐私风险。作者提出了属性推理的一种变体，探究此攻击的一些性质。

2. Auditing Membership Leakages of Multi-Exit Networks<br>
*问题*：并非所有输入都需要相同的计算成本来产生可靠的预测，多出口网络正逐渐受到关注。主干模型具有早期退出的能力，允许在模型的中间层进行预测。然而，目前的各种设计主要考虑资源使用效率和预测准确性两个方面，尚未探讨由此产生的隐私风险。<br>
*方案*：首次从成员泄露的角度分析了多出口网络的隐私问题。首先，利用现有的攻击方法来量化多出口网络在成员泄露方面的脆弱性。然后，提出了一种混合攻击，利用出口信息来提高现有攻击的性能。<br>
*结论*：多出口网络在成员泄露方面的脆弱性较低，附加在主干模型上的退出（数量和深度）与攻击性能高度相关。在三种不同对手设置下，评估了混合攻击引起的成员泄露威胁，得到一个无模型和无数据的敌手。最终，考虑了一种专门针对多出口网络的防御机制 TimeGuard。<br>
*PS*：完整地考虑某个威胁问题，考虑如何改进[强调威胁]，寻找解决方案[比较完整的研究]。

3. Characterizing and Detecting Non-Consensual Photo Sharing on Social Networks<br>
*问题*：在社交平台上，存在着巨大的视觉侵犯和照片滥用问题。用户可能会在不知情的情况下被拍摄并曝光[非共识性分享问题]，无法通过主动访问控制或旁观者检测来解决。<br>
*方案*：（1）提出了 Videre，识别并警告在不知情的情况下被拍摄的影像。首先，通过深入的用户研究阐述非共识性拍摄照片中遇到的主要特征。然后，以此为背景建立一个数据集，并[基于多深度特征融合]构建了一个主动检测器。（2）为了减轻逐个识别不知情者的开销，设计了一种基于签名的本地预授权过滤器，也可以隐含地避免分类错误。<br>
*PS*：实际的隐私问题，重点在于理论建模。

4. Enhanced Membership Inference Attacks against Machine Learning Models<br>
机器学习算法泄露了多少关于其训练数据的信息，以及为什么会泄露？会员推断攻击作为量化这种泄漏的审计工具。在本文中，我们提出了一个全面的假设检验框架，使我们不仅能够以一致的方式正式表达先前的工作，还能设计新的使用参考模型的会员推断攻击，以实现显著更高的功效（真阳性率），针对任何误报率。更重要的是，我们解释了不同攻击为何表现不同。我们提供了一种无法区分游戏的模板，并解释了攻击成功率在游戏不同实例中的解释。我们讨论了攻击者在问题表述中产生的各种不确定性，并展示了我们的方法如何尝试将攻击不确定性最小化为关于数据点在训练集中存在或不存在的一位秘密。我们对所有类型的攻击进行了差异分析，解释了它们之间的差距，并展示了什么原因导致数据点容易受到攻击（因为不同粒度的记忆原因不同，从过拟合到条件记忆）。我们的审计框架作为Privacy Meter软件工具的开放部分。

5. Feature Inference Attack on Shapley Values<br>
作为合作博弈理论中的解决方案概念，Shapley值在模型可解释性研究中得到了高度认可，并被领先的机器学习即服务（MLaaS）提供商，如谷歌、微软和IBM广泛采用。然而，尽管Shapley值为基础的模型可解释性方法已经被深入研究，但很少有研究人员考虑到Shapley值带来的隐私风险，尽管可解释性和隐私是机器学习（ML）模型的两个基石。在本文中，我们使用特征推断攻击研究基于Shapley值的模型可解释性方法的隐私风险：根据它们的Shapley值解释重建私有模型输入。具体而言，我们提出了两个对手。第一个对手可以通过在辅助数据集上训练一个攻击模型并进行黑箱访问模型可解释性服务来重建私有输入。第二个对手，即使没有任何背景知识，也可以通过利用模型输入和输出之间的局部线性相关性成功地重建大部分私有特征。我们在领先的MLaaS平台上执行了所提出的攻击，即谷歌云、微软Azure和IBM aix360。实验结果表明，领先MLaaS平台中使用的最先进的基于Shapley值的模型可解释性方法的脆弱性，并强调了在未来研究中设计隐私保护模型可解释性方法的重要性和必要性。据我们所知，这也是第一个研究Shapley值隐私风险的工作。

6. Graph Unlearning<br>
机器遗忘是一种在收到移除请求后从机器学习（ML）模型中移除某些训练数据影响的过程。虽然直接且合法，但从头开始重新训练 ML 模型会产生很高的计算开销。为了解决这个问题，在图像和文本数据领域提出了许多近似算法，其中 SISA 是最先进的解决方案。它随机将训练集分割成多个片段，并为每个片段训练一个组成模型。然而，直接将 SISA 应用于图数据可能会严重破坏图结构信息，从而降低所得 ML 模型的效用。在本文中，我们提出了 GraphEraser，一种针对图数据量身定制的新型机器遗忘框架。其贡献包括两种新颖的图分割算法和一种基于学习的聚合方法。我们在五个真实世界的图数据集上进行了广泛的实验，以说明 GraphEraser 的遗忘效率和模型效用。它实现了 2.06 倍（小数据集）到 35.94 倍（大数据集）的遗忘时间改进。另一方面，GraphEraser 达到了最高 62.5% 更高的 F1 分数，我们提出的基于学习的聚合方法达到了最高 112% 更高的 F1 分数。

7. Group Property Inference Attacks Against Graph Neural Networks<br>
最近的研究表明，机器学习（ML）模型容易受到侵犯隐私的攻击，这些攻击会泄露有关训练数据的信息。在这项工作中，我们将图神经网络（GNN）作为目标模型，并关注一种特定类型的隐私攻击，即属性推理攻击（PIA），该攻击通过访问GNN来推断训练图中的敏感属性。尽管现有的研究已经研究了针对图级属性（例如，节点度和图密度）的PIA，但我们是首个对群组属性推理攻击（GPIA）进行系统研究的，GPIA推断训练图中特定节点和链接组的分布（例如，男性节点之间的链接比女性节点之间的链接更多）。首先，我们考虑了一个威胁模型分类，并为这些设置设计了六种不同的攻击。其次，我们通过对三个具有代表性的GNN模型和三个真实世界图进行大量实验来证明这些攻击的有效性。第三，我们分析了导致GPIA成功的基本因素，并表明，训练有或没有目标属性的图上的GNN模型在模型参数和/或模型输出上存在一定的差异，这使得攻击者能够推断出属性的存在。此外，我们设计了一套针对GPIA攻击的防御机制，并通过实证表明，这些机制可以在保证GNN模型准确度的前提下有效降低攻击准确度。

8. L-SRR: Local Differential Privacy for Location-Based Services with Staircase Randomized Response<br>
基于位置的服务（LBS）在移动设备中得到了显著的发展和广泛的应用。众所周知，LBS应用可能通过收集敏感位置信息导致严重的隐私问题。一种强大的隐私模型“本地差分隐私”（LDP）近年来已被部署在许多不同的应用中（例如，谷歌RAPPOR，iOS和微软遥测），但由于现有LDP机制的低效用，它并不适用于LBS应用。为了解决这一不足，我们提出了第一个针对各种基于位置的服务的LDP框架（即“L-SRR”），该框架可以私密地收集和分析具有高效用的用户位置。具体来说，我们设计了一种名为“阶梯随机响应”（SRR）的新型随机化机制，并扩展了实证估计，以显著提高SRR在不同LBS应用中的效用（例如，交通密度估计和k-最近邻）。我们对四个真实LBS数据集进行了大量实验，并在实际应用中与其他LDP方案进行了基准测试。实验结果表明，L-SRR的性能显著优于其他方案。

9. LPGNet: Link Private Graph Networks for Node Classification<br>
分类任务在标记的图结构数据上有很多重要的应用，范围从社交推荐到金融建模。深度神经网络越来越多地用于图上的节点分类，其中具有相似特征的节点需要被赋予相同的标签。图卷积网络（GCN）是一种在这个任务上表现良好的广泛研究的神经网络结构。然而，最近针对GCN的强大的链接窃取攻击表明，即使只有黑盒访问已训练模型，推断训练图中存在哪些链接（或边）也是实际的。在本文中，我们提出了一种名为LPGNet的新神经网络结构，用于在具有隐私敏感边的图上进行训练。LPGNet通过在训练过程中使用一种新颖的图边结构设计，为边提供差分隐私（DP）保证。我们凭经验证明，LPGNet模型通常处于提供隐私和效用之间的最佳平衡点：它们比不使用边信息的“平凡”私有结构（例如，普通MLP）提供更好的效用，并且比使用完整边结构的普通GCN在抵抗现有链接窃取攻击方面表现更好。在我们评估的大多数数据集中，LPGNet还比DPGCN提供更好的隐私-效用权衡，而DPGCN是将差分隐私整合到传统GCN中的最先进的机制。

10. Membership Inference Attacks and Generalization: A Causal Perspective<br>
会员推断（MI）攻击揭示了当前神经网络随机训练方法中的隐私弱点。然而，人们并不十分了解它们为何会出现。它们仅仅是不完美泛化的自然结果吗？在训练过程中，我们应该解决哪些基本原因来减轻这些攻击？为了回答这些问题，我们提出了第一个基于原则性因果推理来解释MI攻击及其与泛化之间关系的方法。我们提供了能量化解释6种攻击变体所达到的MI攻击性能的因果图。我们反驳了一些先前的非定量假设，这些假设过于简化或高估了潜在原因的影响，因此无法捕捉到多个因素之间的复杂相互作用。我们的因果模型还通过它们共享的因果因素展示了泛化与MI攻击之间的新联系。我们的因果模型具有很高的预测能力（0.90），即它们的分析预测经常与未见实验中的观察结果相匹配，这使得通过它们进行分析成为了一种实用的替代方法。

11. Membership Inference Attacks by Exploiting Loss Trajectory<br>
机器学习模型容易受到成员推断攻击的影响，攻击者的目标是预测某个特定样本是否包含在目标模型的训练数据集中。现有的攻击方法通常仅利用来自给定目标模型的输出信息（主要是损失）。因此，在实际场景中，成员和非成员样本产生相似的小损失时，这些方法自然无法区分它们。为了解决这一限制，本文提出了一种新的攻击方法，名为TrajectoryMIA，它可以利用目标模型整个训练过程中的成员信息来提高攻击性能。为了在常见的黑盒设置中实施攻击，我们利用知识蒸馏，并通过在不同蒸馏时代的一系列中间模型上评估的损失来表示成员信息，即蒸馏损失轨迹，以及来自给定目标模型的损失。不同数据集和模型架构的实验结果表明，我们的攻击在不同指标方面具有很大优势。例如，在CINIC-10上，我们的攻击在0.1%的低假阳性率下实现了比现有方法至少高6倍的真阳性率。进一步分析表明，我们的攻击在更严格的场景中具有普遍有效性。

## 联邦学习
1. Cerberus: Exploring Federated Prediction of Security Events<br>
*问题*：对抗网络攻击的防御手段越来越依赖于主动方法，根据过去的事件来预测对手的下一步行动。构建准确的预测模型需要很多组织的知识，包括网络结构、安全态势和政策。<br>
*方案*：使用联邦学习来预测未来安全事件，即Cerberus系统，采用循环神经网络（RNN）来实现协作训练。在一个入侵防御产品数据集上实例化Cerberus，并从性能、鲁棒性和隐私方面进行评估，同时考虑贡献分配机制。<br>
*结论*：本工作揭示了预测安全任务中联邦学习的积极作用与挑战。<br>
*PS*：emmm，感觉有点水，是一个联邦学习在安全领域的具体应用，可能确实落地到了现实的痛点领域。

2. DPIS: An Enhanced Mechanism for Differentially Private SGD with Importance Sampling<br>
如今，差分隐私（DP）已成为隐私保护的广泛认可标准，而深度神经网络（DNN）在机器学习领域取得了巨大成功。将这两种技术相结合，即具有差分隐私的深度学习，有望保护隐私地发布使用敏感数据（如医疗记录）训练的高效用模型。为此，经典的机制是 DP-SGD，它是一种用于 DNN 训练的差分隐私版本的随机梯度下降（SGD）优化器。随后的方法改进了模型训练过程的各个方面，包括噪声衰减时间表、模型架构、特征工程和超参数调整。然而，自原始 DP-SGD 算法以来，SGD 优化器中强制执行 DP 的核心机制一直未发生变化，这越来越成为限制 DP 合规机器学习解决方案性能的基本障碍。受此启发，我们提出了 DPIS，一种新颖的差分隐私SGD训练机制，可作为 DP-SGD 核心优化器的替代品，与后者相比具有一致且显著的准确性提升。主要思想是在每次 SGD 迭代中，使用重要性采样（IS）进行小批量选择，从而减少采样方差和满足 DP 所需注入梯度的随机噪声量。尽管在非隐私设置中，使用 IS 的 SGD 在机器学习文献中已被广泛研究，但将 IS 集成到 DP-SGD 的复杂数学机制中是非常重要的；此外，IS 还涉及到额外的私有数据发布，这些数据必须在差分隐私下受到保护，以及计算密集型梯度计算。DPIS 通过新颖的机制设计、细粒度的隐私分析、效率增强和自适应梯度裁剪优化来应对这些挑战。在四个基准数据集（即 MNIST、FMNIST、CIFAR-10 和 IMDb）上进行的大量实验，涉及卷积神经网络和循环神经网络，证明了 DPIS 在具有差分隐私的深度学习方面优于现有解决方案的有效性。

3. EIFFeL: Ensuring Integrity for Federated Learning<br>
联邦学习（FL）使客户端能够与服务器协作训练机器学习模型。为了确保隐私，服务器对来自客户端的更新进行安全聚合。不幸的是，由于更新被掩盖，这使得无法验证更新的完整性。因此，可以在不被检测的情况下注入旨在毒化模型的畸形更新。在本文中，我们将联邦学习中更新隐私和完整性问题进行形式化，并提出了一种新的系统，EIFFeL，它能够实现已验证更新的安全聚合。EIFFeL是一个通用框架，可以实施任意完整性检查，并从聚合中移除畸形更新，同时不违反隐私。我们的实证评估证明了EIFFeL的实用性。例如，在有100个客户端和10%毒化的情况下，每次迭代仅需2.4秒，EIFFeL就可以训练出与非毒化联邦学习器相同精度的MNIST分类模型。

4. Eluding Secure Aggregation in Federated Learning via Model Inconsistency<br>
安全聚合是一种加密协议，可以安全地计算其输入的聚合。在联邦学习中保护模型更新的隐私方面，它具有关键作用。实际上，使用安全聚合可以防止服务器了解用户提供的个体模型更新的值和来源，从而阻碍推理和数据归属攻击。在这项工作中，我们展示了恶意服务器可以轻松地规避安全聚合，就好像后者不存在一样。我们设计了两种不同的攻击方法，能够推断出关于个体私有训练数据集的信息，而不依赖于参与安全聚合的用户数量。这使得它们成为大规模、实际联邦学习应用中具体的威胁。这些攻击是通用的，无论使用哪种安全聚合协议，效果都是一样的。它们利用了联邦学习协议的一个漏洞，该漏洞是由于安全聚合的不正确使用和参数验证不足造成的。我们的工作表明，当前使用安全聚合的联邦学习实现仅提供一种“虚假的安全感”。

5. Federated Boosted Decision Trees with Differential Privacy<br>
针对可在分布式数据上进行训练的可扩展、安全、高效的隐私保护机器学习模型，需求巨大。尽管深度学习模型通常在集中式非安全环境中取得最佳效果，但在受到隐私和通信约束的情况下，不同的模型可能表现更好。因此，基于树的方法（如XGBoost）因其高性能和易用性而受到了广泛关注；特别是在表格数据上，它们通常能达到最先进的结果。因此，最近的一些研究关注于将像XGBoost这样的梯度提升决策树（GBDT）模型，通过同态加密（HE）和安全多方计算（MPC）等加密机制，转化为联邦设置。然而，这些方法并不总是提供正式的隐私保证，或者考虑全部超参数和实现设置。在这项工作中，我们在差分隐私（DP）下实现了GBDT模型。我们提出了一个捕捉并扩展现有差分隐私决策树方法的通用框架。我们的方法框架针对联邦环境进行了定制，我们展示了通过谨慎选择技术，可以在保持高度隐私的同时实现非常高的效用。


