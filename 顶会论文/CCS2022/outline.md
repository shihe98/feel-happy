# 2022 CCS 关注内容
## 人工智能的特性
1. is your explanation stable? a robustness evaluation framework for feature attribution.<br>
*关键点*：用特征归因理解神经网络的可解释性，即某个决策和关键特征有关系。当前大部分算法旨在提升模型的忠实度。<br>
*问题*：现实环境中存在许多随机噪声，会干扰图像分类任务的特征归因；解释算法容易受到对抗性攻击，为恶意干扰的输入生成相同的解释。<br>
*方案*：提出了用于特征归因的中值检验（MeTFA），在理论上量化不确定性并提高解释算法的稳定性，适用于任何特征归因方法。<br>
*功能*：（1）检查一个特征重要或不重要，并生成显著映射以可视化结果。（2）计算特征归因分数的置信区间，并生成平滑映射以提高解释的稳定性。<br>
*优点*：（1）改善了解释的视觉质量，显著降低了不稳定性，同时保持了原始方法的忠实度。**[万金油强化]**（2）两个典型应用来展示MeTFA在应用中的潜力。**[定量评估忠实度与防御效果]**
2. AI/ML for Network Security: The Emperor has no Clothes<br>
*问题*：基于机器学习（ML）的网络流量检测方案过于黑盒，网络运营商不愿信任并将部署它们。因为这些模型容易出现欠规范化问题（**未能以足够详细的方式指定模型**），导致模型表现出意想不到的糟糕行为。<br>
*痛点*：开发可解释的ML解决方案（例如决策树），帮助解释黑盒模型的决策过程。合成高保真度又易于理解的可解释模型是具有挑战性的。<br>
*方案*：合成高保真度和低复杂度的决策树，帮助网络运营商判断他们的ML模型是否欠缺规范。Trustee将现有ML模型和训练数据集作为输入，生成高保真度、易于解释的决策树以及相关信任报告作为输出。<br>
*优点*：利用已发布的、完全可复制的ML模型，来识别模型欠规范化的三个常见实例，即快捷学习的证据、伪相关性的存在以及对分布外异常样本的脆弱性。<br>
*PS*：把人工智能的规范化作为整体的任务，利用决策树做一个模型级别的检测。作者直接把赛道提高一个维度，属于首发性工作。

## 人工智能&安全攻击
1. FenceSitter: Black-box, Content-Agnostic, and Synchronization-Free Enrollment-Phase Attacks on Speaker Recognition Systems<br>
*问题*：声纹识别系统为合法用户提供访问权限，可以在训练阶段（后门攻击）和识别阶段（规避攻击）被绕过。本文探讨了其新攻击面，即注册阶段攻击范式。<br>
*方案*：当合法用户注册到声纹识别系统时，攻击者使用不可察觉的对抗性环境声音对此系统进行投毒。本文将连续对抗扰动插入到无关的环境声音中，并通过内容脱敏和物理实现来优化对抗性片段。**[长序列环境声音载体上计算对抗扰动难以处理]** 此外，通过基于自然演化策略的梯度估计，使攻击在黑盒设置下可用。<br>
*优点*：在英文和中文语音数据集上进行了大量实验，包括密集识别（CSI）、开放识别（OSI）和说话者验证（SV）任务。[有效性、鲁棒性与实用性]<br>
*PS*：声音识别系统在注册阶段的安全性【投毒攻击】，属于首发性工作。<br>

2. Harnessing Perceptual Adversarial Patches for Crowd Counting<br>
*问题*：人群计数在物理世界中（如对抗性贴片）容易受到对抗性示例的影响。然而，现有针对人群计数的对抗性示例生成方法在不同的黑盒模型之间缺乏可迁移性，限制了它们在现实世界的实用性。<br>
*方案*：基于攻击可迁移性与模型不变特征正相关的事实，本文提出了一种基于模型共享感知特征定制人群计数场景对抗性扰动的感知对抗性贴片（PAP）生成框架。<br>
*细节*：本文手工设计一种自适应的人群密度加权方法，可以捕获不同模型间的不变尺度感知特征，并利用密度引导注意力捕获模型共享的位置感知。【增强对抗性贴片的攻击可迁移性】<br>
*优点*：（1）在数字和物理世界中均能达到最先进的攻击性能。（2）进行对抗性训练可以提高原始模型在缓解人群计数场景中的若干实际挑战方面的性能，包括数据集间的泛化能力以及对复杂背景的鲁棒性。<br>
*PS*：实际问题下的对抗样本攻击，泛化攻击效果。

3. Identifying a Training-Set Attack's Target Using Renormalized Influence Estimation<br>
*问题*：向训练集注入恶意样本，使得训练后的模型对一个或多个特定测试样本进行错误标记。本文提出目标识别任务，以判断特定测试样本是否为训练集攻击的目标。【目标识别可以与对抗性实例识别结合】<br>
*方案*：本文不是关注单一的攻击方法或数据模态，而是基于影响力估计，量化每个训练实例对模型预测的贡献。<br>
*难点*：现有影响力估计器在实际表现不佳的原因是由于过度依赖训练实例和具有较大损失的迭代。本文采用重标准化影响力估计器修复了这个弱点。<br>
*优点*：识别对抗性和非对抗性设置中的有影响力的训练样本群体方面表现远超原始估计器。【文本、视觉和语音/专门优化对抗性实例/后门和投毒攻击】<br>
*PS*：目标识别任务则简化为检测具有异常影响力值的测试实例，针对数据集的过滤。

4. LoneNeuron: A Highly-Effective Feature-Domain Neural Trojan Using Invisible and Polymorphic Watermarks<br>
*问题*：潜藏在预训练神经网络中的木马是针对DNN模型供应链的有害攻击。代码中毒和模型中毒的后门直到最近才开始引起关注。
*方案*：本文提出了一种新颖的模型中毒神经木马，对特征域模式作出反应。这些模式会转化为隐形的、样本特异性的和多态性的像素域水印。
*优点*：通过LoneNeuron独特的水印多态性属性，同一特征域触发器在像素域中被解析为多个水印，进一步提高了水印的随机性、隐蔽性和抵抗木马检测的能力。
*PS*：可以逃过最先进的木马检测器，且是针对视觉变换器（ViTs）的第一种有效的后门攻击。

5. Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models<br>
*问题*：神经文本排序模型存在对抗性漏洞。它们已被发现但仍未被充分探讨。<br>
*方案*：提出了一种针对黑盒神经通道排序模型的模仿对抗攻击。**首先**，目标通道排序模型可以通过枚举关键查询/候选项来透明化和模仿，训练一个排序模仿模型。**然后**，利用排序模仿模型，进而精心操纵排序结果，将操纵攻击转移到目标排序模型。**同时**，提出了一种基于梯度的攻击方法，通过成对的目标函数生成对抗性触发器，这些触发器只需要很少的令牌就能引起预谋的错乱。【为了使触发器伪装，将下一句预测损失和语言模型流利性约束添加到目标函数】<br>
*优点*：排序模仿攻击模型和对抗触发器对各种SOTA神经排序模型均有效。<br>
**PS**：很新的问题，将对抗性攻击聚焦于在神经网络在排序模型上的应用。<br>

6. Perception-Aware Attack: Creating Adversarial Music via Reverse-Engineering Human Perception<br>
以往的对抗性音频攻击主要集中在通过在原始信号上添加微小的噪声样扰动来确保攻击音频信号分类器的有效性。目前尚不清楚攻击者是否能够创建既能被人类感知到的又具有攻击效果的音频信号扰动。在这项工作中，我们将针对音乐信号的对抗性攻击制定为一种新的感知感知攻击框架，将人类研究整合到对抗性攻击设计中。具体而言，我们邀请人类参与者根据原始和扰动音乐信号对的配对来评估他们感知到的偏差，并通过回归分析对人类感知过程进行反向工程，以预测给定扰动信号的人类感知偏差。然后将感知感知攻击制定为一个优化问题，该问题通过最小化来自回归人类感知模型的感知偏差预测来找到最佳扰动信号。实验结果表明，与之前针对YouTube版权检测器的工作相比，该攻击产生的对抗性音乐具有明显更好的感知质量。

7. Physical Hijacking Attacks against Object Trackers<br>
现代自主系统在视觉感知流程中依赖于物体检测和物体跟踪。尽管许多最近的研究针对自动驾驶车辆的物体检测组件进行了攻击，但这些攻击在完整的流程中，将物体跟踪与物体检测相结合以提高物体检测器的准确性时，却无法发挥作用。同时，现有针对物体跟踪的攻击方法要么缺乏现实应用性，要么无法对抗强大的一类物体跟踪器，即暹罗跟踪器。在本文中，我们提出了AttrackZone，一种新的可实现物理攻击的暹罗跟踪器劫持攻击方法，该方法系统地确定环境中可用于物理扰动的有效区域。AttrackZone利用暹罗区域提议网络生成热图的过程，以便控制物体的边界框，从而导致包括车辆碰撞和行人潜入未授权区域的物理后果。在数字和物理领域的评估表明，AttrackZone平均只需0.3-3秒即可实现其92％的攻击目标。

8. Post-breach Recovery: Protection against White-box Adversarial Examples for Leaked DNN Models<br>
服务器安全漏洞在当今的互联网上是一种不幸的现实。在深度神经网络（DNN）模型的背景下，它们尤为有害，因为泄露的模型会给攻击者提供"白盒"访问权限来生成对抗性样本，而这种威胁模型没有实用的强大防御手段。对于那些在专有DNN（如医学影像）上投入了数年时间和数百万资金的实践者来说，这似乎是一场不可避免的灾难。在本文中，我们考虑了DNN模型在遭受安全漏洞后的恢复问题。我们提出了一种名为Neo的新系统，它可以创建泄露模型的新版本，并在推断时使用一个过滤器来检测和移除基于之前泄露模型生成的对抗性样本。不同模型版本的分类表面稍有偏移（通过引入隐藏分布），Neo可以检测到攻击对泄露模型的过度拟合。我们证明了，在各种任务和攻击方法中，Neo能够以非常高的准确性过滤掉泄露模型的攻击，并对反复攻击服务器的攻击者提供强大的保护（7-10次恢复）。Neo在应对各种强烈适应性攻击方面表现良好，可恢复的安全漏洞数量略有下降，并展示了作为DNN防御措施补充的潜力。

9. SPECPATCH: Human-In-The-Loop Adversarial Audio Spectrogram Patch Attack on Speech Recognition<br>
在本文中，我们提出了 SpecPatch，一种将人类置于循环中的自动语音识别（ASR）系统的对抗性音频攻击。现有的音频对抗攻击者假设用户无法注意到对抗性音频，从而允许成功传递定制的对抗性示例或扰动。然而，在实际攻击场景中，智能语音控制系统（例如智能手表、智能扬声器、智能手机）的用户对可疑声音保持警惕，尤其是在传递语音命令时。一旦用户被可疑音频提醒，他们会试图通过中断对抗音频并发出更强大的语音命令来覆盖恶意语音来纠正错误识别的命令。这使得现有的攻击在用户交互与对抗性音频传递同时发生的典型场景下无效。为了真正实现不可察觉且稳定的对抗攻击，并处理可能出现的用户中断，我们设计了 SpecPatch，一种实用的语音攻击，该攻击使用不到一秒的音频补丁信号传递攻击命令，并利用周期性噪声破坏用户与 ASR 系统之间的通信。我们分析了 CTC（连接时序分类）损失的前向和反向过程，并利用 CTC 的弱点实现我们的攻击目标。与现有攻击相比，我们将攻击影响长度（即攻击目标命令的长度）扩展了 287%。此外，我们证明了在用户干预下，我们的攻击在超线和空中场景下均实现了 100% 的成功率。

10. SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders<br>
自监督学习是一种新兴的机器学习（ML）范式。与利用高质量标记数据集的监督学习相比，自监督学习依赖于未标记数据集来预训练强大的编码器，然后将其作为各种下游任务的特征提取器。大量的数据和计算资源消耗使编码器本身成为模型所有者的宝贵知识产权。最近的研究表明，ML模型的版权受到模型窃取攻击的威胁，这种攻击旨在训练一个替代模型来模仿给定模型的行为。我们通过实证研究表明，预训练编码器对模型窃取攻击非常脆弱。然而，目前大部分关于版权保护算法的工作，如水印技术，主要集中在分类器上。与此同时，预训练编码器版权保护的内在挑战尚未得到充分研究。我们通过提出SSLGuard，首个针对预训练编码器的水印方案，填补了这一空白。给定一个干净的预训练编码器，SSLGuard将水印注入其中并输出一个带有水印的版本。该方案还应用了阴影训练技术，以在潜在的模型窃取攻击下保留水印。我们的广泛评估表明，SSLGuard在水印注入和验证方面非常有效，且能抵抗模型窃取和其他水印移除攻击，如输入噪声、输出扰动、覆写、模型剪枝和微调。

11. StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning<br>
预训练编码器是通用特征提取器，可用于许多下游任务。最近在自监督学习方面的进展可以使用大量无标签数据预训练高效的编码器，从而导致了编码器作为服务（EaaS）的兴起。预训练编码器可能被认为是机密的，因为其训练通常需要大量的数据和计算资源，同时其公开发布可能促使AI的滥用，例如用于生成Deepfake。在本文中，我们提出了第一个名为StolenEncoder的攻击，用于窃取预训练的图像编码器。我们评估了StolenEncoder在多个我们自己预训练的目标编码器以及三个实际目标编码器上的效果，包括由谷歌预训练的ImageNet编码器、由OpenAI预训练的CLIP编码器和作为付费EaaS部署的Clarifai的通用嵌入式编码器。我们的结果表明，StolenEncoder窃取的编码器与目标编码器具有相似的功能。特别是，建立在目标编码器和被盗编码器基础上的下游分类器具有相似的准确性。此外，使用StolenEncoder窃取目标编码器所需的数据和计算资源远少于从头开始预训练它。我们还探讨了三种扰动目标编码器产生的特征向量的防御方法。我们的评估表明，这些防御措施不足以减轻StolenEncoder的影响。

12. Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets<br>
我们介绍了一种针对机器学习模型的新型攻击类别。我们表明，一个可以对训练数据集进行投毒的对手，可以导致在这个数据集上训练的模型泄露其他方训练数据点的重要私密细节。我们的主动推理攻击将针对机器学习训练数据的完整性和隐私的两个独立研究方向联系在一起。我们的攻击在成员推理、属性推理和数据提取方面都非常有效。例如，我们的定向攻击可以通过投毒训练数据集的＜0.1%，将推理攻击的性能提高1到2个数量级。此外，一个控制了大量训练数据（例如，50%）的对手可以发起无目标攻击，使得对所有其他用户的原本私密数据点进行更精确的推理的能力提高8倍。我们的研究结果对于多方计算协议中机器学习的加密隐私保证的相关性提出了质疑，如果各方可以任意选择他们的训练数据份额。

13. Understanding Real-world Threats to Deep Learning Models in Android Apps<br>
深度学习（DL）以其卓越的性能而著称，已经在许多应用中广泛使用，同时也引来了针对模型的各种威胁。主要的威胁之一来自对抗性攻击。研究人员已经对这种威胁进行了多年的深入研究，并提出了数十种生成对抗性样本（AEs）的方法。但是，大多数方法仅在有限的模型和数据集（例如，MNIST，CIFAR-10）上进行评估。因此，攻击现实世界DL模型的有效性尚不十分明确。在本文中，我们进行了第一个针对现实世界DNN模型的对抗性攻击的系统性研究，并提供了一个名为RWM的现实世界模型数据集。特别地，我们设计了一套方法来调整当前AE生成算法以适应不同的现实世界DL模型，包括从Android应用程序中自动提取DL模型，捕获应用程序中DL模型的输入和输出，生成AE并通过观察应用程序的执行来验证它们。对于黑箱DL模型，我们设计了一种基于语义的方法来构建合适的数据集，并在执行基于转移的攻击时使用它们来训练替代模型。在分析了来自62,583个现实世界应用程序的245个DL模型之后，我们有了一个独特的机会来了解现实世界DL模型与当代AE生成算法之间的差距。令我们惊讶的是，当前的AE生成算法只能直接攻击6.53％的模型。受益于我们的方法，成功率提高到47.35％。

14. When Evil Calls: Targeted Adversarial Voice over IP Network<br>
随着COVID-19大流行从根本上重塑了远程生活和工作方式，语音IP（VoIP）电话和视频会议已成为联系社区的主要方式。然而，在通过这种通信渠道传输敌对语音样本的可行性和局限性方面，几乎没有进行过研究。在本文中，我们提出了TAINT - 针对性的敌对语音IP网络，这是针对商业语音识别平台在VoIP上的第一个针对性、查询高效、硬标签黑盒敌对攻击。VoIP的独特通道特性带来了诸如信号衰减、随机信道噪声、频率选择性等重要挑战。为解决这些挑战，我们通过逆向工程对VoIP的结构和信道特性进行了系统分析。然后开发了一种抗噪声高效梯度估计方法，以确保敌对样本生成过程的稳定和快速收敛。
我们在四个商业自动语音识别（ASR）系统上，通过五个最受欢迎的VoIP会议软件（VCS）展示了我们在空中和线上设置中的攻击。我们证明TAINT在VoIP通道增加的情况下，其性能可以与现有方法相媲美。即使在最具挑战性的场景下，比如Zoom中有活跃发言者的情况下，TAINT仍然可以在10次尝试内成功，同时不会成为视频会议的焦点。
## 人工智能&隐私保护
1. Are Attribute Inference Attacks Just Imputation?<br>
*问题*：模型会暴露其训练数据中的敏感信息。在属性推断攻击中，攻击者可以接触某些训练数据的一部分并访问训练后的模型，从而推断这些记录中敏感特征的未知值。<br>
*方案*：研究了一种属性推断的细粒度变体，敏感值推断。攻击者的目标是从候选集中识别出[具有特定敏感值的未知属性的]某些记录。此方案明确地比较了属性推断与数据插补，后者在各种关于训练数据的假设下捕获了训练的统计分布。<br>
*结论*：（1）先前的属性推断方法并未揭示模型中关于训练数据的更多信息。攻击者甚至不需要访问模型，与训练属性推理攻击所需的底层分布知识相同即可。（2）没有模型就学不到的东西，黑盒推理攻击很少学到。（3）论文中的白盒攻击可以识别出具有敏感值属性的一些记录[没有访问模型的情况下不会预测这些记录]。<br>
*PS*：差分隐私训练和删除易受攻击的记录等防御措施，并未减轻这种隐私风险。作者提出了属性推理的一种变体，探究此攻击的一些性质。

2. Auditing Membership Leakages of Multi-Exit Networks<br>
*问题*：并非所有输入都需要相同的计算成本来产生可靠的预测，多出口网络正逐渐受到关注。主干模型具有早期退出的能力，允许在模型的中间层进行预测。然而，目前的各种设计主要考虑资源使用效率和预测准确性两个方面，尚未探讨由此产生的隐私风险。<br>
*方案*：首次从成员泄露的角度分析了多出口网络的隐私问题。首先，利用现有的攻击方法来量化多出口网络在成员泄露方面的脆弱性。然后，提出了一种混合攻击，利用出口信息来提高现有攻击的性能。<br>
*结论*：多出口网络在成员泄露方面的脆弱性较低，附加在主干模型上的退出（数量和深度）与攻击性能高度相关。在三种不同对手设置下，评估了混合攻击引起的成员泄露威胁，得到一个无模型和无数据的敌手。最终，考虑了一种专门针对多出口网络的防御机制 TimeGuard。<br>
*PS*：完整地考虑某个威胁问题，考虑如何改进[强调威胁]，寻找解决方案[比较完整的研究]。

3. Characterizing and Detecting Non-Consensual Photo Sharing on Social Networks<br>
*问题*：在社交平台上，存在着巨大的视觉侵犯和照片滥用问题。用户可能会在不知情的情况下被拍摄并曝光[非共识性分享问题]，无法通过主动访问控制或旁观者检测来解决。<br>
*方案*：（1）提出了 Videre，识别并警告在不知情的情况下被拍摄的影像。首先，通过深入的用户研究阐述非共识性拍摄照片中遇到的主要特征。然后，以此为背景建立一个数据集，并[基于多深度特征融合]构建了一个主动检测器。（2）为了减轻逐个识别不知情者的开销，设计了一种基于签名的本地预授权过滤器，也可以隐含地避免分类错误。<br>
*PS*：实际的隐私问题，重点在于理论建模。

4. Enhanced Membership Inference Attacks against Machine Learning Models<br>
机器学习算法泄露了多少关于其训练数据的信息，以及为什么会泄露？会员推断攻击作为量化这种泄漏的审计工具。在本文中，我们提出了一个全面的假设检验框架，使我们不仅能够以一致的方式正式表达先前的工作，还能设计新的使用参考模型的会员推断攻击，以实现显著更高的功效（真阳性率），针对任何误报率。更重要的是，我们解释了不同攻击为何表现不同。我们提供了一种无法区分游戏的模板，并解释了攻击成功率在游戏不同实例中的解释。我们讨论了攻击者在问题表述中产生的各种不确定性，并展示了我们的方法如何尝试将攻击不确定性最小化为关于数据点在训练集中存在或不存在的一位秘密。我们对所有类型的攻击进行了差异分析，解释了它们之间的差距，并展示了什么原因导致数据点容易受到攻击（因为不同粒度的记忆原因不同，从过拟合到条件记忆）。我们的审计框架作为Privacy Meter软件工具的开放部分。

5. Feature Inference Attack on Shapley Values<br>
作为合作博弈理论中的解决方案概念，Shapley值在模型可解释性研究中得到了高度认可，并被领先的机器学习即服务（MLaaS）提供商，如谷歌、微软和IBM广泛采用。然而，尽管Shapley值为基础的模型可解释性方法已经被深入研究，但很少有研究人员考虑到Shapley值带来的隐私风险，尽管可解释性和隐私是机器学习（ML）模型的两个基石。在本文中，我们使用特征推断攻击研究基于Shapley值的模型可解释性方法的隐私风险：根据它们的Shapley值解释重建私有模型输入。具体而言，我们提出了两个对手。第一个对手可以通过在辅助数据集上训练一个攻击模型并进行黑箱访问模型可解释性服务来重建私有输入。第二个对手，即使没有任何背景知识，也可以通过利用模型输入和输出之间的局部线性相关性成功地重建大部分私有特征。我们在领先的MLaaS平台上执行了所提出的攻击，即谷歌云、微软Azure和IBM aix360。实验结果表明，领先MLaaS平台中使用的最先进的基于Shapley值的模型可解释性方法的脆弱性，并强调了在未来研究中设计隐私保护模型可解释性方法的重要性和必要性。据我们所知，这也是第一个研究Shapley值隐私风险的工作。

6. Graph Unlearning<br>
机器遗忘是一种在收到移除请求后从机器学习（ML）模型中移除某些训练数据影响的过程。虽然直接且合法，但从头开始重新训练 ML 模型会产生很高的计算开销。为了解决这个问题，在图像和文本数据领域提出了许多近似算法，其中 SISA 是最先进的解决方案。它随机将训练集分割成多个片段，并为每个片段训练一个组成模型。然而，直接将 SISA 应用于图数据可能会严重破坏图结构信息，从而降低所得 ML 模型的效用。在本文中，我们提出了 GraphEraser，一种针对图数据量身定制的新型机器遗忘框架。其贡献包括两种新颖的图分割算法和一种基于学习的聚合方法。我们在五个真实世界的图数据集上进行了广泛的实验，以说明 GraphEraser 的遗忘效率和模型效用。它实现了 2.06 倍（小数据集）到 35.94 倍（大数据集）的遗忘时间改进。另一方面，GraphEraser 达到了最高 62.5% 更高的 F1 分数，我们提出的基于学习的聚合方法达到了最高 112% 更高的 F1 分数。

7. Group Property Inference Attacks Against Graph Neural Networks<br>
最近的研究表明，机器学习（ML）模型容易受到侵犯隐私的攻击，这些攻击会泄露有关训练数据的信息。在这项工作中，我们将图神经网络（GNN）作为目标模型，并关注一种特定类型的隐私攻击，即属性推理攻击（PIA），该攻击通过访问GNN来推断训练图中的敏感属性。尽管现有的研究已经研究了针对图级属性（例如，节点度和图密度）的PIA，但我们是首个对群组属性推理攻击（GPIA）进行系统研究的，GPIA推断训练图中特定节点和链接组的分布（例如，男性节点之间的链接比女性节点之间的链接更多）。首先，我们考虑了一个威胁模型分类，并为这些设置设计了六种不同的攻击。其次，我们通过对三个具有代表性的GNN模型和三个真实世界图进行大量实验来证明这些攻击的有效性。第三，我们分析了导致GPIA成功的基本因素，并表明，训练有或没有目标属性的图上的GNN模型在模型参数和/或模型输出上存在一定的差异，这使得攻击者能够推断出属性的存在。此外，我们设计了一套针对GPIA攻击的防御机制，并通过实证表明，这些机制可以在保证GNN模型准确度的前提下有效降低攻击准确度。

8. L-SRR: Local Differential Privacy for Location-Based Services with Staircase Randomized Response<br>
基于位置的服务（LBS）在移动设备中得到了显著的发展和广泛的应用。众所周知，LBS应用可能通过收集敏感位置信息导致严重的隐私问题。一种强大的隐私模型“本地差分隐私”（LDP）近年来已被部署在许多不同的应用中（例如，谷歌RAPPOR，iOS和微软遥测），但由于现有LDP机制的低效用，它并不适用于LBS应用。为了解决这一不足，我们提出了第一个针对各种基于位置的服务的LDP框架（即“L-SRR”），该框架可以私密地收集和分析具有高效用的用户位置。具体来说，我们设计了一种名为“阶梯随机响应”（SRR）的新型随机化机制，并扩展了实证估计，以显著提高SRR在不同LBS应用中的效用（例如，交通密度估计和k-最近邻）。我们对四个真实LBS数据集进行了大量实验，并在实际应用中与其他LDP方案进行了基准测试。实验结果表明，L-SRR的性能显著优于其他方案。

9. LPGNet: Link Private Graph Networks for Node Classification<br>
分类任务在标记的图结构数据上有很多重要的应用，范围从社交推荐到金融建模。深度神经网络越来越多地用于图上的节点分类，其中具有相似特征的节点需要被赋予相同的标签。图卷积网络（GCN）是一种在这个任务上表现良好的广泛研究的神经网络结构。然而，最近针对GCN的强大的链接窃取攻击表明，即使只有黑盒访问已训练模型，推断训练图中存在哪些链接（或边）也是实际的。在本文中，我们提出了一种名为LPGNet的新神经网络结构，用于在具有隐私敏感边的图上进行训练。LPGNet通过在训练过程中使用一种新颖的图边结构设计，为边提供差分隐私（DP）保证。我们凭经验证明，LPGNet模型通常处于提供隐私和效用之间的最佳平衡点：它们比不使用边信息的“平凡”私有结构（例如，普通MLP）提供更好的效用，并且比使用完整边结构的普通GCN在抵抗现有链接窃取攻击方面表现更好。在我们评估的大多数数据集中，LPGNet还比DPGCN提供更好的隐私-效用权衡，而DPGCN是将差分隐私整合到传统GCN中的最先进的机制。

10. Membership Inference Attacks and Generalization: A Causal Perspective<br>
会员推断（MI）攻击揭示了当前神经网络随机训练方法中的隐私弱点。然而，人们并不十分了解它们为何会出现。它们仅仅是不完美泛化的自然结果吗？在训练过程中，我们应该解决哪些基本原因来减轻这些攻击？为了回答这些问题，我们提出了第一个基于原则性因果推理来解释MI攻击及其与泛化之间关系的方法。我们提供了能量化解释6种攻击变体所达到的MI攻击性能的因果图。我们反驳了一些先前的非定量假设，这些假设过于简化或高估了潜在原因的影响，因此无法捕捉到多个因素之间的复杂相互作用。我们的因果模型还通过它们共享的因果因素展示了泛化与MI攻击之间的新联系。我们的因果模型具有很高的预测能力（0.90），即它们的分析预测经常与未见实验中的观察结果相匹配，这使得通过它们进行分析成为了一种实用的替代方法。

11. Membership Inference Attacks by Exploiting Loss Trajectory<br>
机器学习模型容易受到成员推断攻击的影响，攻击者的目标是预测某个特定样本是否包含在目标模型的训练数据集中。现有的攻击方法通常仅利用来自给定目标模型的输出信息（主要是损失）。因此，在实际场景中，成员和非成员样本产生相似的小损失时，这些方法自然无法区分它们。为了解决这一限制，本文提出了一种新的攻击方法，名为TrajectoryMIA，它可以利用目标模型整个训练过程中的成员信息来提高攻击性能。为了在常见的黑盒设置中实施攻击，我们利用知识蒸馏，并通过在不同蒸馏时代的一系列中间模型上评估的损失来表示成员信息，即蒸馏损失轨迹，以及来自给定目标模型的损失。不同数据集和模型架构的实验结果表明，我们的攻击在不同指标方面具有很大优势。例如，在CINIC-10上，我们的攻击在0.1%的低假阳性率下实现了比现有方法至少高6倍的真阳性率。进一步分析表明，我们的攻击在更严格的场景中具有普遍有效性。

12. Private and Reliable Neural Network Inference<br>
可靠的神经网络（NNs）提供了重要的推理时可靠性保证，如公平性和稳健性。与此相辅相成的是，保护客户数据隐私的神经网络推理。迄今为止，这两个新兴领域在很大程度上是相互独立的，但它们的结合将变得越来越重要。在这项工作中，我们提出了第一个在可靠神经网络上实现隐私保护推理的系统。我们的关键思路是为随机平滑的核心算法构建模块设计高效的全同态加密（FHE）对等体，随机平滑是获得可靠模型的一种最先进的技术。FHE中缺乏所需的控制流使得这项任务具有挑战性，因为简单的解决方案会导致无法接受的运行时间。
我们利用这些构建模块在一个名为Phoenix的系统中实现具有稳健性和公平性保证的隐私保护神经网络推理。在实验中，我们证明Phoenix在不产生过多延迟的情况下实现了其目标。据我们所知，这是第一个将客户数据隐私与神经网络的可靠性保证领域联系起来的研究。

13. Shifted Inverse: A General Mechanism for Monotonic Functions under User Differential Privacy<br>
尽管大部分关于差分隐私的研究都集中在保护元组的隐私上，但人们已经意识到这种简单模型无法捕捉许多现实世界应用中复杂的用户-元组关系。因此，用户差分隐私（user-DP）最近受到了更多关注，其中包括作为特殊情况的图数据的节点差分隐私（node-DP）。现有的大部分关于用户差分隐私的研究仅针对求和估计问题进行了研究。在这项工作中，我们设计了一个通用的差分隐私机制，用于在用户差分隐私下具有强优化保证的任何单调函数。虽然我们的通用机制可能在超多项式时间内运行，但我们展示了如何在一些常见单调函数上实例化一个多项式时间的近似版本，包括求和、k-选择、最大频率和不同计数。最后，我们对所有这些函数进行了实验，结果表明，我们的框架在许多情况下更加通用，且获得了更好的结果。
## 联邦学习
1. Cerberus: Exploring Federated Prediction of Security Events<br>
*问题*：对抗网络攻击的防御手段越来越依赖于主动方法，根据过去的事件来预测对手的下一步行动。构建准确的预测模型需要很多组织的知识，包括网络结构、安全态势和政策。<br>
*方案*：使用联邦学习来预测未来安全事件，即Cerberus系统，采用循环神经网络（RNN）来实现协作训练。在一个入侵防御产品数据集上实例化Cerberus，并从性能、鲁棒性和隐私方面进行评估，同时考虑贡献分配机制。<br>
*结论*：本工作揭示了预测安全任务中联邦学习的积极作用与挑战。<br>
*PS*：emmm，感觉有点水，是一个联邦学习在安全领域的具体应用，可能确实落地到了现实的痛点领域。

2. DPIS: An Enhanced Mechanism for Differentially Private SGD with Importance Sampling<br>
如今，差分隐私（DP）已成为隐私保护的广泛认可标准，而深度神经网络（DNN）在机器学习领域取得了巨大成功。将这两种技术相结合，即具有差分隐私的深度学习，有望保护隐私地发布使用敏感数据（如医疗记录）训练的高效用模型。为此，经典的机制是 DP-SGD，它是一种用于 DNN 训练的差分隐私版本的随机梯度下降（SGD）优化器。随后的方法改进了模型训练过程的各个方面，包括噪声衰减时间表、模型架构、特征工程和超参数调整。然而，自原始 DP-SGD 算法以来，SGD 优化器中强制执行 DP 的核心机制一直未发生变化，这越来越成为限制 DP 合规机器学习解决方案性能的基本障碍。受此启发，我们提出了 DPIS，一种新颖的差分隐私SGD训练机制，可作为 DP-SGD 核心优化器的替代品，与后者相比具有一致且显著的准确性提升。主要思想是在每次 SGD 迭代中，使用重要性采样（IS）进行小批量选择，从而减少采样方差和满足 DP 所需注入梯度的随机噪声量。尽管在非隐私设置中，使用 IS 的 SGD 在机器学习文献中已被广泛研究，但将 IS 集成到 DP-SGD 的复杂数学机制中是非常重要的；此外，IS 还涉及到额外的私有数据发布，这些数据必须在差分隐私下受到保护，以及计算密集型梯度计算。DPIS 通过新颖的机制设计、细粒度的隐私分析、效率增强和自适应梯度裁剪优化来应对这些挑战。在四个基准数据集（即 MNIST、FMNIST、CIFAR-10 和 IMDb）上进行的大量实验，涉及卷积神经网络和循环神经网络，证明了 DPIS 在具有差分隐私的深度学习方面优于现有解决方案的有效性。

3. EIFFeL: Ensuring Integrity for Federated Learning<br>
联邦学习（FL）使客户端能够与服务器协作训练机器学习模型。为了确保隐私，服务器对来自客户端的更新进行安全聚合。不幸的是，由于更新被掩盖，这使得无法验证更新的完整性。因此，可以在不被检测的情况下注入旨在毒化模型的畸形更新。在本文中，我们将联邦学习中更新隐私和完整性问题进行形式化，并提出了一种新的系统，EIFFeL，它能够实现已验证更新的安全聚合。EIFFeL是一个通用框架，可以实施任意完整性检查，并从聚合中移除畸形更新，同时不违反隐私。我们的实证评估证明了EIFFeL的实用性。例如，在有100个客户端和10%毒化的情况下，每次迭代仅需2.4秒，EIFFeL就可以训练出与非毒化联邦学习器相同精度的MNIST分类模型。

4. Eluding Secure Aggregation in Federated Learning via Model Inconsistency<br>
安全聚合是一种加密协议，可以安全地计算其输入的聚合。在联邦学习中保护模型更新的隐私方面，它具有关键作用。实际上，使用安全聚合可以防止服务器了解用户提供的个体模型更新的值和来源，从而阻碍推理和数据归属攻击。在这项工作中，我们展示了恶意服务器可以轻松地规避安全聚合，就好像后者不存在一样。我们设计了两种不同的攻击方法，能够推断出关于个体私有训练数据集的信息，而不依赖于参与安全聚合的用户数量。这使得它们成为大规模、实际联邦学习应用中具体的威胁。这些攻击是通用的，无论使用哪种安全聚合协议，效果都是一样的。它们利用了联邦学习协议的一个漏洞，该漏洞是由于安全聚合的不正确使用和参数验证不足造成的。我们的工作表明，当前使用安全聚合的联邦学习实现仅提供一种“虚假的安全感”。

5. Federated Boosted Decision Trees with Differential Privacy<br>
针对可在分布式数据上进行训练的可扩展、安全、高效的隐私保护机器学习模型，需求巨大。尽管深度学习模型通常在集中式非安全环境中取得最佳效果，但在受到隐私和通信约束的情况下，不同的模型可能表现更好。因此，基于树的方法（如XGBoost）因其高性能和易用性而受到了广泛关注；特别是在表格数据上，它们通常能达到最先进的结果。因此，最近的一些研究关注于将像XGBoost这样的梯度提升决策树（GBDT）模型，通过同态加密（HE）和安全多方计算（MPC）等加密机制，转化为联邦设置。然而，这些方法并不总是提供正式的隐私保证，或者考虑全部超参数和实现设置。在这项工作中，我们在差分隐私（DP）下实现了GBDT模型。我们提出了一个捕捉并扩展现有差分隐私决策树方法的通用框架。我们的方法框架针对联邦环境进行了定制，我们展示了通过谨慎选择技术，可以在保持高度隐私的同时实现非常高的效用。

6. STAR: Secret Sharing for Private Threshold Aggregation Reporting<br>
阈值聚合上报系统为开发者提供了一种实用且能保护隐私的方案，以了解他们的应用程序在实际使用中的情况。不幸的是，迄今为止所提出的系统在大规模广泛采用方面不太实用，原因在于：i) 过高的信任假设；ii) 高计算成本；或 iii) 庞大的用户群。因此，真正私密的方法的采用仅限于少量巨大（且成本极高）的项目。在这项工作中，我们通过提出 STAR 系统来改进私有数据收集的现状，该系统具有高效、易于部署的特点，并对用户数据收集提供加密强制的 κ-匿名保护。STAR 协议易于实施，运行成本低，同时提供与当前最先进技术相似或超过的隐私属性。我们对 STAR 的开源实现进行测量，发现其运行速度比现有技术快 1773 倍，通信量减少了 62.4 倍，运行成本降低了 24 倍。


