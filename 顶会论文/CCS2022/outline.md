# 2022 CCS 关注内容
## 人工智能的特性
1. is your explanation stable? a robustness evaluation framework for feature attribution.<br>
*关键点*：用特征归因理解神经网络的可解释性，即某个决策和关键特征有关系。当前大部分算法旨在提升模型的忠实度。<br>
*问题*：现实环境中存在许多随机噪声，会干扰图像分类任务的特征归因；解释算法容易受到对抗性攻击，为恶意干扰的输入生成相同的解释。<br>
*方案*：提出了用于特征归因的中值检验（MeTFA），在理论上量化不确定性并提高解释算法的稳定性，适用于任何特征归因方法。<br>
*功能*：（1）检查一个特征重要或不重要，并生成显著映射以可视化结果。（2）计算特征归因分数的置信区间，并生成平滑映射以提高解释的稳定性。<br>
*优点*：（1）改善了解释的视觉质量，显著降低了不稳定性，同时保持了原始方法的忠实度。**[万金油强化]**（2）两个典型应用来展示MeTFA在应用中的潜力。**[定量评估忠实度与防御效果]**
2. AI/ML for Network Security: The Emperor has no Clothes<br>
*问题*：基于机器学习（ML）的网络流量检测方案过于黑盒，网络运营商不愿信任并将部署它们。因为这些模型容易出现欠规范化问题（**未能以足够详细的方式指定模型**），导致模型表现出意想不到的糟糕行为。<br>
*痛点*：开发可解释的ML解决方案（例如决策树），帮助解释黑盒模型的决策过程。合成高保真度又易于理解的可解释模型是具有挑战性的。<br>
*方案*：合成高保真度和低复杂度的决策树，帮助网络运营商判断他们的ML模型是否欠缺规范。Trustee将现有ML模型和训练数据集作为输入，生成高保真度、易于解释的决策树以及相关信任报告作为输出。<br>
*优点*：利用已发布的、完全可复制的ML模型，来识别模型欠规范化的三个常见实例，即快捷学习的证据、伪相关性的存在以及对分布外异常样本的脆弱性。<br>
*PS*：把人工智能的规范化作为整体的任务，利用决策树做一个模型级别的检测。作者直接把赛道提高一个维度，属于首发性工作。

## 人工智能&安全攻击
1. FenceSitter: Black-box, Content-Agnostic, and Synchronization-Free Enrollment-Phase Attacks on Speaker Recognition Systems<br>
*问题*：声纹识别系统为合法用户提供访问权限，可以在训练阶段（后门攻击）和识别阶段（规避攻击）被绕过。本文探讨了其新攻击面，即注册阶段攻击范式。<br>
*方案*：当合法用户注册到声纹识别系统时，攻击者使用不可察觉的对抗性环境声音对此系统进行投毒。本文将连续对抗扰动插入到无关的环境声音中，并通过内容脱敏和物理实现来优化对抗性片段。**[长序列环境声音载体上计算对抗扰动难以处理]** 此外，通过基于自然演化策略的梯度估计，使攻击在黑盒设置下可用。<br>
*优点*：在英文和中文语音数据集上进行了大量实验，包括密集识别（CSI）、开放识别（OSI）和说话者验证（SV）任务。[有效性、鲁棒性与实用性]<br>
*PS*：声音识别系统在注册阶段的安全性【投毒攻击】，属于首发性工作。<br>

2. Harnessing Perceptual Adversarial Patches for Crowd Counting<br>
*问题*：人群计数在物理世界中（如对抗性贴片）容易受到对抗性示例的影响。然而，现有针对人群计数的对抗性示例生成方法在不同的黑盒模型之间缺乏可迁移性，限制了它们在现实世界的实用性。<br>
*方案*：基于攻击可迁移性与模型不变特征正相关的事实，本文提出了一种基于模型共享感知特征定制人群计数场景对抗性扰动的感知对抗性贴片（PAP）生成框架。<br>
*细节*：本文手工设计一种自适应的人群密度加权方法，可以捕获不同模型间的不变尺度感知特征，并利用密度引导注意力捕获模型共享的位置感知。【增强对抗性贴片的攻击可迁移性】<br>
*优点*：（1）在数字和物理世界中均能达到最先进的攻击性能。（2）进行对抗性训练可以提高原始模型在缓解人群计数场景中的若干实际挑战方面的性能，包括数据集间的泛化能力以及对复杂背景的鲁棒性。<br>
*PS*：实际问题下的对抗样本攻击，泛化攻击效果。

3. Identifying a Training-Set Attack's Target Using Renormalized Influence Estimation<br>
*问题*：向训练集注入恶意样本，使得训练后的模型对一个或多个特定测试样本进行错误标记。本文提出目标识别任务，以判断特定测试样本是否为训练集攻击的目标。【目标识别可以与对抗性实例识别结合】<br>
*方案*：本文不是关注单一的攻击方法或数据模态，而是基于影响力估计，量化每个训练实例对模型预测的贡献。<br>
*难点*：现有影响力估计器在实际表现不佳的原因是由于过度依赖训练实例和具有较大损失的迭代。本文采用重标准化影响力估计器修复了这个弱点。<br>
*优点*：识别对抗性和非对抗性设置中的有影响力的训练样本群体方面表现远超原始估计器。【文本、视觉和语音/专门优化对抗性实例/后门和投毒攻击】<br>
*PS*：目标识别任务则简化为检测具有异常影响力值的测试实例，针对数据集的过滤。

4. LoneNeuron: A Highly-Effective Feature-Domain Neural Trojan Using Invisible and Polymorphic Watermarks<br>
*问题*：潜藏在预训练神经网络中的木马是针对DNN模型供应链的有害攻击。代码中毒和模型中毒的后门直到最近才开始引起关注。
*方案*：本文提出了一种新颖的模型中毒神经木马，对特征域模式作出反应。这些模式会转化为隐形的、样本特异性的和多态性的像素域水印。
*优点*：通过LoneNeuron独特的水印多态性属性，同一特征域触发器在像素域中被解析为多个水印，进一步提高了水印的随机性、隐蔽性和抵抗木马检测的能力。
*PS*：可以逃过最先进的木马检测器，且是针对视觉变换器（ViTs）的第一种有效的后门攻击。

4. Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models<br>
*问题*：神经文本排序模型存在对抗性漏洞。它们已被发现但仍未被充分探讨。<br>
*方案*：提出了一种针对黑盒神经通道排序模型的模仿对抗攻击。**首先**，目标通道排序模型可以通过枚举关键查询/候选项来透明化和模仿，训练一个排序模仿模型。**然后**，利用排序模仿模型，进而精心操纵排序结果，将操纵攻击转移到目标排序模型。**同时**，提出了一种基于梯度的攻击方法，通过成对的目标函数生成对抗性触发器，这些触发器只需要很少的令牌就能引起预谋的错乱。【为了使触发器伪装，将下一句预测损失和语言模型流利性约束添加到目标函数】<br>
*优点*：排序模仿攻击模型和对抗触发器对各种SOTA神经排序模型均有效。<br>
**PS**：很新的问题，将对抗性攻击聚焦于在神经网络在排序模型上的应用。<br>

5. Perception-Aware Attack: Creating Adversarial Music via Reverse-Engineering Human Perception<br>
*问题*：尚不清楚攻击者是否能够创建既能被人类感知到的又具有攻击效果的音频信号扰动。<br>
*方案*：本文将针对音乐信号的对抗性攻击制定为一种新的感知感知攻击框架，考虑人类研究。根据原始和扰动音乐信号对的配对来评估人类感知到的偏差，通过回归分析对人类感知过程进行反向工程。然后，将感知攻击变为一个优化问题，最小化来自回归人类感知模型的感知偏差预测来找到最佳扰动信号。<br>
*优点*：该攻击产生的对抗性音乐具有明显更好的感知质量。<br>
*PS*：很奇特的研究，考虑到人类的感知因素。基于感知的安全检测/攻击，也是一个研究点。<br>

6. Physical Hijacking Attacks against Object Trackers<br>
*问题*：自动驾驶场景下，物体跟踪与物体检测结合能够提高物体检测器的准确性。现有针对物体跟踪的攻击方法效果与实用性较差。<br>
*方案*：提出一种可实现物理攻击的SOTA跟踪器劫持攻击方法，能够确定环境中可用于物理扰动的有效区域。它利用暹罗区域提议网络生成热图的过程，以便控制物体的边界框，威胁自动驾驶安全性。<br>
*PS*：引入目前攻击在实际场景下的弱点，重点介绍自己的攻击方案。<br>

7. Post-breach Recovery: Protection against White-box Adversarial Examples for Leaked DNN Models<br>
*问题*：泄露的模型会给攻击者提供"白盒"访问权限来生成对抗性样本，这种威胁模型缺少实用的防御手段。<br>
*方案*：创建泄露模型的新版本，并在推断时使用一个过滤器来检测和移除基于之前泄露模型生成的对抗性样本。【不同模型版本的分类表面稍有偏移，本方案可以检测到攻击对泄露模型的过度拟合】<br>
*优点*：本方案可以过滤掉泄露模型的对抗攻击，并且对各种强适应性攻击表现良好。<br>
*PS*：考虑白盒下对抗攻击，包装成一种新的攻击场景。<br>

8. SPECPATCH: Human-In-The-Loop Adversarial Audio Spectrogram Patch Attack on Speech Recognition<br>
*问题*：现有的音频对抗攻击者假设用户无法注意到对抗性音频，从而允许成功传递定制的对抗性示例或扰动。然而，在实际攻击场景中，智能语音控制系统（例如智能手表、智能扬声器、智能手机）的用户对可疑声音保持警惕。这使得现有的攻击在用户交互与对抗性音频传递同时发生的典型场景下无效。<br>
*方案*：本方案旨在开发真正实现不可察觉且稳定的对抗攻击，并处理可能出现的用户中断。该攻击使用小于一秒的音频补丁信号传递攻击命令，并利用周期性噪声破坏用户与系统之间的通信。【分析了CTC（连接时序分类）损失的前向和反向过程，并利用 CTC 的弱点实现攻击目标】<br>
*优点*：本攻击将攻击影响长度（即攻击目标命令的长度）扩展3倍。在用户干预下，在超线和空中场景下均实现了 100% 的攻击成功率。<br>
*PS*：考虑到音频交互场景【更贴近实际】，强调了攻击的困难与有效性。<br>

9. SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders<br>
*问题*：与利用高质量标记数据集的监督学习相比，自监督学习依赖于未标记数据集来预训练强大的编码器，然后将其作为各种下游任务的特征提取器。ML模型的版权容易受到模型窃取攻击的威胁，攻击者训练一个替代模型来模仿给定模型的行为。目前的版权保护算法【水印技术】集中在分类器上，预训练编码器版权保护的内在挑战尚未得到充分研究。<br>
*方案*：本方案提出首个针对预训练编码器的水印方案。给定一个干净的预训练编码器，它将水印注入其中并输出一个带有水印的版本。该方案还应用了阴影训练技术，以在潜在的模型窃取攻击下保留水印。<br>
*优点*：在水印注入和验证方面非常有效，且能抵抗模型窃取和其他水印移除攻击，如输入噪声、输出扰动、覆写、模型剪枝和微调。<br>
*PS*：首发工作，把某个点的研究放到另外一个场景。方法A迁移到场景B，做一些优化。<br>

10. StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning<br>
*问题*：自监督学习可以使用大量无标签数据预训练高效的编码器。预训练编码器可能被认为是机密的，因为其训练通常需要大量的数据和计算资源【存在AI滥用的隐患，如Deepfake】
*方案*：用于窃取预训练的图像编码器。<br>
*实验*：在多个预训练的目标编码器以及三个实际目标编码器上【谷歌预训练的ImageNet编码器、由OpenAI预训练的CLIP编码器和作为付费EaaS部署的Clarifai的通用嵌入式编码器】，StolenEncoder窃取的编码器与目标编码器具有相似的功能。<br>
*优点*：建立在目标编码器和被盗编码器基础上的下游分类器具有相似的准确性，窃取所需的数据和计算资源远少于从头开始预训练编码器。目前三种扰动目标编码器产生的特征向量的防御方，并不足以减轻StolenEncoder的影响。<br>
*PS*：针对自监督学习的模型窃取攻击。也可以考虑自监督学习下的成员推理，遗忘学习，后门攻击与防御。<br>

11. Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets<br>
*问题*：对训练数据集进行投毒的对手，可以导致在这个数据集上训练的模型泄露其他方训练数据点的重要私密细节。<br>
*方案*：将针对机器学习训练数据的完整性和隐私的两个独立研究方向联系在一起。<br>
*优点*：定向攻击可以通过投毒＜0.1%训练数据集，将推理攻击的性能提高1到2个数量级。此外，一个控制了大量训练数据（例如，50%）的对手可以发起无目标攻击，使得对所有其他用户的原本私密数据点进行更精确的推理。<br>
*PS*：若各方可以任意选择训练数据的份额，多方计算协议中机器学习的加密隐私保证存在隐患。**隐私与安全结合，需要注意威胁模型与场景**<br>

12. Understanding Real-world Threats to Deep Learning Models in Android Apps<br>
*问题*：对抗攻击在现实世界DL模型中具有不确定性。<br>
*方案*：提供了一个名为RWM的现实世界模型数据集。具体地，设计了一套方法来调整当前对抗攻击算法以适应不同的现实世界DL模型【从Android应用程序中提取DL模型，捕获应用程序中DL模型的输入和输出，生成对抗样本并通过观察应用程序的执行来验证对抗攻击算法】对于黑箱DL模型，设计了一种基于语义的方法来构建数据集，并在执行基于迁移的攻击时使用它们来训练替代模型。<br>
*优点*：分析现实世界DL模型与目前AE生成算法之间的差距。当前的AE生成算法只能直接攻击6.53％的模型，而本方案将成功率提高到47.35％。<br>
*PS*：考虑安卓APP应用场景，把对抗攻击落实到具体应用。在最上层优化针对安卓APP的对抗攻击算法。**真实性**<br>

13. When Evil Calls: Targeted Adversarial Voice over IP Network<br>
*问题*：语音IP电话和视频会议已成为联系社区的主要方式。然而，通过这种渠道传输对抗语音样本的可行性和局限性没有进行过研究。<br>
*方案*：针对商业语音识别在VoIP上的第一个针对性、查询高效、硬标签黑盒对抗攻击。VoIP的独特通道特性带来了诸如信号衰减、随机信道噪声、频率选择性等重要挑战。具体地，通过逆向工程对VoIP的结构和信道特性进行了系统分析，设计了一种抗噪声高效梯度估计方法，以确保敌对样本生成过程的稳定和快速收敛。<br>
*优点*：在四个商业自动语音识别（ASR）系统上，通过五个最受欢迎的VoIP会议软件展示了线上和线下设置中的攻击。在VoIP通道增加的情况下，其性能可以与现有方法相当。即使在最具挑战性的场景下，比如Zoom中有活跃发言者的情况下，仍可以在10次尝试内成功，同时不会成为视频会议的焦点。<br>
*PS*：针对语音IP电话和视频会议场景下的对抗攻击，仍然是对商业语音识别的攻击，只不过是考虑新的限制。**故事讲的好**。<br>
## 人工智能&隐私保护
1. Are Attribute Inference Attacks Just Imputation?<br>
*问题*：模型会暴露其训练数据中的敏感信息。在属性推断攻击中，攻击者可以接触某些训练数据的一部分并访问训练后的模型，从而推断这些记录中敏感特征的未知值。<br>
*方案*：研究了一种属性推断的细粒度变体，敏感值推断。攻击者的目标是从候选集中识别出[具有特定敏感值的未知属性的]某些记录。此方案明确地比较了属性推断与数据插补，后者在各种关于训练数据的假设下捕获了训练的统计分布。<br>
*结论*：（1）先前的属性推断方法并未揭示模型中关于训练数据的更多信息。攻击者甚至不需要访问模型，与训练属性推理攻击所需的底层分布知识相同即可。（2）没有模型就学不到的东西，黑盒推理攻击很少学到。（3）论文中的白盒攻击可以识别出具有敏感值属性的一些记录[没有访问模型的情况下不会预测这些记录]。<br>
*PS*：差分隐私训练和删除易受攻击的记录等防御措施，并未减轻这种隐私风险。作者提出了属性推理的一种变体，探究此攻击的一些性质。

2. Auditing Membership Leakages of Multi-Exit Networks<br>
*问题*：并非所有输入都需要相同的计算成本来产生可靠的预测，多出口网络正逐渐受到关注。主干模型具有早期退出的能力，允许在模型的中间层进行预测。然而，目前的各种设计主要考虑资源使用效率和预测准确性两个方面，尚未探讨由此产生的隐私风险。<br>
*方案*：首次从成员泄露的角度分析了多出口网络的隐私问题。首先，利用现有的攻击方法来量化多出口网络在成员泄露方面的脆弱性。然后，提出了一种混合攻击，利用出口信息来提高现有攻击的性能。<br>
*结论*：多出口网络在成员泄露方面的脆弱性较低，附加在主干模型上的退出（数量和深度）与攻击性能高度相关。在三种不同对手设置下，评估了混合攻击引起的成员泄露威胁，得到一个无模型和无数据的敌手。最终，考虑了一种专门针对多出口网络的防御机制 TimeGuard。<br>
*PS*：完整地考虑某个威胁问题，考虑如何改进[强调威胁]，寻找解决方案[比较完整的研究]。<br>

3. Characterizing and Detecting Non-Consensual Photo Sharing on Social Networks<br>
*问题*：在社交平台上，存在着巨大的视觉侵犯和照片滥用问题。用户可能会在不知情的情况下被拍摄并曝光[非共识性分享问题]，无法通过主动访问控制或旁观者检测来解决。<br>
*方案*：（1）提出了 Videre，识别并警告在不知情的情况下被拍摄的影像。首先，通过深入的用户研究阐述非共识性拍摄照片中遇到的主要特征。然后，以此为背景建立一个数据集，并[基于多深度特征融合]构建了一个主动检测器。（2）为了减轻逐个识别不知情者的开销，设计了一种基于签名的本地预授权过滤器，也可以隐含地避免分类错误。<br>
*PS*：实际的隐私问题，重点在于理论建模。<br>

4. Enhanced Membership Inference Attacks against Machine Learning Models<br>
*问题*：成员推断攻击作为量化模型信息泄漏的审计工具。<br>
*方案*：本文提出一个全面的假设检验框架，不仅能够以一致的方式正式表达先前的工作，还能设计新的参考模型的成员推断攻击，实现显著更高的功效（真阳性率）。同时，它能解释不同攻击为何表现不同。【提供了一种无法区分游戏的模板，并解释了攻击成功率在游戏不同实例中的解释】<br>
*贡献*：讨论了攻击者在问题表述中产生的各种不确定性，并展示了此方法如何尝试将攻击不确定性最小化为关于数据点在训练集中存在或不存在的问题。【对所有类型的攻击进行了差异分析，解释了它们之间的差距，并展示了什么原因导致数据点容易受到攻击（因为不同粒度的记忆原因不同，从过拟合到条件记忆）】<br>
*PS*：系统地研究成员推理攻击，突出可解释性。**不确定性可以作为强化攻击的通用手段**<br>

5. Feature Inference Attack on Shapley Values<br>
*问题*：博弈论中常用Shapley值研究模型可解释性。然而，它会带来的隐私风险。本文使用特征推断攻击研究基于Shapley值的模型可解释性方法的隐私风险。<br>
*方案*：根据Shapley值解释重建私有模型输入。本文提出两种敌手：第一个敌手在辅助数据集上训练一个攻击模型，然后进行黑箱访问模型可解释性服务来重建私有输入。第二个敌手，即使没有任何背景知识，也可以通过利用模型输入和输出之间的局部线性相关性成功地重建大部分私有特征。<br>
*优点*：最先进的基于Shapley值的模型可解释性方法存在隐私泄露风险，未来需要兼顾隐私保护效果。<br>
*PS*：从成熟的可解释性出发，考虑隐私。此思路可用于启发研究：某成熟方案的可解释性、隐私性与安全性。<br>

6. Graph Unlearning<br>
*问题*：SISA遗忘算法是最先进的解决方案【随机将训练集分割成多个片段，并为每个片段训练一个组成模型】然而，直接将 SISA 应用于图数据可能会严重破坏图结构信息。<br>
*方案*：本文提出一种针对图数据量身定制的新型机器遗忘框架。其贡献包括两种新颖的图分割算法和一种基于学习的聚合方法。<br>
*优点*：在图遗忘领域，更快的遗忘效率以及更好的聚合方法【较少影响模型可用性】。<br>
*PS*：首发性工作，将遗忘学习放到图领域。下面图遗忘的可衡量性或者成员推理也是研究点。<br>

7. Group Property Inference Attacks Against Graph Neural Networks<br>
*问题*：图神经网络（GNN）作为目标模型，也会遭受属性推理攻击（PIA）。该攻击通过访问GNN来推断训练图中的敏感属性。<br>
*方案*：现有研究针对图级属性（例如，节点度和图密度）的PIA，此攻击是首个对群组的属性推理攻击（GPIA）。<br>
*优点*：采用三个具有代表性的GNN模型和三个真实世界图进行大量实验。【训练有或没有目标属性的图上的GNN模型在模型参数和/或模型输出上存在一定的差异，这使得攻击者能够推断出属性的存在】此外，本文设计了一套针对GPIA攻击的防御机制。<br>
*PS*：对图神经网络属性推理攻击的延伸，工作不够可以把防御也提出。<br>

8. L-SRR: Local Differential Privacy for Location-Based Services with Staircase Randomized Response<br>
*问题*：基于位置的服务（LBS）通过收集敏感位置信息导致严重的隐私问题。而本地差分隐私不适用于LBS应用。<br>
*方案*：本文提出了第一个针对各种基于位置的服务的LDP框架，私密地收集和分析具有高效用的用户位置。具体地，设计了“阶梯随机响应”（SRR）的新型随机化机制并扩展了实证估计。<br>
*优点*：对四个真实LBS数据集进行了大量实验，此方案的性能显著优于现有方案。<br>
*PS*：将本地差分隐私引入基于位置的服务，找贴合实际的应用场景。<br>

9. LPGNet: Link Private Graph Networks for Node Classification<br>
*问题*：深度神经网络广泛地用于图上的节点分类，其中具有相似特征的节点需要被赋予相同的标签【常用图卷积网络（GCN）】目前，针对GCN的强大的链接窃取攻击表明，即使只有黑盒访问权限，推断训练图中存在哪些链接（或边）也是实际的。<br>
*方案*：本文提出一种新神经网络结构，用于在具有隐私敏感边的图上进行训练，为边提供差分隐私（DP）保证。<br>
*优点*：此模型可以提供隐私和效用之间的最佳平衡点：比不使用边信息的“平凡”私有结构（例如，普通MLP）提供更好的效用；比使用完整边结构的普通GCN在抵抗现有链接窃取攻击方面表现更好。此方案【LPGNet】在隐私-效用权衡上优于最先进的机制【DPGCN】。<br>
*PS*：为图数据中的边提供差分隐私保护。虽然并非首发性工作，可从隐私-效用均衡的方面考虑。**思路：安全-隐私-效率-效用2-3者均衡**<br>

10. Membership Inference Attacks and Generalization: A Causal Perspective<br>
*问题*：现有研究不了解成员推断（MI）攻击。【不完美泛化？应该解决哪些基本原因来减轻这些攻击？】<br>
*研究*：本文提出第一个基于原则性因果推理来解释MI攻击及其与泛化之间关系的方法。【提供了量化解释6种攻击变体所达到的MI攻击性能的因果图】<br>
*贡献*：（1）反驳了一些先前的非定量假设。【过于简化或高估了潜在原因的影响，无法捕捉到多个因素之间的复杂相互作用】（2）因果模型通过它们共享的因果因素展示了泛化与MI攻击之间的新联系。【此方法的分析预测经常与未见实验中的观察结果相匹配】<br>
*PS*：成员推理攻击的可解释性，需要更加系统的研究。**思路：某种常见攻击/防御的可解释性研究**<br>

11. Membership Inference Attacks by Exploiting Loss Trajectory<br>
*问题*：成员推理攻击通常利用来自给定目标模型的输出信息（主要是损失）。若成员和非成员样本产生相似的小损失时，攻击方法失效。<br>
*方案*：提出TrajectoryMIA攻击，可以利用目标模型整个训练过程中的成员信息来提高攻击性能。为了在常见的黑盒设置中实施攻击，利用知识蒸馏并通过在不同蒸馏时代的一系列中间模型上评估的损失来表示成员信息，即蒸馏损失轨迹，以及来自给定目标模型的损失。<br>
*优点*：不同数据集和模型架构的实验结果表明，此攻击在不同指标方面具有很大优势，且在更严格的场景中具有普遍有效性。<br>
*PS*：攻击限制很低。【impact很大】从另外一个方面考虑成员推理攻击，即蒸馏损失轨迹，是否可用于作为训练水印或者数据集水印。<br>

12. Private and Reliable Neural Network Inference<br>
*问题*：神经网路推理需要具有可靠性与隐私保护效果。<br>
*方案*：提出了第一个在可靠神经网络上实现隐私保护推理的系统。具体地，为随机平滑的核心算法构建模块设计高效的全同态加密（FHE）对等体【随机平滑是获得可靠模型的一种最先进的技术】。<br>
*难点*：FHE中缺乏所需的控制流，简单的解决方案会导致无法接受的运行时间。<br>
*优点*：此方案在不产生过多延迟的情况下兼顾稳健性、公平性与隐私保护效果。<br>
*PS*：首发性工作，考虑某种场景下的隐私。**思路：将多种安全属性进行融合，例如隐私性、安全性、稳健性以及高效**<br>

13. Shifted Inverse: A General Mechanism for Monotonic Functions under User Differential Privacy<br>
*问题*：差分隐私的研究都集中在保护元组上。现实世界中，用户-元组关系下的差分隐私研究还是空白。本文关注用户差分隐私（user-DP），包括作为特殊情况的图数据的节点差分隐私（node-DP）。
*方案*：本文设计了一个通用的差分隐私机制，用于在用户差分隐私下具有强优化保证的任何单调函数。尽管此通用机制可能在超多项式时间内运行，但在一些常见单调函数上可以实例化一个多项式时间的近似版本。【求和、k-选择、最大频率和不同计数】
*优点*：此框架在许多情况下更加通用，适用于用户差分隐私（user-DP）。
*PS*：关注更复杂数据结构下的差分隐私机制。**神经网络应用中更复杂的数据结构，是否存在基于树、图与超图等的应用（差分隐私、后门攻击等）**

## 联邦学习
1. Cerberus: Exploring Federated Prediction of Security Events<br>
*问题*：对抗网络攻击的防御手段越来越依赖于主动方法，根据过去的事件来预测对手的下一步行动。构建准确的预测模型需要很多组织的知识，包括网络结构、安全态势和政策。<br>
*方案*：使用联邦学习来预测未来安全事件，即Cerberus系统，采用循环神经网络（RNN）来实现协作训练。在一个入侵防御产品数据集上实例化Cerberus，并从性能、鲁棒性和隐私方面进行评估，同时考虑贡献分配机制。<br>
*结论*：本工作揭示了预测安全任务中联邦学习的积极作用与挑战。<br>
*PS*：emmm，感觉有点水，是一个联邦学习在安全领域的具体应用，可能确实落地到了现实的痛点领域。

2. DPIS: An Enhanced Mechanism for Differentially Private SGD with Importance Sampling<br>
*问题*：深度神经网络中可以应用差分隐私技术。【经典的机制是 DP-SGD，是一种用于训练的差分隐私版本的随机梯度下降（SGD）优化器】随后的方法改进了模型训练过程的各个方面，【噪声衰减时间表、模型架构、特征工程和超参数调整】而SGD 优化器中强制执行 DP 的核心机制一直未发生变化【限制 DP 合规机器学习解决方案性能】。<br>
*方案*：提出了 DPIS作为 DP-SGD 核心优化器的替代品【也是一种差分隐私SGD训练机制】。在每次 SGD 迭代中，使用重要性采样（IS）进行小批量选择，从而减少采样方差和满足 DP 所需注入梯度的随机噪声量。一方面，将 IS 集成到 DP-SGD 的复杂数学机制中非常重要；另一方面，IS 还涉及到额外的私有数据发布【这些数据必须在差分隐私下受到保护，以及计算密集型梯度计算】<br>
*优点*：DPIS 通过设计细粒度的隐私分析、效率增强和自适应梯度裁剪优化来应对这些挑战。实验证明，DPIS 在具有差分隐私的深度学习【MNIST、FMNIST、CIFAR-10和IMDb】方面优于现有解决方案的有效性。<br>
*PS*：参照DP-SGD设计一种新的差分隐私SGD训练机制，为了打破这个方法的桎梏。**思路：找热点问题的另一条解决方法，在某方面突出即可**<br>

3. EIFFeL: Ensuring Integrity for Federated Learning<br>
*问题*：联邦学习下更新被掩盖，服务器无法验证更新的完整性。因此，可以在不被检测的情况下注入旨在毒化模型的畸形更新。<br>
*方案*：本文将联邦学习中更新隐私和完整性问题进行形式化，提出一种新系统EIFFeL，能够实现已验证更新的安全聚合。它是一个通用框架，可以实施任意完整性检查，并从聚合中移除畸形更新，同时不侵犯隐私。<br>
*优点*：在有100个客户端和10%毒化的情况下，每次迭代仅需2.4秒，EIFFeL就可以训练出与非毒化联邦学习器相同精度的MNIST分类模型。<br>
*PS*：突出服务器端验证问题，在不侵犯隐私前提下完成验证与过滤。**兼顾联邦学习的隐私与安全**<br>

5. Eluding Secure Aggregation in Federated Learning via Model Inconsistency<br>
*问题*：安全聚合可以安全地计算其输入的聚合，可以防止服务器了解用户提供的个体模型更新的值和来源，从而阻碍推理和数据归属攻击。<br>
*方案*：恶意服务器可以轻松地规避安全聚合。本文设计了两种不同的攻击方法，能够推断出关于个体私有训练数据集的信息，而不依赖于参与安全聚合的用户数量。<br>
*分析*：这些攻击是通用的，无论使用哪种安全聚合协议，均有攻击效果。它们利用了联邦学习协议的一个漏洞，【安全聚合的不正确使用和参数验证不足造成的】说明当前使用的安全聚合提供一种“虚假的安全感”。<br>
*PS*：针对安全聚合机制的隐私攻击，发现了通用性的漏洞。**（1）方案的通用性很重要（2）针对联邦学习的隐私攻击很火**<br>

6. Federated Boosted Decision Trees with Differential Privacy<br>
*问题*：基于树的方法（如XGBoost）具有高性能与易用性，特别是在表格数据上，往往达到最先进的结果。近期，关注于将像XGBoost这样的梯度提升决策树（GBDT）模型，通过同态加密（HE）和安全多方计算（MPC）等技术，转化为联邦设置。<br>
*难点*：HE与MPC并不总是提供正式的隐私保证，或者考虑全部超参数和实现设置。<br>
*方案*：在差分隐私（DP）下实现了GBDT模型，提出了一个捕捉并扩展现有差分隐私决策树方法的通用框架。<br>
*优点*：此方法针对联邦环境进行了定制，采用谨慎选择技术，可以在兼顾隐私与效用。<br>
*PS*：基于树方法与差分隐私相结合。**以GBDT为核心，探究隐私攻击/防御、安全攻击/防御**<br>

7. STAR: Secret Sharing for Private Threshold Aggregation Reporting<br>
*问题*：阈值聚合上报系统为开发者提供了一种实用且能保护隐私的方案。然而，目前系统在大规模广泛采用方面不太实用：i) 过高的信任假设；ii) 高计算成本；或 iii) 庞大的用户群。<br>
*方案*：提出 STAR 系统【高效且易于部署】来改进私有数据收集的现状，对用户数据收集提供加密强制的 κ-匿名保护。<br>
*优点*：STAR 协议易于实施，运行成本低，同时提供与当前最先进技术相似或超过的隐私属性。<br>
*PS*：是一种私有数据收集方案，全方位提高效果。**是否可以用于其它数据类型，如图、文本等**<br>


