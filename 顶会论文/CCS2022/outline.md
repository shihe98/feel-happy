# 2022 CCS 关注内容
## 人工智能的特性
1. is your explanation stable? a robustness evaluation framework for feature attribution.<br>
*关键点*：用特征归因理解神经网络的可解释性，即某个决策和关键特征有关系。当前大部分算法旨在提升模型的忠实度。<br>
*问题*：现实环境中存在许多随机噪声，会干扰图像分类任务的特征归因；解释算法容易受到对抗性攻击，为恶意干扰的输入生成相同的解释。<br>
*方案*：提出了用于特征归因的中值检验（MeTFA），在理论上量化不确定性并提高解释算法的稳定性，适用于任何特征归因方法。<br>
*功能*：（1）检查一个特征重要或不重要，并生成显著映射以可视化结果。（2）计算特征归因分数的置信区间，并生成平滑映射以提高解释的稳定性。<br>
*优点*：（1）改善了解释的视觉质量，显著降低了不稳定性，同时保持了原始方法的忠实度。**[万金油强化]**（2）两个典型应用来展示MeTFA在应用中的潜力。**[定量评估忠实度与防御效果]**
2. AI/ML for Network Security: The Emperor has no Clothes<br>
*问题*：基于机器学习（ML）的网络流量检测方案过于黑盒，网络运营商不愿信任并将部署它们。因为这些模型容易出现欠规范化问题（**未能以足够详细的方式指定模型**），导致模型表现出意想不到的糟糕行为。<br>
*痛点*：开发可解释的ML解决方案（例如决策树），帮助解释黑盒模型的决策过程。合成高保真度又易于理解的可解释模型是具有挑战性的。<br>
*方案*：合成高保真度和低复杂度的决策树，帮助网络运营商判断他们的ML模型是否欠缺规范。Trustee将现有ML模型和训练数据集作为输入，生成高保真度、易于解释的决策树以及相关信任报告作为输出。<br>
*优点*：利用已发布的、完全可复制的ML模型，来识别模型欠规范化的三个常见实例，即快捷学习的证据、伪相关性的存在以及对分布外异常样本的脆弱性。<br>
*PS*：把人工智能的规范化作为整体的任务，利用决策树做一个模型级别的检测。作者直接把赛道提高一个维度，属于首发性工作。

## 人工智能&隐私保护
1. Are Attribute Inference Attacks Just Imputation?<br>
*问题*：模型会暴露其训练数据中的敏感信息。在属性推断攻击中，攻击者可以接触某些训练数据的一部分并访问训练后的模型，从而推断这些记录中敏感特征的未知值。<br>
*方案*：研究了一种属性推断的细粒度变体，敏感值推断。攻击者的目标是从候选集中识别出[具有特定敏感值的未知属性的]某些记录。此方案明确地比较了属性推断与数据插补，后者在各种关于训练数据的假设下捕获了训练的统计分布。<br>
*结论*：（1）先前的属性推断方法并未揭示模型中关于训练数据的更多信息。攻击者甚至不需要访问模型，与训练属性推理攻击所需的底层分布知识相同即可。（2）没有模型就学不到的东西，黑盒推理攻击很少学到。（3）论文中的白盒攻击可以识别出具有敏感值属性的一些记录[没有访问模型的情况下不会预测这些记录]。<br>
*PS*：差分隐私训练和删除易受攻击的记录等防御措施，并未减轻这种隐私风险。作者提出了属性推理的一种变体，探究此攻击的一些性质。

2. Auditing Membership Leakages of Multi-Exit Networks<br>
*问题*：并非所有输入都需要相同的计算成本来产生可靠的预测，多出口网络正逐渐受到关注。主干模型具有早期退出的能力，允许在模型的中间层进行预测。然而，目前的各种设计主要考虑资源使用效率和预测准确性两个方面，尚未探讨由此产生的隐私风险。<br>
*方案*：首次从成员泄露的角度分析了多出口网络的隐私问题。首先，利用现有的攻击方法来量化多出口网络在成员泄露方面的脆弱性。然后，提出了一种混合攻击，利用出口信息来提高现有攻击的性能。<br>
*结论*：多出口网络在成员泄露方面的脆弱性较低，附加在主干模型上的退出（数量和深度）与攻击性能高度相关。在三种不同对手设置下，评估了混合攻击引起的成员泄露威胁，得到一个无模型和无数据的敌手。最终，考虑了一种专门针对多出口网络的防御机制 TimeGuard。<br>
*PS*：完整地考虑某个威胁问题，考虑如何改进[强调威胁]，寻找解决方案[比较完整的研究]。

3. Characterizing and Detecting Non-Consensual Photo Sharing on Social Networks<br>
*问题*：在社交平台上，存在着巨大的视觉侵犯和照片滥用问题。用户可能会在不知情的情况下被拍摄并曝光[非共识性分享问题]，无法通过主动访问控制或旁观者检测来解决。<br>
*方案*：（1）提出了 Videre，识别并警告在不知情的情况下被拍摄的影像。首先，通过深入的用户研究阐述非共识性拍摄照片中遇到的主要特征。然后，以此为背景建立一个数据集，并[基于多深度特征融合]构建了一个主动检测器。（2）为了减轻逐个识别不知情者的开销，设计了一种基于签名的本地预授权过滤器，也可以隐含地避免分类错误。<br>
*PS*：实际的隐私问题，重点在于理论建模。

4. Enhanced Membership Inference Attacks against Machine Learning Models<br>
机器学习算法泄露了多少关于其训练数据的信息，以及为什么会泄露？会员推断攻击作为量化这种泄漏的审计工具。在本文中，我们提出了一个全面的假设检验框架，使我们不仅能够以一致的方式正式表达先前的工作，还能设计新的使用参考模型的会员推断攻击，以实现显著更高的功效（真阳性率），针对任何误报率。更重要的是，我们解释了不同攻击为何表现不同。我们提供了一种无法区分游戏的模板，并解释了攻击成功率在游戏不同实例中的解释。我们讨论了攻击者在问题表述中产生的各种不确定性，并展示了我们的方法如何尝试将攻击不确定性最小化为关于数据点在训练集中存在或不存在的一位秘密。我们对所有类型的攻击进行了差异分析，解释了它们之间的差距，并展示了什么原因导致数据点容易受到攻击（因为不同粒度的记忆原因不同，从过拟合到条件记忆）。我们的审计框架作为Privacy Meter软件工具的开放部分。

5. Feature Inference Attack on Shapley Values<br>
作为合作博弈理论中的解决方案概念，Shapley值在模型可解释性研究中得到了高度认可，并被领先的机器学习即服务（MLaaS）提供商，如谷歌、微软和IBM广泛采用。然而，尽管Shapley值为基础的模型可解释性方法已经被深入研究，但很少有研究人员考虑到Shapley值带来的隐私风险，尽管可解释性和隐私是机器学习（ML）模型的两个基石。在本文中，我们使用特征推断攻击研究基于Shapley值的模型可解释性方法的隐私风险：根据它们的Shapley值解释重建私有模型输入。具体而言，我们提出了两个对手。第一个对手可以通过在辅助数据集上训练一个攻击模型并进行黑箱访问模型可解释性服务来重建私有输入。第二个对手，即使没有任何背景知识，也可以通过利用模型输入和输出之间的局部线性相关性成功地重建大部分私有特征。我们在领先的MLaaS平台上执行了所提出的攻击，即谷歌云、微软Azure和IBM aix360。实验结果表明，领先MLaaS平台中使用的最先进的基于Shapley值的模型可解释性方法的脆弱性，并强调了在未来研究中设计隐私保护模型可解释性方法的重要性和必要性。据我们所知，这也是第一个研究Shapley值隐私风险的工作。

## 联邦学习
1. Cerberus: Exploring Federated Prediction of Security Events<br>
*问题*：对抗网络攻击的防御手段越来越依赖于主动方法，根据过去的事件来预测对手的下一步行动。构建准确的预测模型需要很多组织的知识，包括网络结构、安全态势和政策。<br>
*方案*：使用联邦学习来预测未来安全事件，即Cerberus系统，采用循环神经网络（RNN）来实现协作训练。在一个入侵防御产品数据集上实例化Cerberus，并从性能、鲁棒性和隐私方面进行评估，同时考虑贡献分配机制。<br>
*结论*：本工作揭示了预测安全任务中联邦学习的积极作用与挑战。<br>
*PS*：emmm，感觉有点水，是一个联邦学习在安全领域的具体应用，可能确实落地到了现实的痛点领域。

2. DPIS: An Enhanced Mechanism for Differentially Private SGD with Importance Sampling<br>
如今，差分隐私（DP）已成为隐私保护的广泛认可标准，而深度神经网络（DNN）在机器学习领域取得了巨大成功。将这两种技术相结合，即具有差分隐私的深度学习，有望保护隐私地发布使用敏感数据（如医疗记录）训练的高效用模型。为此，经典的机制是 DP-SGD，它是一种用于 DNN 训练的差分隐私版本的随机梯度下降（SGD）优化器。随后的方法改进了模型训练过程的各个方面，包括噪声衰减时间表、模型架构、特征工程和超参数调整。然而，自原始 DP-SGD 算法以来，SGD 优化器中强制执行 DP 的核心机制一直未发生变化，这越来越成为限制 DP 合规机器学习解决方案性能的基本障碍。受此启发，我们提出了 DPIS，一种新颖的差分隐私SGD训练机制，可作为 DP-SGD 核心优化器的替代品，与后者相比具有一致且显著的准确性提升。主要思想是在每次 SGD 迭代中，使用重要性采样（IS）进行小批量选择，从而减少采样方差和满足 DP 所需注入梯度的随机噪声量。尽管在非隐私设置中，使用 IS 的 SGD 在机器学习文献中已被广泛研究，但将 IS 集成到 DP-SGD 的复杂数学机制中是非常重要的；此外，IS 还涉及到额外的私有数据发布，这些数据必须在差分隐私下受到保护，以及计算密集型梯度计算。DPIS 通过新颖的机制设计、细粒度的隐私分析、效率增强和自适应梯度裁剪优化来应对这些挑战。在四个基准数据集（即 MNIST、FMNIST、CIFAR-10 和 IMDb）上进行的大量实验，涉及卷积神经网络和循环神经网络，证明了 DPIS 在具有差分隐私的深度学习方面优于现有解决方案的有效性。

3. EIFFeL: Ensuring Integrity for Federated Learning<br>
联邦学习（FL）使客户端能够与服务器协作训练机器学习模型。为了确保隐私，服务器对来自客户端的更新进行安全聚合。不幸的是，由于更新被掩盖，这使得无法验证更新的完整性。因此，可以在不被检测的情况下注入旨在毒化模型的畸形更新。在本文中，我们将联邦学习中更新隐私和完整性问题进行形式化，并提出了一种新的系统，EIFFeL，它能够实现已验证更新的安全聚合。EIFFeL是一个通用框架，可以实施任意完整性检查，并从聚合中移除畸形更新，同时不违反隐私。我们的实证评估证明了EIFFeL的实用性。例如，在有100个客户端和10%毒化的情况下，每次迭代仅需2.4秒，EIFFeL就可以训练出与非毒化联邦学习器相同精度的MNIST分类模型。

4. Eluding Secure Aggregation in Federated Learning via Model Inconsistency<br>
安全聚合是一种加密协议，可以安全地计算其输入的聚合。在联邦学习中保护模型更新的隐私方面，它具有关键作用。实际上，使用安全聚合可以防止服务器了解用户提供的个体模型更新的值和来源，从而阻碍推理和数据归属攻击。在这项工作中，我们展示了恶意服务器可以轻松地规避安全聚合，就好像后者不存在一样。我们设计了两种不同的攻击方法，能够推断出关于个体私有训练数据集的信息，而不依赖于参与安全聚合的用户数量。这使得它们成为大规模、实际联邦学习应用中具体的威胁。这些攻击是通用的，无论使用哪种安全聚合协议，效果都是一样的。它们利用了联邦学习协议的一个漏洞，该漏洞是由于安全聚合的不正确使用和参数验证不足造成的。我们的工作表明，当前使用安全聚合的联邦学习实现仅提供一种“虚假的安全感”。

5. Federated Boosted Decision Trees with Differential Privacy<br>
针对可在分布式数据上进行训练的可扩展、安全、高效的隐私保护机器学习模型，需求巨大。尽管深度学习模型通常在集中式非安全环境中取得最佳效果，但在受到隐私和通信约束的情况下，不同的模型可能表现更好。因此，基于树的方法（如XGBoost）因其高性能和易用性而受到了广泛关注；特别是在表格数据上，它们通常能达到最先进的结果。因此，最近的一些研究关注于将像XGBoost这样的梯度提升决策树（GBDT）模型，通过同态加密（HE）和安全多方计算（MPC）等加密机制，转化为联邦设置。然而，这些方法并不总是提供正式的隐私保证，或者考虑全部超参数和实现设置。在这项工作中，我们在差分隐私（DP）下实现了GBDT模型。我们提出了一个捕捉并扩展现有差分隐私决策树方法的通用框架。我们的方法框架针对联邦环境进行了定制，我们展示了通过谨慎选择技术，可以在保持高度隐私的同时实现非常高的效用。


