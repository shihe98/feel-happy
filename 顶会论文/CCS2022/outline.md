# 2022 CCS 关注内容
## 人工智能的特性
1. is your explanation stable? a robustness evaluation framework for feature attribution.<br>
*关键点*：用特征归因理解神经网络的可解释性，即某个决策和关键特征有关系。当前大部分算法旨在提升模型的忠实度。<br>
*问题*：现实环境中存在许多随机噪声，会干扰图像分类任务的特征归因；解释算法容易受到对抗性攻击，为恶意干扰的输入生成相同的解释。<br>
*方案*：提出了用于特征归因的中值检验（MeTFA），在理论上量化不确定性并提高解释算法的稳定性，适用于任何特征归因方法。<br>
*功能*：（1）检查一个特征重要或不重要，并生成显著映射以可视化结果。（2）计算特征归因分数的置信区间，并生成平滑映射以提高解释的稳定性。<br>
*优点*：（1）改善了解释的视觉质量，显著降低了不稳定性，同时保持了原始方法的忠实度。**[万金油强化]**（2）两个典型应用来展示MeTFA在应用中的潜力。**[定量评估忠实度与防御效果]**
2. AI/ML for Network Security: The Emperor has no Clothes
*问题*：基于机器学习（ML）的网络流量检测方案过于黑盒，网络运营商不愿信任并将部署它们。因为这些模型容易出现欠规范化问题（**未能以足够详细的方式指定模型**），导致模型表现出意想不到的糟糕行为。<br>
*痛点*：开发可解释的ML解决方案（例如决策树），帮助解释黑盒模型的决策过程。合成高保真度又易于理解的可解释模型是具有挑战性的。<br>
*方案*：合成高保真度和低复杂度的决策树，帮助网络运营商判断他们的ML模型是否欠缺规范。Trustee将现有ML模型和训练数据集作为输入，生成高保真度、易于解释的决策树以及相关信任报告作为输出。
*优点*：利用已发布的、完全可复制的ML模型，来识别模型欠规范化的三个常见实例，即快捷学习的证据、伪相关性的存在以及对分布外异常样本的脆弱性。
*PS*：把人工智能的规范化作为整体的任务，利用决策树做一个模型级别的检测。作者直接把赛道提高一个维度，属于首发性工作。

## 人工智能&隐私保护
1. Am I Private and If So, how Many?: Communicating Privacy Guarantees of Differential Privacy with Risk Communication Formats
