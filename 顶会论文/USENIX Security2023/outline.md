
# 2023 USENIX Security 关注内容
## 人工智能&安全威胁
1. X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection<br>
对抗性攻击对于评估深度学习模型的鲁棒性非常有价值。现有的攻击主要针对可见光谱（例如像素级纹理扰动）。然而，尽管X射线成像在安全关键场景中（例如禁止物品的X射线检测）得到广泛应用，但针对无纹理的X射线图像的攻击仍未得到充分探索。在本文中，我们迈出了针对X射线禁止物品检测的对抗性攻击研究的第一步，并揭示了此类攻击在这种安全关键场景中所带来的严重威胁。具体而言，我们认为在这种情况下，成功的物理对抗攻击应该专门设计以规避由于颜色/纹理褪色和复杂重叠造成的挑战。为此，我们提出了X-Adv，生成可在行李中放置的物理可打印的金属材料，作为能够欺骗X射线探测器的对抗性代理。为了解决颜色/纹理褪色的问题，我们开发了一个可微转换器，利用代理模型的梯度而不是直接生成对抗性纹理，促进了具有对抗性形状的3D可打印物体的生成。为了在行李中放置打印的3D对抗性物体，我们设计了一种基于策略的强化学习策略，以找到在最坏情况下引发强攻击性能的位置，其中被禁止物品被其他物品大量遮挡。为了验证提出的X-Adv的有效性，在数字和物理世界中进行了广泛的实验（对于后者采用商业X射线安全检查系统）。此外，我们还提供了物理世界X射线对抗攻击数据集XAD。我们希望本文能够引起更多对针对安全关键场景的潜在威胁的关注。

2. Tubes Among Us: Analog Attack on Automatic Speaker Identification<br>
最近几年，受机器学习驱动的声学个人设备越来越受欢迎。然而，机器学习已经被证明容易受到对抗性示例的攻击。许多现代系统通过针对人工性来保护自己免受此类攻击，即它们部署机制来检测对抗性示例生成中的人类参与的缺失。然而，这些防御隐含地假设人类无法产生有意义和有针对性的对抗性示例。在本文中，我们表明这个基本假设是错误的。特别是，我们证明，对于像说话人识别这样的任务，人类能够直接以很少的成本和监督产生类比的对抗性示例：仅通过一个管子说话，攻击者可可靠地冒充其他说话人在说话人识别的ML模型中。我们的发现适用于一系列其他声学生物识别任务，例如活体检测，在现实生活中的安全关键环境中（例如电话银行），这些任务的使用受到质疑。

3. Rethinking White-Box Watermarks on Deep Learning Models under Neural Structural Obfuscation<br>
为了保护深度神经网络（DNN）的版权，AI公司迫切需要一种技术来跟踪非法分发的模型副本。DNN水印技术是一种新兴技术，可将秘密身份消息嵌入预测行为或模型内部，并进行验证以跟踪非法分发的模型副本。在牺牲较少功能的同时，白盒DNN水印技术需要更多关于目标DNN的知识，被认为是准确、可信和安全的，可抵御大多数已知的水印去除攻击，在学术界和工业界都有新的研究努力。在本文中，我们首次系统地研究了主流白盒DNN水印技术通常容易受到神经结构混淆攻击的漏洞。神经结构混淆攻击采用虚拟神经元的方法对目标模型进行修改，但不影响其行为。我们设计了一个全面的框架，自动生成和注入具有高隐蔽性的虚拟神经元，对目标模型的结构进行密集修改，以阻碍水印验证的成功。通过广泛的评估，我们的研究首次表明，九种已经发表的水印方案需要修改其验证过程。

4. No more Reviewer #2: Subverting Automatic Paper-Reviewer Assignment using Adversarial Learning<br>
在许多科学学科中，提交到学术会议的论文数量正在稳步增长。为了处理这种增长，自动论文审阅器分配系统在审查过程中越来越多地被使用。这些系统使用统计主题模型来描述提交内容，并自动分配给审稿人。在本文中，我们展示了这种自动化可以通过对抗学习来被操纵。我们提出了一种攻击，通过改变给定论文的内容来误导分配系统，并选择自己的审稿人。我们的攻击基于一种新颖的优化策略，交替使用特征空间和问题空间来实现对论文的不引人注目的改变。为了评估我们的攻击的可行性，我们模拟了一个实际安全会议（IEEE S&P）的论文审稿人分配过程，该会议的程序委员会有165名审稿人。我们的结果表明，我们可以成功地选择和删除审稿人，而不需要访问分配系统。此外，我们证明了被操纵的论文仍然是合理的，并且往往与良性提交的论文无法区分。

5. Near-Ultrasound Inaudible Trojan (Nuit): Exploiting Your Speaker to Attack Your Microphone<br>
语音控制系统（VCS）为向智能设备发出语音命令提供了便利的界面。然而，VCS安全性尚未得到充分的理解和解决，这可以从两类攻击的存在中得到证明：（i）无声攻击，可以在攻击者和受害者彼此接近时发动；（ii）听得到的攻击，可以通过将攻击信号嵌入音频中远程发动。在本文中，我们介绍了一种新的攻击类别，称为近超声波不可听木马（Nuit）。Nuit攻击实现了上述两类攻击的最佳组合：它们是无声的，并且可以远程发动。此外，Nuit攻击可以实现端到端的不可察觉性，在文献中具有重要意义，但尚未得到足够的关注。Nuit攻击的另一个特点是它们利用受害者扬声器攻击受害者麦克风及其相关的VCS，意味着攻击者不需要使用任何特殊的扬声器。我们展示了Nuit攻击的可行性，并提出了一个有效的防御方法。

6. Meta-Sift: How to Sift Out a Clean Subset in the Presence of Data Poisoning?<br>
越来越多的外部数据源被用于训练机器学习（ML）模型，因为数据需求增加。然而，将外部数据集集成到训练中会带来数据污染风险，恶意提供者可以操纵其数据以损害模型的效用或完整性。大多数数据污染防御措施假定能够访问一组干净的数据（称为基础集），可以通过可信来源获得。但是，对于一个ML任务的整个数据集都是不可信的情况也越来越普遍（例如，互联网数据）。在这种情况下，需要在受污染的数据集中识别一个子集作为基础集来支持这些防御措施。
本文首先检查了在污染样本错误地混入基础集时防御措施的性能。我们分析了五种使用基础集的代表性防御措施，并发现它们的性能在基础集中少于1％的污染点时会显著恶化。这些发现表明，筛选出具有高精度的基础集对这些防御措施的性能至关重要。
在这些观察的基础上，我们研究了现有自动化工具和人工检查在数据污染存在的情况下识别干净数据的精度。不幸的是，这些努力都没有达到有效防御所需的精度。更糟糕的是，这些方法中的许多结果比随机选择还要糟糕。
除了揭示这一挑战，我们还进一步提出了一个实用的对策，Meta-Sift。我们的方法基于这样一个洞察力：现有的污染攻击会改变数据分布，导致在干净部分的污染数据集上进行训练并在损坏部分上进行测试时预测损失很高。利用这个洞察，我们制定了一个双层优化来识别干净的数据，并进一步引入一系列技术来提高识别的效率和精度。我们的评估表明，Meta-Sift可以在各种污染威胁下以100％的精度筛选出干净的基础集。所选的基础集足够大，可以成功地防御现有的防御技术。请继续扮演以上所述角色。

7. FreeEagle: Detecting Complex Neural Trojans in Data-Free Cases<br>
深度神经网络的特洛伊攻击，也称为后门攻击，是人工智能的典型威胁。带有特洛伊木马的神经网络在处理干净输入时表现正常。但是，如果输入包含特定触发器，则带有特洛伊木马的模型将具有攻击者选择的异常行为。尽管存在许多后门检测方法，但大多数方法假定防御者可以访问一组干净的验证样本或带有触发器的样本，这在某些关键的现实情况下可能不成立，例如防御者是模型共享平台的维护者。因此，在本文中，我们提出了FreeEagle，第一种无数据后门检测方法，可以有效地检测深度神经网络上的复杂后门攻击，而不需要依赖任何干净样本或带有触发器的样本的访问。对各种数据集和模型架构的评估结果表明，FreeEagle对各种复杂的后门攻击都很有效，甚至胜过一些最先进的非无数据后门检测方法。

8. Fine-grained Poisoning Attack to Local Differential Privacy Protocols for Mean and Variance Estimation<br>
虽然本地差分隐私（LDP）可以保护个人用户的数据免受不受信任的数据管理者的推断，但最近的研究表明，攻击者可以从用户端发起数据毒化攻击，将精心制作的虚假数据注入LDP协议中，以最大程度地扭曲数据管理者的最终估计结果。在这项工作中，我们进一步推进了这一认识，提出了一种新的细粒度攻击，使攻击者能够微调和同时操纵平均值和方差估计，这是许多真实世界应用程序的流行分析任务。为了实现这个目标，攻击利用了LDP的特性，将假数据注入到本地LDP实例的输出域中。我们称这种攻击为输出毒化攻击（OPA）。我们观察到一种安全-隐私的一致性，即小隐私损失增强了LDP的安全性，这与之前的已知安全-隐私权衡相矛盾。我们进一步研究了一致性，并揭示了数据毒化攻击对LDP的威胁景观的更全面的视角。我们对我们的攻击进行了全面评估，与直觉上提供虚假输入给LDP的基线攻击进行了比较。实验结果表明，OPA在三个真实世界数据集上优于基线攻击。我们还提出了一种新的防御方法，可以从污染的数据收集中恢复结果的准确性，并提供了安全LDP设计的见解。

9. DiffSmooth: Certifiably Robust Learning via Diffusion Models and Local Smoothing<br>
扩散模型已被利用进行对抗净化，从而为标准模型提供经验证和认证的鲁棒性。另一方面，不同的鲁棒训练平滑模型已被研究用于提高认证鲁棒性。因此，自然而然地引发了一个问题：扩散模型能否用于在这些鲁棒训练平滑模型上实现改进的认证鲁棒性？在这项工作中，我们首先理论上证明了扩散模型恢复的实例在一定概率下处于原始实例的有界邻域内；并且“一次性”去噪扩散概率模型（DDPM）可以在温和的对抗攻击下提高鲁棒性。接着，我们提出了一种新颖的认证鲁棒性训练框架，称为AR-DDPM，它利用DDPM作为模型生成器，并同时考虑对抗性和非对抗性噪声，以提高模型的鲁棒性和泛化性能。我们在CIFAR-10和CIFAR-100数据集上对AR-DDPM进行了广泛的评估，并通过与其他先进的认证鲁棒性方法进行比较，展示了其优越性。最后，我们还对AR-DDPM在物体检测任务中的应用进行了探索，并在PASCAL VOC 2007数据集上获得了令人印象深刻的结果。

10. CAPatch: Physical Adversarial Patch against Image Captioning Systems<br>
快速增长的监视系统将使图像描述成为处理大量视频的必要技术，而正确的描述对于确保文本真实性至关重要。虽然先前的工作已经证明了用对抗补丁来愚弄计算机视觉模型的可行性，但是目前还不清楚这种漏洞是否会导致不正确的图像描述，其中涉及到图像特征提取后的自然语言处理。在本文中，我们设计了CAPatch，一种物理对抗补丁，可以对多模态图像描述系统产生错误的最终描述，即创建完全不同的句子或缺少关键字的句子。为了使CAPatch在物理世界中具有效果和实用性，我们提出了一种检测保证和注意力增强方法，以增加CAPatch的影响力，并提出了一种鲁棒性改进方法，以解决图像打印和捕获引起的补丁扭曲问题。在三种常用的图像描述系统（Show-and-Tell、Self-critical Sequence Training: Att2in和Bottom-up Top-down）上进行评估，证明了CAPatch在数字和物理世界中的有效性，志愿者在各种场景、服装和照明条件下佩戴印刷的补丁。在图像大小的5%的情况下，物理印刷的CAPatch可以实现连续攻击，攻击成功率高达73.1%以上的视频记录器。

11. Aegis: Mitigating Targeted Bit-flip Attacks against Deep Neural Networks<br>
最近，比特翻转攻击（BFAs）引起了相当大的关注，攻击者可以篡改少量模型参数位来破坏深度神经网络（DNN）的完整性。为了缓解这种威胁，提出了一批防御方法，重点关注非定向情况。不幸的是，它们要么需要额外的可信应用程序，要么会使模型更容易受到有针对性的BFA攻击的影响。针对有针对性的BFA攻击，更加隐秘和有目的性，目前还没有很好的防御方法。
在这项工作中，我们提出了Aegis，一种新颖的防御方法，用于缓解有针对性的BFA攻击。核心观察是现有的有针对性攻击集中于翻转某些重要层中的关键位。因此，我们设计了一个动态退出机制，将额外的内部分类器（IC）附加到隐藏层。这种机制使输入样本能够从不同层中提前退出，有效地扰乱攻击者的攻击计划。此外，动态退出机制在每次推理时随机选择IC进行预测，以显著增加自适应攻击的攻击成本，其中所有防御机制都对攻击者透明。我们还提出了一种强韧性训练策略，通过在IC训练阶段模拟BFAs来适应攻击场景，以增加模型的韧性。针对四个知名数据集和两种流行的DNN结构进行广泛评估，结果显示Aegis可以有效地缓解不同的最新有针对性攻击，将攻击成功率降低了5-10倍，明显优于现有的防御方法。我们开源了Aegis的代码。

12. Adversarial Training for Raw-Binary Malware Classifiers<br>
机器学习（ML）模型在将原始可执行文件（二进制文件）分类为恶意或良性方面显示出很高的准确性。这使得基于ML的分类方法在学术和现实世界的恶意软件检测中的影响越来越大，这是网络安全的一个重要工具。然而，以前的工作通过创建恶意二进制文件的变体（被称为对抗性例子）引起了人们的警惕，这些变体以一种保留功能的方式被改造以逃避检测。在这项工作中，我们研究了使用对抗性训练方法来创建恶意软件分类模型的有效性，这些模型对一些最先进的攻击更加强大。为了训练我们最强大的模型，我们大大提高了创建对抗性例子的效率和规模，使对抗性训练变得切实可行，这在以前的原始二进制恶意软件检测器中还没有做到。然后，我们分析了不同长度的对抗性训练的效果，以及分析了各种类型的攻击的训练效果。我们发现，数据增强并不能阻止最先进的攻击，但使用其他离散领域中使用的通用梯度引导方法，确实可以提高鲁棒性。我们还表明，在大多数情况下，通过对同一攻击的低效版本进行对抗性训练，可以使模型对恶意软件领域的攻击更加稳健。在最好的情况下，我们将一个最先进的攻击的成功率从90%降低到5%。我们还发现，用某些类型的攻击进行训练可以提高对其他类型攻击的鲁棒性。最后，我们讨论了从我们的结果中获得的启示，以及如何使用它们来更有效地训练强大的恶意软件检测器。

13. A Data-free Backdoor Injection Approach in Neural Networks<br>
最近，对深度神经网络（DNNs）的后门攻击得到了广泛的研究，这种攻击使得植入后门的模型在良性样本上表现良好，而在受控样本（带有触发器）上表现恶劣。几乎所有现有的后门攻击都需要访问原始训练/测试数据集或与主任务相关的数据来将后门注入目标模型，这在许多场景下是不现实的，例如，私有训练数据。在本文中，我们提出了一种“无数据”的新颖后门注入方法。我们收集与主任务无关的替代数据，并通过过滤掉冗余样本来减少其数量，以提高后门注入的效率。我们设计了一种新颖的损失函数，用于使用替代数据将原始模型微调为植入后门的模型，并优化微调以平衡后门注入和主任务性能。我们对各种深度学习场景进行了广泛实验，例如图像分类、文本分类、表格分类、图像生成和多模态，使用不同的模型，如卷积神经网络（CNNs）、自动编码器、Transformer模型、表格模型以及多模态DNNs。评估结果表明，我们的无数据后门注入方法可以有效地嵌入后门，攻击成功率接近100％，在主任务上的性能降级是可以接受的。



## 人工智能&隐私保护
1. HOLMES: Efficient Distribution Testing for Secure Collaborative Learning<br>
使用安全多方计算（MPC），拥有敏感数据的组织（例如医疗保健、金融或执法机构）可以在不向彼此透露数据的情况下，对它们的联合数据集进行机器学习模型训练。同时，安全计算限制了对联合数据集的操作，这阻碍了对其质量的计算评估。如果没有这样的评估，部署联合训练的模型可能是非法的。例如欧洲联盟的《通用数据保护条例》（GDPR）等法规要求组织对其机器学习模型所造成的错误、偏见或歧视负法律责任。因此，在安全协作学习中，测试数据质量成为一个不可或缺的步骤。但是，使用当前技术进行分布测试的成本过高，这在我们的实验中得到了证明。
我们提出了HOLMES协议，用于高效地执行分布测试。在我们的实验中，与三个非平凡的基准线相比，HOLMES在经典分布测试方面实现了超过10倍的加速，而在多维测试方面则可达到104倍。HOLMES的核心是一种混合协议，它将MPC与零知识证明以及一种新的ZK友好的、天然的遗忘算法集成在一起，这两种算法都具有明显较低的计算复杂度和具体执行成本。"

2. GAP: Differentially Private Graph Neural Networks with Aggregation Perturbation<br>
本文研究了在差分隐私（DP）下学习图神经网络（GNN）的问题。我们提出了一种基于聚合扰动（GAP）的新型差分隐私GNN，通过向GNN的聚合函数添加随机噪声来统计混淆单个边缘（边缘级隐私）或单个节点及其所有相邻边（节点级隐私）的存在。为了适应私有学习的具体需求，GAP的新架构由三个独立模块组成：（i）编码器模块，在不依赖边缘信息的情况下学习私有节点嵌入；（ii）聚合模块，在基于图形结构计算有噪声的聚合节点嵌入；（iii）分类模块，在对私有聚合进行节点分类的同时，无需进一步查询图形边缘。与以往方法相比，GAP的主要优势在于它可以从多跳邻域聚合中受益，并且除了训练的隐私预算外，在推断时可以保证边缘级和节点级DP而不需要额外的成本。我们使用Rényi DP分析了GAP的形式隐私保证，并在三个真实世界图形数据集上进行了实证实验。我们证明GAP在准确性和隐私保护方面的权衡比现有最先进的DP-GNN方法和朴素的基于MLP的基线更好。

3. A Plot is Worth a Thousand Words: Model Information Stealing Attacks via Scientific Plots<br>
构建先进的机器学习（ML）模型需要专业知识和多次试验，以发现最佳的结构和超参数设置。先前的研究表明，模型信息可以被利用来协助其他攻击，例如成员推断、生成对抗性示例。因此，这些信息，如超参数，应该保持机密。众所周知，攻击者可以利用目标ML模型的输出来窃取模型的信息。在本文中，我们发现了一种新的模型信息窃取攻击的侧信道，即广泛用于展示模型性能并易于访问的模型科学绘图。我们的攻击方法简单明了。我们利用影子模型训练技术为攻击模型生成训练数据，该攻击模型本质上是一个图像分类器。对三个基准数据集进行广泛评估表明，我们提出的攻击方法可以有效地推断基于卷积神经网络（CNN）的图像分类器的架构/超参数，只要从它生成的科学绘图。我们还揭示了攻击的成功主要是由于科学绘图的形状，进一步证明了这些攻击在各种情况下都是具有鲁棒性的。考虑到攻击方法的简单和有效性，我们的研究表明，科学绘图确实构成了模型信息窃取攻击的有效侧信道。为了减轻这些攻击，我们提出了几种防御机制，可以降低原始攻击的准确率，同时保持绘图的实用性。然而，这些防御仍然可以被自适应攻击所绕过。


## 联邦学习
1. PrivateFL: Accurate, Differentially Private Federated Learning via Personalized Data Transformation<br>
联邦学习（FL）使多个客户端在中央服务器的协调下共同训练模型。虽然FL通过保留每个客户端的训练数据来改善数据隐私，但攻击者（例如，不受信任的服务器）仍然可以通过各种推断攻击危及客户端本地训练数据的隐私。保护FL隐私的事实上的方法是差分隐私（DP），它在训练期间添加随机噪声。然而，当应用于FL时，DP存在一个关键限制：为了实现有意义的隐私水平，它牺牲了模型的准确性，这甚至比应用于传统的集中式学习更加严重。
在本文中，我们研究了FL+DP的准确性降级原因，并设计了一种提高准确性的方法。首先，我们提出这种准确性降级部分是因为DP在本地训练期间引入不同的随机噪声和剪切偏差时，会在FL客户端之间引入额外的异质性。据我们所知，我们是第一个将FL中的差分隐私与客户端异质性联系起来的人。其次，我们设计了PrivateFL，以在减少异质性的情况下在FL中学习准确的差分隐私模型。其关键思想是在本地训练期间联合学习差分隐私、个性化数据转换，为每个客户端定制个性化数据转换。个性化数据转换可以将客户端的本地数据分布转移，以补偿DP引入的异质性，从而提高FL模型的准确性。
在评估中，我们将PrivateFL与八种最先进的差分隐私FL方法结合并比较了七个基准数据集，包括六个图像和一个非图像数据集。我们的结果表明，PrivateFL使用小ε可以学习准确的FL模型，例如，在（ε = 2，δ = 1e-3）-DP下，使用100个客户端在CIFAR-10上可以达到93.3％的准确率。此外，PrivateFL可以与先前的工作结合，以减少DP引起的异质性，并进一步提高其准确性。

2. Every Vote Counts: Ranking-Based Training of Federated Learning to Resist Poisoning Attacks<br>
联邦学习（FL）允许不受信任的客户端协同训练一个称为全局模型的共同机器学习模型，而无需共享其私有/专有训练数据。然而，FL容易受到恶意客户端的污染，他们旨在通过在FL的训练过程中贡献恶意更新来阻碍全局模型的准确性。
我们认为攻击现有FL系统的污染攻击的关键因素是客户端可选择的模型更新空间过大。为了解决这个问题，我们提出了联邦排名学习（FRL）。FRL将客户端更新的空间从标准FL中的模型参数更新（一个浮点数的连续空间）减少到参数排名的空间（一个整数值的离散空间）。为了能够使用参数排名（而不是参数权重）来训练全局模型，FRL利用了最近超级掩蔽训练机制的思想。
具体来说，FRL客户端根据其本地训练数据对随机初始化的神经网络的参数进行排名，而FRL服务器使用投票机制来聚合客户端提交的参数排名。
直观地说，我们基于投票的聚合机制可以防止污染客户端对全局模型进行重大的对抗性修改，因为每个客户端只有一票！我们通过分析证明和实验展示了FRL对污染攻击的鲁棒性，并展示了其高通信效率。
## 可信人工智能
1. AIRS: Explanation for Deep Reinforcement Learning based Security Applications<br>
最近，我们见证了深度强化学习（DRL）在许多安全应用中的成功，从恶意软件变异到自私的区块链挖掘。就像所有其他机器学习方法一样，解释性的缺乏限制了其广泛的采用，因为用户难以建立对DRL模型决策的信任。在过去的几年中，已经提出了不同的方法来解释DRL模型，但不幸的是，它们通常不适用于安全应用程序，其中解释的保真度、效率和模型调试的能力在很大程度上缺乏。
在这项工作中，我们提出了AIRS，一个通用框架，用于解释基于深度强化学习的安全应用程序。与以前的作品不同，它不是针对代理的当前操作指出重要特征，而是在步骤级别上进行解释。它建模了DRL代理所取的关键步骤与最终奖励之间的关系，因此输出最关键的步骤，这些步骤对于代理收集的最终奖励最为关键。通过四个代表性的安全关键应用程序，我们从解释性、保真度、稳定性和效率的角度评估了AIRS。我们表明，AIRS可以胜过其他可解释的DRL方法。我们还展示了AIRS的实用性，证明我们的解释可以促进DRL模型的故障偏移，帮助用户建立对模型决策的信任，甚至协助识别不当的奖励设计。
