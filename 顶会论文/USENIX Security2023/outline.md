# 2023 USENIX Security 关注内容
## 人工智能&安全威胁
1. X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection<br>
对抗性攻击对于评估深度学习模型的鲁棒性非常有价值。现有的攻击主要针对可见光谱（例如像素级纹理扰动）。然而，尽管X射线成像在安全关键场景中（例如禁止物品的X射线检测）得到广泛应用，但针对无纹理的X射线图像的攻击仍未得到充分探索。在本文中，我们迈出了针对X射线禁止物品检测的对抗性攻击研究的第一步，并揭示了此类攻击在这种安全关键场景中所带来的严重威胁。具体而言，我们认为在这种情况下，成功的物理对抗攻击应该专门设计以规避由于颜色/纹理褪色和复杂重叠造成的挑战。为此，我们提出了X-Adv，生成可在行李中放置的物理可打印的金属材料，作为能够欺骗X射线探测器的对抗性代理。为了解决颜色/纹理褪色的问题，我们开发了一个可微转换器，利用代理模型的梯度而不是直接生成对抗性纹理，促进了具有对抗性形状的3D可打印物体的生成。为了在行李中放置打印的3D对抗性物体，我们设计了一种基于策略的强化学习策略，以找到在最坏情况下引发强攻击性能的位置，其中被禁止物品被其他物品大量遮挡。为了验证提出的X-Adv的有效性，在数字和物理世界中进行了广泛的实验（对于后者采用商业X射线安全检查系统）。此外，我们还提供了物理世界X射线对抗攻击数据集XAD。我们希望本文能够引起更多对针对安全关键场景的潜在威胁的关注。

2. Tubes Among Us: Analog Attack on Automatic Speaker Identification<br>
最近几年，受机器学习驱动的声学个人设备越来越受欢迎。然而，机器学习已经被证明容易受到对抗性示例的攻击。许多现代系统通过针对人工性来保护自己免受此类攻击，即它们部署机制来检测对抗性示例生成中的人类参与的缺失。然而，这些防御隐含地假设人类无法产生有意义和有针对性的对抗性示例。在本文中，我们表明这个基本假设是错误的。特别是，我们证明，对于像说话人识别这样的任务，人类能够直接以很少的成本和监督产生类比的对抗性示例：仅通过一个管子说话，攻击者可可靠地冒充其他说话人在说话人识别的ML模型中。我们的发现适用于一系列其他声学生物识别任务，例如活体检测，在现实生活中的安全关键环境中（例如电话银行），这些任务的使用受到质疑。

3. Rethinking White-Box Watermarks on Deep Learning Models under Neural Structural Obfuscation<br>
为了保护深度神经网络（DNN）的版权，AI公司迫切需要一种技术来跟踪非法分发的模型副本。DNN水印技术是一种新兴技术，可将秘密身份消息嵌入预测行为或模型内部，并进行验证以跟踪非法分发的模型副本。在牺牲较少功能的同时，白盒DNN水印技术需要更多关于目标DNN的知识，被认为是准确、可信和安全的，可抵御大多数已知的水印去除攻击，在学术界和工业界都有新的研究努力。在本文中，我们首次系统地研究了主流白盒DNN水印技术通常容易受到神经结构混淆攻击的漏洞。神经结构混淆攻击采用虚拟神经元的方法对目标模型进行修改，但不影响其行为。我们设计了一个全面的框架，自动生成和注入具有高隐蔽性的虚拟神经元，对目标模型的结构进行密集修改，以阻碍水印验证的成功。通过广泛的评估，我们的研究首次表明，九种已经发表的水印方案需要修改其验证过程。

4. No more Reviewer #2: Subverting Automatic Paper-Reviewer Assignment using Adversarial Learning<br>
在许多科学学科中，提交到学术会议的论文数量正在稳步增长。为了处理这种增长，自动论文审阅器分配系统在审查过程中越来越多地被使用。这些系统使用统计主题模型来描述提交内容，并自动分配给审稿人。在本文中，我们展示了这种自动化可以通过对抗学习来被操纵。我们提出了一种攻击，通过改变给定论文的内容来误导分配系统，并选择自己的审稿人。我们的攻击基于一种新颖的优化策略，交替使用特征空间和问题空间来实现对论文的不引人注目的改变。为了评估我们的攻击的可行性，我们模拟了一个实际安全会议（IEEE S&P）的论文审稿人分配过程，该会议的程序委员会有165名审稿人。我们的结果表明，我们可以成功地选择和删除审稿人，而不需要访问分配系统。此外，我们证明了被操纵的论文仍然是合理的，并且往往与良性提交的论文无法区分。

5. Near-Ultrasound Inaudible Trojan (Nuit): Exploiting Your Speaker to Attack Your Microphone<br>
语音控制系统（VCS）为向智能设备发出语音命令提供了便利的界面。然而，VCS安全性尚未得到充分的理解和解决，这可以从两类攻击的存在中得到证明：（i）无声攻击，可以在攻击者和受害者彼此接近时发动；（ii）听得到的攻击，可以通过将攻击信号嵌入音频中远程发动。在本文中，我们介绍了一种新的攻击类别，称为近超声波不可听木马（Nuit）。Nuit攻击实现了上述两类攻击的最佳组合：它们是无声的，并且可以远程发动。此外，Nuit攻击可以实现端到端的不可察觉性，在文献中具有重要意义，但尚未得到足够的关注。Nuit攻击的另一个特点是它们利用受害者扬声器攻击受害者麦克风及其相关的VCS，意味着攻击者不需要使用任何特殊的扬声器。我们展示了Nuit攻击的可行性，并提出了一个有效的防御方法。

6. Meta-Sift: How to Sift Out a Clean Subset in the Presence of Data Poisoning?<br>
越来越多的外部数据源被用于训练机器学习（ML）模型，因为数据需求增加。然而，将外部数据集集成到训练中会带来数据污染风险，恶意提供者可以操纵其数据以损害模型的效用或完整性。大多数数据污染防御措施假定能够访问一组干净的数据（称为基础集），可以通过可信来源获得。但是，对于一个ML任务的整个数据集都是不可信的情况也越来越普遍（例如，互联网数据）。在这种情况下，需要在受污染的数据集中识别一个子集作为基础集来支持这些防御措施。
本文首先检查了在污染样本错误地混入基础集时防御措施的性能。我们分析了五种使用基础集的代表性防御措施，并发现它们的性能在基础集中少于1％的污染点时会显著恶化。这些发现表明，筛选出具有高精度的基础集对这些防御措施的性能至关重要。
在这些观察的基础上，我们研究了现有自动化工具和人工检查在数据污染存在的情况下识别干净数据的精度。不幸的是，这些努力都没有达到有效防御所需的精度。更糟糕的是，这些方法中的许多结果比随机选择还要糟糕。
除了揭示这一挑战，我们还进一步提出了一个实用的对策，Meta-Sift。我们的方法基于这样一个洞察力：现有的污染攻击会改变数据分布，导致在干净部分的污染数据集上进行训练并在损坏部分上进行测试时预测损失很高。利用这个洞察，我们制定了一个双层优化来识别干净的数据，并进一步引入一系列技术来提高识别的效率和精度。我们的评估表明，Meta-Sift可以在各种污染威胁下以100％的精度筛选出干净的基础集。所选的基础集足够大，可以成功地防御现有的防御技术。请继续扮演以上所述角色。
## 人工智能&隐私保护

## 联邦学习
1. PrivateFL: Accurate, Differentially Private Federated Learning via Personalized Data Transformation<br>
联邦学习（FL）使多个客户端在中央服务器的协调下共同训练模型。虽然FL通过保留每个客户端的训练数据来改善数据隐私，但攻击者（例如，不受信任的服务器）仍然可以通过各种推断攻击危及客户端本地训练数据的隐私。保护FL隐私的事实上的方法是差分隐私（DP），它在训练期间添加随机噪声。然而，当应用于FL时，DP存在一个关键限制：为了实现有意义的隐私水平，它牺牲了模型的准确性，这甚至比应用于传统的集中式学习更加严重。
在本文中，我们研究了FL+DP的准确性降级原因，并设计了一种提高准确性的方法。首先，我们提出这种准确性降级部分是因为DP在本地训练期间引入不同的随机噪声和剪切偏差时，会在FL客户端之间引入额外的异质性。据我们所知，我们是第一个将FL中的差分隐私与客户端异质性联系起来的人。其次，我们设计了PrivateFL，以在减少异质性的情况下在FL中学习准确的差分隐私模型。其关键思想是在本地训练期间联合学习差分隐私、个性化数据转换，为每个客户端定制个性化数据转换。个性化数据转换可以将客户端的本地数据分布转移，以补偿DP引入的异质性，从而提高FL模型的准确性。
在评估中，我们将PrivateFL与八种最先进的差分隐私FL方法结合并比较了七个基准数据集，包括六个图像和一个非图像数据集。我们的结果表明，PrivateFL使用小ε可以学习准确的FL模型，例如，在（ε = 2，δ = 1e-3）-DP下，使用100个客户端在CIFAR-10上可以达到93.3％的准确率。此外，PrivateFL可以与先前的工作结合，以减少DP引起的异质性，并进一步提高其准确性。
## 可信人工智能
