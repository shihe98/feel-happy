
# 2023 USENIX Security 关注内容
## 人工智能&安全威胁
1. X-Adv: Physical Adversarial Object Attacks against X-ray Prohibited Item Detection<br>
*问题*：现有的对抗攻击主要针对可见光谱（例如像素级纹理扰动），但针对无纹理的X射线图像的攻击仍未得到充分探索。【针对X射线禁止物品检测的对抗性攻击】<br>
*关键点*：成功的物理对抗攻击应该专门设计以规避由于颜色/纹理褪色和复杂重叠造成的挑战。<br>
*方案*：提出X-Adv，生成可在行李中放置的物理可打印的金属材料，作为能够欺骗X射线探测器的对抗性代理。（1）为了解决颜色/纹理褪色的问题，开发了一个可微转换器，利用代理模型的梯度而不是直接生成对抗性纹理，生成具有对抗性形状的3D可打印物体。（2）利用强化学习找到在最坏情况下引发强攻击性能的位置，其中被禁止物品被其他物品大量遮挡。<br>
*优点*：在数字和物理世界中进行了广泛的实验，证明X-Adv的有效性。<br>
*PS*：针对物理世界安检场景进行对抗攻击研究，impact很大，限制也很多。**尽量扩大研究的现实影响**<br>

2. Tubes Among Us: Analog Attack on Automatic Speaker Identification<br>
*问题*：目前，许多现代系统通过针对人工性来保护自己免受对抗攻击，即它们根据人类参与的缺失来检测对抗性示例。这些防御隐含地假设人类无法产生有意义和有针对性的对抗性示例。【本文表明这个基本假设存在错误】<br>
*结论*：对于说话人识任务，人类能够直接以很少的成本和监督产生类比的对抗性示例。【仅通过一个管子说话，攻击者可冒充其他说话人】对于一系列其他声学生物识别任务，例如活体检测，在现实生活中的安全关键环境中（例如电话银行）存在安全隐患。<br>
*PS*：针对声学生物识别任务进行对抗攻击研究，impact很大。**打破固有的防御假设，即逃逸[主流的]防御**<br>

3. Rethinking White-Box Watermarks on Deep Learning Models under Neural Structural Obfuscation<br>
*问题*：DNN水印技术可将秘密身份消息嵌入预测行为或模型内部，并进行验证以跟踪非法分发的模型副本。【常用白盒DNN水印技术】<br>
*方案*：本文首次系统地研究了主流白盒DNN水印技术，容易受到神经结构混淆攻击。它采用虚拟神经元的方法对目标模型进行修改，但不影响其行为。此方案自动生成和注入具有高隐蔽性的虚拟神经元，对目标模型的结构进行密集修改，以阻碍水印验证的成功。<br>
*优点*：九种已经发表的水印方案无法奏效。<br>
*PS*：深度学习模型水印的应用，属于另辟蹊径的工作。**针对白盒水印**<br>

4. No more Reviewer #2: Subverting Automatic Paper-Reviewer Assignment using Adversarial Learning<br>
*问题*：自动论文审阅器分配系统使用统计主题模型来描述提交内容，并自动分配给审稿人。这种自动化可以通过对抗学习来被操纵。<br>
*方案*：改变论文的内容来误导分配系统，并选择指定的审稿人。此攻击基于一种新颖的优化策略，交替使用特征空间和问题空间来实现对论文的不引人注目的改变。<br>
*优点*：在IEEE S&P中，可以成功地选择和删除审稿人，而不需要访问分配系统。<br>
*PS*：很实在的研究，考虑最熟悉的应用场景。**艺高人胆大**<br>

5. Near-Ultrasound Inaudible Trojan (Nuit): Exploiting Your Speaker to Attack Your Microphone<br>
*问题*：语音控制系统（VCS）为向智能设备发出语音命令。VCS安全性值得研究，现有两类攻击：（i）无声攻击，可以在攻击者和受害者彼此接近时发动（ii）听得到的攻击，可以通过将攻击信号嵌入音频中远程发动。<br>
*方案*：本文提出一种近超声波不可听木马（Nuit），然后提出了一个有效的防御方法。<br>
*优点*：（1）无声且可以远程发动（2）可以实现端到端的不可察觉性（3）利用受害者扬声器攻击受害者麦克风及其相关的VCS<br>
*PS*：针对语音控制系统的木马攻击，找一些现实的攻击优势。**必要可以自攻自防**<br>

6. Meta-Sift: How to Sift Out a Clean Subset in the Presence of Data Poisoning?<br>
*问题*：外部数据集成到训练中会带来数据污染风险。【投毒/后门】多数数据污染防御措施假定能够访问一组干净的数据（称为基础集），可以通过可信来源获得。然而，现实情况中整个数据集都是不可信的情况也越来越普遍。本文需要在受污染的数据集中识别一个子集作为基础集来支持这些防御措施。<br>
*探索*：首先检查了在污染样本错误地混入基础集时防御措施的性能。在五种使用基础集的代表性防御措施中，它们的性能在基础集中少于1％的污染点时会显著恶化。因此，筛选出具有高精度的基础集对这些防御措施的性能至关重要。<br>
*方案*：现有自动化工具和人工检查在数据污染存在的情况下识别干净数据的精度并不理想。本文提出Meta-Sift：现有的污染攻击会改变数据分布，导致在干净部分的污染数据集上进行训练并在损坏部分上进行测试时预测损失很高。因此，Meta-Sift采用一个双层优化来识别干净的数据，并引入一系列技术来提高识别的效率和精度。<br>
*优点*：Meta-Sift可以在各种污染威胁下以100％的精度筛选出干净的基础集，辅助现有的防御技术。<br>
*PS*：为现有防御提供假设支撑。**思路：攻击/防御中假设难以实现，专攻假设来辅助后续方案**<br>
*问题*：
*方案*：
*优点*：
*PS*：
7. FreeEagle: Detecting Complex Neural Trojans in Data-Free Cases<br>
*问题*：许多后门检测方法假定防御者可以访问一组干净的验证样本或带有触发器的样本，这在现实情况下可能不成立，例如防御者是模型共享平台的维护者。<br>
*方案*：本文提出了FreeEagle，第一种无数据后门检测方法，可以有效地检测DNN上的复杂后门攻击，而不需要依赖任何干净样本或带有触发器的样本的访问。<br>
*优点*：FreeEagle对各种复杂的后门攻击都很有效，甚至胜过一些最先进的非无数据后门检测方法。<br>
*PS*：用模型逆向数据可以等同于训练数据，很多攻击可以把无数据访问的概念引入。**模型即是数据**<br>

8. Fine-grained Poisoning Attack to Local Differential Privacy Protocols for Mean and Variance Estimation<br>
*问题*：攻击者可以从用户端发起数据毒化攻击，将精心制作的虚假数据注入LDP协议中，以最大程度地扭曲数据管理者的最终估计结果。<br>
*方案*：本文提出一种细粒度攻击，使攻击者能够微调并操纵平均值和方差估计。具体地，攻击利用了LDP的特性，将假数据注入到本地LDP实例的输出域中。【输出毒化攻击（OPA）】<br>
*结论*：发现安全-隐私的一致性，即小隐私损失增强了LDP的安全性，与之前的已知安全-隐私权衡相矛盾。此研究揭示了数据毒化攻击对LDP的威胁。本文还提出了一种新的防御方法，可以从污染的数据收集中恢复结果的准确性。<br>
*优点*：以直觉上提供虚假输入给LDP的基线攻击为参照，OPA在三个真实世界数据集上优于基线攻击。<br>
*PS*：数据投毒聚焦到差分隐私上，影响最终估计结果。**把方案的每一个步骤都考虑进去，去思考隐私与安全**<br>

9. DiffSmooth: Certifiably Robust Learning via Diffusion Models and Local Smoothing<br>
*问题*：扩散模型已用于对抗净化，为标准模型提供经验证和认证的鲁棒性。另一方面，不同的鲁棒训练平滑模型已被用于提高认证鲁棒性。扩散模型能否用于在这些鲁棒训练平滑模型上实现改进的认证鲁棒性？<br>
*方案*：本文理论上证明了扩散模型恢复的实例在一定概率下处于原始实例的有界邻域内，并且一次性去噪扩散概率模型（DDPM）可以在温和的对抗攻击下提高鲁棒性。然后，提出一种新颖的认证鲁棒性训练框架，称为AR-DDPM，它利用DDPM作为模型生成器，并同时考虑对抗性和非对抗性噪声，以提高模型的鲁棒性和泛化性能。<br>
*优点*：在CIFAR-10和CIFAR-100数据集上对AR-DDPM进行了广泛的评估，比其他先进的认证鲁棒性方法更好。在物体检测任务中，AR-DDPM在VOC 2007数据集上具有更好的效果。<br>
*PS*：扩散模型聚焦于模型的认证鲁棒性。**扩散模型是否可用于隐私攻击/安全攻击，或者其本身存在隐私与安全问题**<br>

10. CAPatch: Physical Adversarial Patch against Image Captioning Systems<br>
*问题*：监视系统使图像描述成为处理大量视频的必要技术。目前，针对图像描述的对抗攻击【图像特征提取后的自然语言处理】还未研究。<br>
*方案*：本文设计了CAPatch，【物理对抗补丁】可以对多模态图像描述系统产生错误的最终描述，即创建完全不同的句子或缺少关键字的句子。为保证CAPatch在物理世界中的实用性，提出了一种检测保证和注意力增强方法，以增加CAPatch的影响力，然后提出了一种鲁棒性改进方法，以解决图像打印和捕获引起的补丁扭曲问题。<br>
*优点*：在三种常用的图像描述系统（Show-and-Tell、Self-critical Sequence Training: Att2in和Bottom-up Top-down）上进行评估，CAPatch在数字和物理世界中均有效【各种场景、服装和照明条件下佩戴印刷的补丁】。若图像大小为5%，物理印刷的CAPatch可以实现连续攻击。<br>
*PS*：针对图像描述技术的对抗攻击，贴近实际。**图像描述技术的隐私问题与后门攻击是否值得研究**<br>

11. Aegis: Mitigating Targeted Bit-flip Attacks against Deep Neural Networks<br>
*问题*：比特翻转攻击（BFAs）可以篡改少量模型参数位来破坏深度神经网络（DNN）的完整性。然而，目前的防御方法均关注非定向情况。它们要么需要额外的可信应用程序，要么会使模型更容易受到有针对性的BFA攻击的影响。此外，有针对性的BFA攻击还没有很好的防御方法。<br>
*方案*：本文提出Aegis防御方法，用于缓解有针对性的BFA攻击。核心观察是现有的针对性攻击集中于翻转某些重要层中的关键位。Aegis设计了一个动态退出机制，将额外的内部分类器（IC）附加到隐藏层，使输入样本能够从不同层中提前退出，有效地扰乱攻击者的攻击计划【构造多出口模型】。此外，动态退出机制在每次推理时随机选择IC进行预测，显著增加自适应攻击的攻击成本【所有防御机制都对攻击者透明】。本文还提出了一种强韧性训练策略，通过在IC训练阶段模拟BFAs来适应攻击场景，以增加模型的韧性。<br>
*优点*：Aegis可以有效地缓解最新有针对性攻击，将攻击成功率降低了5-10倍，明显优于现有的防御方法。<br>
*PS*：构造多出口模型防御BFA攻击。**BFA是否值得研究，攻击限制更加低的BFA**<br>

14. Adversarial Training for Raw-Binary Malware Classifiers<br>
机器学习（ML）模型在将原始可执行文件（二进制文件）分类为恶意或良性方面显示出很高的准确性。这使得基于ML的分类方法在学术和现实世界的恶意软件检测中的影响越来越大，这是网络安全的一个重要工具。然而，以前的工作通过创建恶意二进制文件的变体（被称为对抗性例子）引起了人们的警惕，这些变体以一种保留功能的方式被改造以逃避检测。在这项工作中，我们研究了使用对抗性训练方法来创建恶意软件分类模型的有效性，这些模型对一些最先进的攻击更加强大。为了训练我们最强大的模型，我们大大提高了创建对抗性例子的效率和规模，使对抗性训练变得切实可行，这在以前的原始二进制恶意软件检测器中还没有做到。然后，我们分析了不同长度的对抗性训练的效果，以及分析了各种类型的攻击的训练效果。我们发现，数据增强并不能阻止最先进的攻击，但使用其他离散领域中使用的通用梯度引导方法，确实可以提高鲁棒性。我们还表明，在大多数情况下，通过对同一攻击的低效版本进行对抗性训练，可以使模型对恶意软件领域的攻击更加稳健。在最好的情况下，我们将一个最先进的攻击的成功率从90%降低到5%。我们还发现，用某些类型的攻击进行训练可以提高对其他类型攻击的鲁棒性。最后，我们讨论了从我们的结果中获得的启示，以及如何使用它们来更有效地训练强大的恶意软件检测器。

15. A Data-free Backdoor Injection Approach in Neural Networks<br>
最近，对深度神经网络（DNNs）的后门攻击得到了广泛的研究，这种攻击使得植入后门的模型在良性样本上表现良好，而在受控样本（带有触发器）上表现恶劣。几乎所有现有的后门攻击都需要访问原始训练/测试数据集或与主任务相关的数据来将后门注入目标模型，这在许多场景下是不现实的，例如，私有训练数据。在本文中，我们提出了一种“无数据”的新颖后门注入方法。我们收集与主任务无关的替代数据，并通过过滤掉冗余样本来减少其数量，以提高后门注入的效率。我们设计了一种新颖的损失函数，用于使用替代数据将原始模型微调为植入后门的模型，并优化微调以平衡后门注入和主任务性能。我们对各种深度学习场景进行了广泛实验，例如图像分类、文本分类、表格分类、图像生成和多模态，使用不同的模型，如卷积神经网络（CNNs）、自动编码器、Transformer模型、表格模型以及多模态DNNs。评估结果表明，我们的无数据后门注入方法可以有效地嵌入后门，攻击成功率接近100％，在主任务上的性能降级是可以接受的。

16. TPatch: A Triggered Physical Adversarial Patch<br>
自动驾驶车辆越来越多地利用基于视觉的感知模块获取有关行驶环境和障碍物的信息。正确的检测和分类对于确保安全的驾驶决策非常重要。现有的研究已经证明，使用印刷的对抗性补丁可以欺骗感知模型，如物体检测器和图像分类器。然而，大多数这些攻击方法是对每个经过的自动驾驶车辆都进行攻击，缺乏针对性。本文提出了TPatch，一种由声音信号触发的物理对抗性补丁。与其他对抗性补丁不同，TPatch在正常情况下保持良性，但可以通过信号注入攻击引入的设计失真来触发隐藏、创建或更改攻击。为了避免引起人类驾驶员的怀疑，并使攻击在现实世界中实际且具有鲁棒性，我们提出了基于内容的伪装方法和攻击鲁棒性增强方法来加强攻击。我们对三个物体检测器（YOLO V3/V5和Faster R-CNN）和八个图像分类器进行了评估，证明了TPatch在模拟和实际世界中的有效性。我们还讨论了可能的传感器、算法和系统级别的防御方法。

17. UnGANable: Defending Against GAN-based Face Manipulation<br>
Deepfakes对我们的社会造成了严重的视觉误导威胁。一个代表性的Deepfakes应用是脸部操作，它修改了图像中受害者的面部特征，例如改变她的年龄或发色。目前最先进的面部操作技术依赖于生成对抗网络（GANs）。在本文中，我们提出了第一个针对基于GAN逆映射（GAN inversion）的面部操作的防御系统，即UnGANable。具体而言，UnGANable专注于防御GAN逆映射。其核心技术是在图像空间中围绕原始图像（称为目标图像）搜索替代图像（称为伪装图像）。当这些伪装图像在网上发布时，它们可能危及GAN逆映射过程。我们考虑了两种最先进的反演技术，包括基于优化的反演和混合反演，并在五种不同的情况下设计了五种不同的防御措施，取决于防御者的背景知识。在两个基准面部数据集上训练的四个流行的GAN模型上进行的广泛实验表明，UnGANable实现了显著的有效性和实用性能，并超越了多个基准方法。我们进一步调查了四个适应性对手来绕过UnGANable，并展示其中一些对手略微有效。

18. Learning Normality is Enough: A Software-based Mitigation against the Inaudible Voice Attacks<br>
听不见声音攻击会在语音助手中默默注入恶意语音命令，以操纵智能音箱等语音控制设备。为了缓解现有和未来设备的这种威胁，本文提出了NormDetect，这是一种基于软件的缓解措施，可以立即应用于各种设备，无需进行任何硬件修改。为了克服攻击模式在设备之间变化的挑战，我们设计了一种通用的检测模型，该模型不依赖于特定设备的音频特征或样本。与现有研究的监督学习方法不同，我们采用受异常检测启发的无监督学习。虽然听不见声音攻击的模式是多种多样的，但我们发现良性音频在时间 - 频率领域中共享相似的模式。因此，我们可以通过学习良性音频的模式（正常情况）来检测攻击（异常情况）。NormDetect将频谱特征映射到低维空间，执行相似性查询，并将其替换为频谱重建的标准特征嵌入。这导致攻击的重建误差比正常情况更大。基于我们从24个智能设备收集的383,320个测试样本的评估显示，平均AUC为99.48％，EER为2.23％，表明NormDetect在检测听不见声音攻击方面的有效性。

19. The Space of Adversarial Strategies<br>
对抗性样本是指被设计用来诱导机器学习模型产生最坏情况行为的输入，已经在过去的十年中得到了广泛的研究。然而，我们对这种现象的理解源于一个相当零散的知识池；目前，有少量攻击，每个攻击都有不同的威胁模型和不可比较的最优定义。在本文中，我们提出了一种系统性方法来表征最坏情况（即最优）的对手。我们首先介绍了一种可扩展的对抗机器学习攻击分解方法，将攻击组件原子化成表面和旅行者。通过我们的分解，我们枚举组件以创建576种攻击（其中568种以前未被探索）。接下来，我们提出了Pareto Ensemble Attack（PEA）：一种理论攻击，它上限了攻击性能。通过我们的新攻击，我们相对于PEA在以下方面测量性能：包括计算成本的三个扩展的?p威胁模型、七个数据集、鲁棒性和非鲁棒性模型，形成对抗策略空间。从我们的评估中，我们发现攻击性能高度依赖于具体情况：领域、模型鲁棒性和威胁模型等都会对攻击效果产生深远影响。我们的调查表明，未来衡量机器学习安全性的研究应该：（1）与领域和威胁模型相结合，（2）超越今天所使用的少量已知攻击。

20. NeuroPots: Realtime Proactive Defense against Bit-Flip Attacks in Neural Networks<br>
深度神经网络（DNN）正在各种安全和安全敏感的应用中变得无处不在，例如自动驾驶汽车和金融系统。最近的研究揭示了位翻转攻击（BFA）可以通过DRAM行攻击破坏DNN的功能——通过精确地将一些位翻转注入量化的模型参数，攻击者可以将模型准确性降低到随机猜测，或将某些输入错误分类到目标类。如果未被检测到，BFA可能会造成灾难性后果。然而，检测BFA是具有挑战性的，因为位翻转可能会发生在DNN模型中的任何权重上，导致检测面很大。与先前试图“修补”DNN模型漏洞的工作不同，我们的工作受到“蜜罐”的思想启发。具体而言，我们提出了一种主动防御概念，名为NeuroPots，它将一些“蜜罐神经元”作为精心制作的漏洞嵌入DNN模型中，以引诱攻击者在其中注入故障，从而使检测和模型恢复更加高效。我们利用NeuroPots开发了一种具有陷门功能的防御框架。我们设计了一个蜜罐神经元选择策略，并提出了两种将陷门嵌入DNN模型的方法。此外，由于注入的大多数位翻转将集中在陷门中，我们使用基于校验和的检测方法高效地检测其中的故障，并通过“刷新”这些有故障的陷门来恢复模型准确性。我们的实验表明，陷门功能的防御在各种DNN模型和数据集上实现了高检测性能，并且以低成本有效地恢复了被攻击的模型。

21. URET: Universal Robustness Evaluation Toolkit (for Evasion)<br>
众所周知，机器学习模型容易受到对抗性逃避攻击的影响，例如图像分类模型。深入了解这种攻击对于确保关键AI任务的安全性和稳健性至关重要。然而，大多数逃避攻击难以对大多数AI系统进行部署，因为它们只关注图像领域，并且只有少数限制。与实际使用的其他输入类型不同，图像由同质、数字、连续和独立的特征组成。此外，某些输入类型包括必须遵守的其他语义和功能约束，以生成现实的对抗性输入。在这项工作中，我们提出了一个新的框架，可以生成不同类型和任务领域的对抗性输入。给定一个输入和一组预定义的输入转换，我们的框架发现了一系列转换，使得结果为语义正确和功能正常的对抗性输入。我们在多个不同的机器学习任务上展示了我们方法的普适性，这些任务具有各种输入表示。我们还展示了生成对抗性示例的重要性，因为它们使得可以部署缓解技术。

22. You Can't See Me: Physical Removal Attacks on LiDAR-based Autonomous Vehicles Driving Frameworks<br>
自动驾驶汽车（AV）越来越多地使用基于LiDAR的目标检测系统来感知道路上的其他车辆和行人。虽然现有的针对基于LiDAR的自动驾驶架构的攻击主要集中在降低AV目标检测模型的置信度得分以诱导障碍物误检测，但我们的研究发现了如何利用基于激光的欺骗技术，在传感器级别有选择性地去除真实障碍物的LiDAR点云数据，然后将其用作AV感知的输入。去除这个关键的LiDAR信息会导致自动驾驶障碍物检测器无法识别和定位障碍物，从而导致AV做出危险的自动驾驶决策。在本文中，我们提出了一种对人眼不可见的方法，通过利用LiDAR传感器数据与自动驾驶框架集成的固有自动转换和过滤过程，隐藏物体并欺骗自动驾驶车辆的障碍物检测器。我们将这种攻击称为物理去除攻击（PRA），并展示了它们对三种流行的AV障碍物检测器（Apollo、Autoware、PointPillars）的有效性，我们实现了45？的攻击能力。我们评估了攻击对三个融合模型（Frustum-ConvNet、AVOD和Integrated-Semantic Level Fusion）的影响，并使用行业级模拟器LGSVL评估了驾驶决策的后果。在我们的移动车辆场景中，我们成功地去除了目标障碍物的90%云点，并获得了92.7%的成功率。最后，我们展示了攻击成功地对抗了两种常见的对抗欺骗和物体隐藏攻击的防御，并讨论了两种增强的防御策略来减轻我们的攻击。

23. SMACK: Semantically Meaningful Adversarial Audio Attack<br>
可语音控制的系统依赖于语音识别和说话人识别作为关键的技术。虽然它们给我们的日常生活带来了革命性的变化，但它们的安全性已成为一个越来越大的关注点。现有的工作已经证明了使用恶意制造的扰动来操纵语音或说话人识别的可行性。尽管这些攻击在目标和技术上各不相同，但它们都需要添加噪声扰动。虽然这些扰动通常受到Lp-bounded邻域的限制，但添加的噪声不可避免地留下了人类可识别的不自然痕迹，并可以用于防御。为了解决这个限制，我们引入了一种新的对抗性音频攻击类别，称为语义上有意义的对抗性音频攻击（SMACK），其中固有的语音属性（如韵律）被修改，以使它们仍然语义上表示相同的语音并保留语音质量。我们以黑盒方式评估了SMACK对五个转录系统和两个说话人识别系统的有效性。通过操作语义属性，我们的对抗性音频示例能够逃避最先进的防御措施，在人类感知研究中与传统的Lp-bounded攻击相比具有更好的语音自然性。
## 人工智能&隐私保护
1. HOLMES: Efficient Distribution Testing for Secure Collaborative Learning<br>
使用安全多方计算（MPC），拥有敏感数据的组织（例如医疗保健、金融或执法机构）可以在不向彼此透露数据的情况下，对它们的联合数据集进行机器学习模型训练。同时，安全计算限制了对联合数据集的操作，这阻碍了对其质量的计算评估。如果没有这样的评估，部署联合训练的模型可能是非法的。例如欧洲联盟的《通用数据保护条例》（GDPR）等法规要求组织对其机器学习模型所造成的错误、偏见或歧视负法律责任。因此，在安全协作学习中，测试数据质量成为一个不可或缺的步骤。但是，使用当前技术进行分布测试的成本过高，这在我们的实验中得到了证明。
我们提出了HOLMES协议，用于高效地执行分布测试。在我们的实验中，与三个非平凡的基准线相比，HOLMES在经典分布测试方面实现了超过10倍的加速，而在多维测试方面则可达到104倍。HOLMES的核心是一种混合协议，它将MPC与零知识证明以及一种新的ZK友好的、天然的遗忘算法集成在一起，这两种算法都具有明显较低的计算复杂度和具体执行成本。"

2. GAP: Differentially Private Graph Neural Networks with Aggregation Perturbation<br>
本文研究了在差分隐私（DP）下学习图神经网络（GNN）的问题。我们提出了一种基于聚合扰动（GAP）的新型差分隐私GNN，通过向GNN的聚合函数添加随机噪声来统计混淆单个边缘（边缘级隐私）或单个节点及其所有相邻边（节点级隐私）的存在。为了适应私有学习的具体需求，GAP的新架构由三个独立模块组成：（i）编码器模块，在不依赖边缘信息的情况下学习私有节点嵌入；（ii）聚合模块，在基于图形结构计算有噪声的聚合节点嵌入；（iii）分类模块，在对私有聚合进行节点分类的同时，无需进一步查询图形边缘。与以往方法相比，GAP的主要优势在于它可以从多跳邻域聚合中受益，并且除了训练的隐私预算外，在推断时可以保证边缘级和节点级DP而不需要额外的成本。我们使用Rényi DP分析了GAP的形式隐私保证，并在三个真实世界图形数据集上进行了实证实验。我们证明GAP在准确性和隐私保护方面的权衡比现有最先进的DP-GNN方法和朴素的基于MLP的基线更好。

3. A Plot is Worth a Thousand Words: Model Information Stealing Attacks via Scientific Plots<br>
构建先进的机器学习（ML）模型需要专业知识和多次试验，以发现最佳的结构和超参数设置。先前的研究表明，模型信息可以被利用来协助其他攻击，例如成员推断、生成对抗性示例。因此，这些信息，如超参数，应该保持机密。众所周知，攻击者可以利用目标ML模型的输出来窃取模型的信息。在本文中，我们发现了一种新的模型信息窃取攻击的侧信道，即广泛用于展示模型性能并易于访问的模型科学绘图。我们的攻击方法简单明了。我们利用影子模型训练技术为攻击模型生成训练数据，该攻击模型本质上是一个图像分类器。对三个基准数据集进行广泛评估表明，我们提出的攻击方法可以有效地推断基于卷积神经网络（CNN）的图像分类器的架构/超参数，只要从它生成的科学绘图。我们还揭示了攻击的成功主要是由于科学绘图的形状，进一步证明了这些攻击在各种情况下都是具有鲁棒性的。考虑到攻击方法的简单和有效性，我们的研究表明，科学绘图确实构成了模型信息窃取攻击的有效侧信道。为了减轻这些攻击，我们提出了几种防御机制，可以降低原始攻击的准确率，同时保持绘图的实用性。然而，这些防御仍然可以被自适应攻击所绕过。

4. V-Cloak: Intelligibility-, Naturalness- & Timbre-Preserving Real-Time Voice Anonymization<br>
即时通讯或社交媒体应用程序生成的语音数据包含独特的用户语音特征，可能被恶意对手滥用以进行身份推断或身份盗窃。现有的语音匿名化技术，例如信号处理和语音转换/合成，会导致感知质量降低。在本文中，我们开发了一个名为V-Cloak的语音匿名化系统，可以在保持音频的可理解性、自然性和音色的情况下实现实时语音匿名化。我们设计的匿名化器具有单次生成模型，可以在不同频率级别上调制原始音频的特征。我们使用精心设计的损失函数训练匿名化器。除了匿名损失外，我们还进一步结合了可理解性损失和基于心理听觉的自然度损失。匿名化器可以实现非指向性和指向性匿名化，以实现不可识别性和不可关联性的匿名目标。
我们在四个数据集上进行了广泛实验，即LibriSpeech（英语）、AISHELL（中文）、CommonVoice（法语）和CommonVoice（意大利语），五个自动说话人验证（ASV）系统（包括两个基于深度神经网络的、两个统计学的和一个商业ASV），以及十一个自动语音识别（ASR）系统（针对不同的语言）。实验结果证实，V-Cloak在匿名性能方面优于五个基线。我们还展示了仅在VoxCeleb1数据集上针对ECAPA-TDNN ASV和DeepSpeech2 ASR训练的V-Cloak对其他ASV具有可转移的匿名性和对其他ASR的跨语言可理解性。此外，我们验证了V-Cloak对各种去噪技术和自适应攻击的鲁棒性。希望V-Cloak可以为我们在五颜六色的世界中提供一层保护。

5. PrivTrace: Differentially Private Trajectory Synthesis by Adaptive Markov Models<br>
发布轨迹数据（个人移动信息）非常有用，但也引起了隐私问题。为了处理隐私问题，在本文中，我们将差分隐私（数据隐私的标准技术）与马尔可夫链模型结合起来，生成合成轨迹。我们注意到现有的研究都使用了马尔可夫链模型，因此提出了一个框架来分析马尔可夫链模型在这个问题中的使用。基于分析，我们提出了一种有效的算法PrivTrace，它自适应地使用一阶和二阶马尔可夫模型。我们使用合成和真实世界数据集评估了PrivTrace和现有方法，以展示我们的方法的优越性。
## 联邦学习
1. PrivateFL: Accurate, Differentially Private Federated Learning via Personalized Data Transformation<br>
联邦学习（FL）使多个客户端在中央服务器的协调下共同训练模型。虽然FL通过保留每个客户端的训练数据来改善数据隐私，但攻击者（例如，不受信任的服务器）仍然可以通过各种推断攻击危及客户端本地训练数据的隐私。保护FL隐私的事实上的方法是差分隐私（DP），它在训练期间添加随机噪声。然而，当应用于FL时，DP存在一个关键限制：为了实现有意义的隐私水平，它牺牲了模型的准确性，这甚至比应用于传统的集中式学习更加严重。
在本文中，我们研究了FL+DP的准确性降级原因，并设计了一种提高准确性的方法。首先，我们提出这种准确性降级部分是因为DP在本地训练期间引入不同的随机噪声和剪切偏差时，会在FL客户端之间引入额外的异质性。据我们所知，我们是第一个将FL中的差分隐私与客户端异质性联系起来的人。其次，我们设计了PrivateFL，以在减少异质性的情况下在FL中学习准确的差分隐私模型。其关键思想是在本地训练期间联合学习差分隐私、个性化数据转换，为每个客户端定制个性化数据转换。个性化数据转换可以将客户端的本地数据分布转移，以补偿DP引入的异质性，从而提高FL模型的准确性。
在评估中，我们将PrivateFL与八种最先进的差分隐私FL方法结合并比较了七个基准数据集，包括六个图像和一个非图像数据集。我们的结果表明，PrivateFL使用小ε可以学习准确的FL模型，例如，在（ε = 2，δ = 1e-3）-DP下，使用100个客户端在CIFAR-10上可以达到93.3％的准确率。此外，PrivateFL可以与先前的工作结合，以减少DP引起的异质性，并进一步提高其准确性。

2. Every Vote Counts: Ranking-Based Training of Federated Learning to Resist Poisoning Attacks<br>
联邦学习（FL）允许不受信任的客户端协同训练一个称为全局模型的共同机器学习模型，而无需共享其私有/专有训练数据。然而，FL容易受到恶意客户端的污染，他们旨在通过在FL的训练过程中贡献恶意更新来阻碍全局模型的准确性。
我们认为攻击现有FL系统的污染攻击的关键因素是客户端可选择的模型更新空间过大。为了解决这个问题，我们提出了联邦排名学习（FRL）。FRL将客户端更新的空间从标准FL中的模型参数更新（一个浮点数的连续空间）减少到参数排名的空间（一个整数值的离散空间）。为了能够使用参数排名（而不是参数权重）来训练全局模型，FRL利用了最近超级掩蔽训练机制的思想。
具体来说，FRL客户端根据其本地训练数据对随机初始化的神经网络的参数进行排名，而FRL服务器使用投票机制来聚合客户端提交的参数排名。
直观地说，我们基于投票的聚合机制可以防止污染客户端对全局模型进行重大的对抗性修改，因为每个客户端只有一票！我们通过分析证明和实验展示了FRL对污染攻击的鲁棒性，并展示了其高通信效率。

3. Gradient Obfuscation Gives a False Sense of Security in Federated Learning<br>
联邦学习被提出作为一种隐私保护的机器学习框架，使多个客户端能够在不共享原始数据的情况下进行协作。然而，这个框架的设计并没有保证客户端隐私保护。先前的工作已经表明，联邦学习中的梯度共享策略可能容易受到数据重建攻击的威胁。然而，在实践中，由于高昂的通信成本或隐私增强要求，客户端可能不会传输原始梯度。实证研究表明，梯度混淆，包括通过梯度噪声注入进行有意混淆和通过梯度压缩进行无意混淆，可以为重建攻击提供更多的隐私保护。在这项工作中，我们提出了一个针对联邦学习中图像分类任务的新的重建攻击框架。我们展示了常用的梯度后处理程序（如梯度量化、梯度稀疏化和梯度扰动）可能会在联邦学习中产生错误的安全感。与先前的研究相反，我们认为隐私增强不应被视为梯度压缩的副产品。此外，我们在提出的框架下设计了一种新方法，以语义层面重建图像。我们量化了语义隐私泄露并将其与传统的图像相似度得分进行比较。我们的比较挑战了文献中的图像数据泄露评估方案。结果强调了需要重新审视并重新设计现有联邦学习算法中客户端数据的隐私保护机制的重要性。
## 可信人工智能
1. AIRS: Explanation for Deep Reinforcement Learning based Security Applications<br>
最近，我们见证了深度强化学习（DRL）在许多安全应用中的成功，从恶意软件变异到自私的区块链挖掘。就像所有其他机器学习方法一样，解释性的缺乏限制了其广泛的采用，因为用户难以建立对DRL模型决策的信任。在过去的几年中，已经提出了不同的方法来解释DRL模型，但不幸的是，它们通常不适用于安全应用程序，其中解释的保真度、效率和模型调试的能力在很大程度上缺乏。
在这项工作中，我们提出了AIRS，一个通用框架，用于解释基于深度强化学习的安全应用程序。与以前的作品不同，它不是针对代理的当前操作指出重要特征，而是在步骤级别上进行解释。它建模了DRL代理所取的关键步骤与最终奖励之间的关系，因此输出最关键的步骤，这些步骤对于代理收集的最终奖励最为关键。通过四个代表性的安全关键应用程序，我们从解释性、保真度、稳定性和效率的角度评估了AIRS。我们表明，AIRS可以胜过其他可解释的DRL方法。我们还展示了AIRS的实用性，证明我们的解释可以促进DRL模型的故障偏移，帮助用户建立对模型决策的信任，甚至协助识别不当的奖励设计。
