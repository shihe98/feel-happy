# 2022 NDSS 关注内容
## 人工智能&安全威胁
1. Adversarial Robustness for Tabular Data through Cost and Utility Awareness<br>
机器学习的许多安全关键应用，例如欺诈或滥用检测，使用表格域中的数据。对于这些应用程序，对抗性示例可能特别具有破坏性。然而，现有关于对抗性鲁棒性的研究主要集中在图像和文本域中的机器学习模型上。我们认为，由于表格数据与图像或文本之间的差异，现有的威胁模型不适用于表格域。这些模型没有捕捉到攻击的成本可能比不可感知性更重要，或者攻击者可能会为不同的对抗性示例分配不同的效用值。我们证明，由于这些差异，用于图像和文本的攻击和防御方法不能直接应用于表格设置。我们通过提出新的成本和效用感知威胁模型来解决这些问题，这些模型适用于针对表格域的攻击者的对抗能力和约束。我们引入了一个框架，使我们能够设计攻击和防御机制，从而产生受到成本或效用感知对手保护的模型，例如受某个财务预算约束的对手。我们展示了我们的方法在三个数据集上的有效性，这些数据集对应于可能具有经济和社会影响的应用程序。

2. Anomaly Detection in the Open World: Normality Shift Detection, Explanation, and Adaptation<br>
概念漂移是基于学习的安全应用程序面临的最棘手的挑战之一，这些应用程序基于相同分布的封闭世界假设进行训练和部署。而在安全领域中最重要的任务之一——异常检测，则对于没有任何异常数据进行训练的情况（即零正样本），可以免疫异常行为的漂移，但这也带来了当正常性发生变化时更严重的影响。然而，现有的研究主要集中于异常行为的概念漂移和/或监督学习，对于零正样本异常检测中的正常性变化问题仍未得到充分探究。在本工作中，我们首次探索了深度学习在安全应用程序中基于零正样本的异常检测中的正常性变化问题，并提出了OWAD，一个通用的框架，用于在实践中检测、解释和适应正常性变化。特别地，OWAD通过以无监督的方式检测变化，减少手动标注的开销，并通过分布级处理提供更好的适应性能。我们通过几个基于长期实际数据的安全相关异常检测应用程序的实验展示了OWAD的有效性。结果表明，OWAD可以提供更好的正常性变化适应性能，并且标注开销更少。我们提供了案例研究来分析正常性变化，并为安全应用程序提供操作建议。我们还在SCADA安全系统上进行了初始的实际部署。

3. Attacks as Defenses: Designing Robust Audio CAPTCHAs Using Attacks on Automatic Speech Recognition Systems<br>
音频CAPTCHA旨在为在线资源提供强大的防御措施；然而，语音转文本机制的进步已经使得这些防御措施失效。由于W3C特别将音频CAPTCHA命名为重要的可访问性启用程序，因此不能简单地放弃它们。因此，对于未来一个安全且可访问的Web而言，显然需要更具强大性能的音频CAPTCHA。我们从最近的攻击语音转文本系统的文献中寻求灵感，构建基于原则的强大音频防御系统。我们首先比较了20篇最近的攻击论文，对它们进行分类和评估，以确定它们是否适合作为新的"对转录具有强韧性、但对人类易于理解"的CAPTCHA的基础。通过展示这些攻击单独使用是不够的，我们提出了一种新的机制，既相对易于理解（通过用户研究进行评估），又难以自动转录（即，P(rmtranscription)=4times10−5P(rmtranscription)=4times10−5）。我们还证明，我们的音频样本在提供给语音转文本系统时被检测为CAPTCHA的概率很高（P(rmevasion)=1.77times10−4P(rmevasion)=1.77times10−4）。最后，我们展示了我们的方法对WaveGuard的鲁棒性，这是一种流行的机制，旨在击败对抗性样本（并使ASR输出原始转录而不是对抗性转录）。我们展示了我们的方法可以以99%的成功率打破WaveGuard。通过这样做，我们不仅展示了一个大约比原来的CAPTCHA难度高了四个数量级的CAPTCHA，而且这样的系统可以根据攻击论文所获得的见解基于人类和计算机处理音频的不同之处进行设计。

4. BARS: Local Robustness Certification for Deep Learning based Traffic Analysis Systems<br>
深度学习（DL）在许多流量分析任务中表现良好。然而，深度学习的脆弱性削弱了这些流量分析器的实际性能（例如遭受逃避攻击）。近年来的许多研究都集中于对基于DL的模型进行强健性认证。但是现有的方法在流量分析领域中的表现远非完美。在本文中，我们尝试同时匹配DL基础流量分析系统的三个属性：（1）高度异构的特征，（2）不同的模型设计，（3）对抗性操作环境。因此，我们提出了BARS，一种基于边界自适应随机平滑的DL基础流量分析系统的通用强健性认证框架。为了获得更紧密的强健性保证，BARS使用优化的平滑噪声来收敛到分类边界。我们首先提出了Distribution Transformer，用于生成优化的平滑噪声。然后为了优化平滑噪声，我们提出了一些特殊的分布函数和两种基于梯度的搜索算法，用于噪声形状和噪声尺度。我们在三个实际的DL基础流量分析系统中实现和评估了BARS。实验结果表明，BARS可以比基线方法获得更紧密的强健性保证。此外，我们通过五个应用案例（例如定量评估强健性）说明了BARS的实用性。

5. BEAGLE: Forensics of Deep Learning Backdoor Attack for Better Defense<br>
深度学习后门攻击的威胁模型类似于传统的网络攻击。攻击取证是传统网络攻击的重要对策，因此对于防御模型后门攻击也非常重要。在本文中，我们提出了一种新颖的模型后门取证技术。给定一些攻击样本，例如具有后门触发器的输入，这些样本可能代表不同类型的后门，我们的技术会自动将它们分解为干净的输入和相应的触发器。然后根据它们的属性对触发器进行聚类，以允许自动攻击分类和总结。然后可以自动合成后门扫描器，以查找其他模型中相同类型的后门实例。我们对2,532个预训练模型、10种常见攻击进行了评估，并与9种基线进行了比较，结果表明我们的技术非常有效。分解后的干净输入和触发器与真实情况非常相似。合成的扫描器明显优于现有扫描器的基本版本，后者几乎无法推广到不同类型的攻击。

6. Backdoor Attacks Against Dataset Distillation<br>
数据集精炼技术已成为提高机器学习模型训练数据效率的重要技术。它将大型数据集的知识压缩为一个更小的合成数据集。在这个更小的精炼数据集上训练的模型可以达到与在原始训练数据集上训练的模型相当的性能。然而，现有的数据集精炼技术主要旨在实现资源使用效率和模型实用性之间的最佳平衡，它们带来的安全风险尚未得到探究。本研究在图像领域对使用数据集精炼模型训练的模型进行了首次反向攻击。具体来说，我们在精炼过程中向合成数据中注入触发器，而不是在模型训练阶段进行所有先前的攻击。我们提出了两种类型的反向攻击，即NAIVEATTACK和DOORPING。NAIVEATTACK仅仅在初始精炼阶段向原始数据添加触发器，而DOORPING则在整个精炼过程中迭代更新触发器。我们在多个数据集、架构和数据集精炼技术上进行了广泛的评估。实证评估显示，NAIVEATTACK在某些情况下可以达到不错的攻击成功率（ASR）得分，而DOORPING在所有情况下都可以达到更高的ASR得分（接近1.0）。此外，我们进行了全面的消融研究，分析可能影响攻击性能的因素。最后，我们评估了多种防御机制对我们的反向攻击，并展示了我们的攻击可以实际地规避这些防御机制。

7. Focusing on Pinocchio's Nose: A Gradients Scrutinizer to Thwart Split-Learning Hijacking Attacks Using Intrinsic Attributes<br>
Split learning是一种保护隐私的分布式学习方法，最近获得了广泛关注。它也面临着新的安全挑战。FSHA是对Split learning的一种严重威胁。在FSHA中，恶意服务器劫持训练，欺骗客户端训练自编码器的编码器，而不是分类模型。客户端发送给服务器的中间结果实际上是私有训练样本的潜在代码，可以使用自编码器的解码器从接收到的代码中高度还原。SplitGuard是目前唯一有效的防御劫持攻击的方法。它是一种主动方法，通过注入虚假标记数据来引起异常行为以检测劫持攻击。这种注入还会对预期模型的正常训练产生不良影响。本文首先展示了SplitGuard对一种名为SplitSpy的自适应劫持攻击是脆弱的。SplitSpy利用了与SplitGuard相同的属性来检测劫持攻击。在SplitSpy中，恶意服务器维护一个影子模型，执行预期任务以检测虚假标记数据并逃避SplitGuard。我们的实验评估表明，SplitSpy可以有效地逃避SplitGuard。然后，我们提出了一种新的被动检测方法，名为Gradients Scrutinizer，它依赖于预期模型和恶意模型之间梯度之间的内在差异：对于预期模型，相同标签样本的梯度之间的期望相似性与不同标签样本的梯度之间的期望相似性不同，而对于恶意模型，它们相同。这种内在可区分性使Gradients Scrutinizer能够有效地检测Split learning劫持攻击，而不会对预期模型的正常训练进行篡改。我们广泛的评估表明，Gradients Scrutinizer可以有效地防止已知的Split learning劫持攻击和对其的自适应反击。

8. RoVISQ: Reduction of Video Service Quality via Adversarial Attacks on Deep Learning-based Video Compression<br>
视频压缩在视频流和分类系统中起着至关重要的作用，可以在给定的带宽预算下最大化终端用户的体验质量（QoE）。在本文中，我们进行了首次系统研究深度学习基础的视频压缩和下游分类系统的对抗攻击。我们的攻击框架名为RoVISQ，可以操纵视频压缩模型的速率-失真（R-D）关系，以实现以下目标之一或两者兼备：（1）增加网络带宽，（2）降低终端用户的视频质量。我们进一步设计了新的目标，用于攻击下游视频分类服务的有针对性和无针对性攻击。最后，我们设计了一种输入不变扰动，可以实时普遍干扰视频压缩和分类系统。与先前提出的对视频分类的攻击不同，我们的对抗扰动是第一个经得起压缩的。我们通过实验证明了RoVISQ攻击对各种防御措施的韧性，例如对抗训练、视频去噪和JPEG压缩。我们在各种视频数据集上进行了广泛的实验，结果表明RoVISQ攻击可以使峰值信噪比降低最多5.6dB，比特率降低最多约2.4倍，并在下游分类器上实现了90%以上的攻击成功率。我们的用户研究进一步证明了RoVISQ攻击对用户QoE的影响。我们在https://sites.google.com/view/demo-of-rovisq/home上提供了一些示例受攻击的视频，这些视频用于我们的调查。

9. The Beatrix Resurrections: Robust Backdoor Detection via Gram Matrices<br>
在训练期间，深度神经网络（DNN）容易受到后门攻击。以这种方式损坏的模型正常运行，但在输入中出现特定模式时，会产生预定义的目标标签。现有的防御通常依赖于统一后门设置的假设，其中有毒样本共享相同的触发器。然而，最近的先进后门攻击表明，在动态后门中触发器会从输入到输入发生变化，从而打败了现有的防御。在本文中，我们提出了一种新颖的技术，称为Beatrix（通过Gram矩阵进行后门检测）。Beatrix利用Gram矩阵来捕捉特征之间的相关性，以及表示的适当高阶信息。通过从正常样本的激活模式中学习类别条件统计信息，Beatrix可以通过捕捉激活模式中的异常来识别有毒样本。为了进一步提高识别目标标签的性能，Beatrix利用基于内核的测试，而不对表示分布做出任何先验假设。我们通过广泛的评估和与最先进的防御技术的比较展示了我们方法的有效性。实验结果表明，我们的方法在检测动态后门方面实现了91.1％的F1分数，而现有技术只能达到36.9％。
## 人工智能&隐私保护
1. Machine Unlearning of Features and Labels<br>
从机器学习模型中删除信息是一个非常复杂的任务，需要部分地撤销训练过程。当敏感数据，如信用卡号码或密码，意外进入模型并需要在之后删除时，这个任务是不可避免的。最近，已经提出了不同的机器学习去学习的方法来解决这个问题。虽然这些方法在删除单个数据点方面是有效的，但它们无法扩展到需要撤销更大组的特征和标签的情况。
在本文中，我们提出了第一种去学习特征和标签的方法。我们的方法建立在影响函数的概念上，并通过模型参数的闭合形式更新实现去学习。它能够在回顾性地调整训练数据对学习模型的影响，并从而纠正数据泄露和隐私问题。对于具有强凸损失函数的学习模型，我们的方法提供了带有理论保证的认证去学习。对于具有非凸损失的模型，我们通过实验证明去学习特征和标签是有效的，而且比其他策略要快得多。
## 联邦学习
1. PPA: Preference Profiling Attack Against Federated Learning<br>
联邦学习（FL）通过多个分散的用户进行全局模型训练，每个用户都有本地数据集。与传统的集中式学习相比，FL不需要直接访问本地数据集，因此旨在缓解数据隐私问题。然而，FL中的数据隐私泄漏仍然存在，包括成员推理、属性推理和数据反演等推理攻击。在这项工作中，我们提出了一种新型的隐私推理攻击，称为偏好轮廓攻击（PPA），可以准确地描述本地用户的私人偏好，例如客户的在线购物中最受欢迎（最不受欢迎）的商品和用户自拍中最常见的表情。通常情况下，PPA可以在本地客户（用户）的特征条件下描述前kk个（即kk=1,2,31,2,3，特别地kk=1k=1）偏好。我们的关键见解是，本地用户模型的梯度变化对给定类别的样本比例具有可区分的敏感性，特别是对于大多数（少数）类别。通过观察用户模型对类别的梯度敏感性，PPA可以描述用户本地数据集中该类别的样本比例，从而暴露用户的偏好。FL的固有统计异质性进一步促进了PPA的发展。我们已经使用四个数据集（MNIST、CIFAR10、RAF-DB和Products-10K）广泛评估了PPA的有效性。我们的结果显示，在MNIST和CIFAR10中，PPA分别实现了90%和98%的前1攻击精度。更重要的是，在购物（即Products-10K）和社交网络（即RAF-DB）的实际商业场景中，PPA在前1攻击准确率方面分别获得了78%的最高攻击准确率，以推断最常订购的商品（作为商业竞争对手），以及88%的最高攻击准确率，以推断受害者用户最常见的面部表情，例如厌恶。对于Products-10K和RAF-DB，前3攻击准确率和前2攻击准确率分别高达88%和100%。我们还表明，PPA对FL的本地用户数量（我们测试的上限为100）和本地训练轮数（我们测试的上限为20）不敏感。虽然现有的对抗措施，如dropout和差分隐私保护，可以在一定程度上降低PPA的准确性，但它们不可避免地会对全局模型造成显著的恶化影响。源代码可在https://github.com/PPAattack上获得。

2. Securing Federated Sensitive Topic Classification against Poisoning Attacks<br>
我们提出了一种基于联邦学习（FL）的解决方案，用于构建一个分布式分类器，能够检测包含敏感内容的URL，即与健康、政治信仰、性取向等类别相关的内容。虽然这样的分类器解决了先前离线/集中式分类器的限制，但仍然容易受到恶意用户的污染攻击，这些用户可能会通过传播有误的模型更新来降低良性用户的准确性。为了防范这种攻击，我们基于主观逻辑和基于残差的攻击检测开发了一个强大的聚合方案。通过理论分析、基于跟踪的模拟以及原型和真实用户的实验验证的组合，我们证明了我们的分类器能够高精度地检测敏感内容，快速学习新标签，并在面对恶意用户的污染攻击以及非恶意用户的不完美输入时保持稳健。
## 可信人工智能
1. Fusion: Efficient and Secure Inference Resilient to Malicious Servers<br>
在安全机器学习推断中，大多数方案假设服务器是半诚实的（遵循协议但试图推理出额外的信息）。然而，在现实世界中，服务器可能是恶意的（例如使用低质量模型或偏离协议）。虽然有一些研究考虑了偏离协议的恶意服务器，但它们忽略了验证模型准确性（其中恶意服务器使用低质量模型），同时保护服务器模型和客户输入的隐私。为了解决这些问题，我们提出了textit{Fusion}，其中客户端将公共样本（具有已知查询结果）与自己的样本混合以进行多方计算的输入，以共同执行安全推断。由于使用低质量模型或偏离协议的服务器只能产生容易被客户端识别的结果，textit{Fusion}迫使服务器诚实行事，从而解决了所有上述问题，而无需利用昂贵的加密技术。我们的评估表明，textit{Fusion}比现有的恶意安全推断协议快48.06倍，使用的通信量少30.90倍（后者当前不支持模型准确性的验证）。此外，为了显示可扩展性，我们在实际的ResNet50模型上进行ImageNet规模的推断，它在WAN设置中耗时8.678分钟，通信量为10.117 GiB，比半诚实协议快1.18倍，通信量少2.64倍。

2. RAI2: Responsible Identity Audit Governing the Artificial Intelligence<br>
身份在负责任的人工智能（AI）中扮演着重要的角色：它作为深度学习（DL）模型的唯一标识符，并可用于跟踪那些对模型不负责任的使用者。因此，有效的DL身份审核对于构建负责任的AI至关重要。除了模型，训练数据集确定了模型可以学习的特征，因此在身份审核中应给予同等关注。在这项工作中，我们提出了第一个实用的方案RAI2，用于负责任的数据集和模型身份审核。我们开发了数据集和模型相似性估计方法，可与对嫌疑模型的黑盒访问一起使用。所提出的方法可以通过估计所有者和嫌疑者之间的相似性来量化地确定数据集和模型的身份。最后，我们基于承诺方案实现了我们的负责任审核方案，使所有者可以将数据集和模型注册到受信任的第三方（TTP），该第三方负责数据集和模型的监管和版权侵犯的取证。对14个模型架构和6个视觉和文本数据集的广泛评估表明，我们的方案可以通过所提出的相似性估计方法准确地识别数据集和模型。我们希望我们的审核方法不仅填补身份仲裁的空白，还能乘上AI治理在这个混乱的世界中的浪潮。
