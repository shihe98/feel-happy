# 2023 S&P 关注内容
## 联邦学习
1. 3DFed: Adaptive and Extensible Framework for Covert Backdoor Attack in Federated Learning<br>
Federated Learning（FL）是一种分布式机器学习模式，它在个体设备上本地训练数据集，但容易受到后门模型污染攻击。攻击者可以通过破坏或冒充这些设备，上传制作的恶意模型更新来操纵全局模型，在攻击者指定的触发器下实现后门行为。然而，现有的后门攻击需要更多的关于受害FL系统的信息，而且它们通常是专门为优化单一目标而设计的，这在现代FL系统中变得无效，因为这些系统往往采用深度防御来从不同角度检测后门模型。受到这些问题的启发，本文提出了3DFed，这是一个自适应、可扩展和多层次的框架，用于在黑盒设置下启动隐蔽的FL后门攻击。3DFed具有三个避免模块，可以伪装后门模型：约束损失的后门训练、噪声掩蔽和诱饵模型。通过将指示器植入后门模型中，3DFed可以从全局模型中获取先前时期的攻击反馈，并动态调整这些后门避免模块的超参数。通过广泛的实验结果，我们展示了当它的所有组件一起工作时，3DFed可以逃避所有最先进的FL后门防御，包括Deepsight、Foolsgold、FLAME、FL-Detector和RFLBAT。未来还可以将新的避免模块纳入3DFed中，因为它是一个可扩展的框架。

## 人工智能&隐私保护
1. A Theory to Instruct Differentially-Private Learning via Clipping Bias Reduction<br>
我们研究了在差分隐私随机梯度下降（DP-SGD）中引入的偏差，其中采用了裁剪或归一化的每个样本梯度。作为确保有界敏感性的最流行但人为操作之一，梯度裁剪使得许多迭代优化方法的复合隐私分析成为可能，而无需对学习模型或输入数据做出额外的假设。尽管其适用性广泛，但梯度裁剪在系统地指导隐私或效用的改善方面也存在理论上的挑战。一般来说，在没有对全局梯度进行有界假设的情况下，经典的收敛分析不适用于裁剪的梯度下降。此外，由于对效用损失的理解有限，许多现有的DP-SGD改进方法都是启发式的，特别是在私有深度学习的应用中。在本文中，我们提供了有意义的理论分析，并通过详尽的实验结果验证了DP-SGD。我们指出，在先前的工作中低估了由梯度裁剪引起的偏差。对于通过DP-SGD进行通用非凸优化，我们展示了一个导致偏差的关键因素是要裁剪的随机梯度的采样噪声。因此，我们利用所开发的理论从各个角度建立了一系列减少采样噪声的改进。从优化角度出发，我们研究了方差减少技术，并提出了内外动量。在学习模型（神经网络）层面上，我们提出了几种技巧来增强网络内部归一化和批量裁剪，以仔细裁剪一批样本的梯度。对于数据预处理，我们提供了最近提出的通过数据归一化和（自我）增强的改进的理论证明。将这些系统改进结合起来，通过DP-SGD进行私有深度学习在许多任务中可以得到显着加强。例如，在计算机视觉应用中，我们使用（epsilon=8，delta=10^-5）的DP保证，成功地在CIFAR10和SVHN上训练了ResNet20，测试精度分别为76.0％和90.1％；在自然语言处理方面，使用（epsilon=4，delta=10^-5），我们成功地在IMDb数据上训练了一个循环神经网络，测试精度为77.5％。

