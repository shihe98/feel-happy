# 2023 S&P 关注内容
## 联邦学习
1. 3DFed: Adaptive and Extensible Framework for Covert Backdoor Attack in Federated Learning<br>
Federated Learning（FL）是一种分布式机器学习模式，它在个体设备上本地训练数据集，但容易受到后门模型污染攻击。攻击者可以通过破坏或冒充这些设备，上传制作的恶意模型更新来操纵全局模型，在攻击者指定的触发器下实现后门行为。然而，现有的后门攻击需要更多的关于受害FL系统的信息，而且它们通常是专门为优化单一目标而设计的，这在现代FL系统中变得无效，因为这些系统往往采用深度防御来从不同角度检测后门模型。受到这些问题的启发，本文提出了3DFed，这是一个自适应、可扩展和多层次的框架，用于在黑盒设置下启动隐蔽的FL后门攻击。3DFed具有三个避免模块，可以伪装后门模型：约束损失的后门训练、噪声掩蔽和诱饵模型。通过将指示器植入后门模型中，3DFed可以从全局模型中获取先前时期的攻击反馈，并动态调整这些后门避免模块的超参数。通过广泛的实验结果，我们展示了当它的所有组件一起工作时，3DFed可以逃避所有最先进的FL后门防御，包括Deepsight、Foolsgold、FLAME、FL-Detector和RFLBAT。未来还可以将新的避免模块纳入3DFed中，因为它是一个可扩展的框架。

2. ADI: Adversarial Dominating Inputs in Vertical Federated Learning Systems<br>
垂直联邦学习（Vertical Federated Learning，VFL）系统近来成为一个突出的概念，用于处理分布在许多个体源中的数据，而无需将其集中。多个参与者以隐私意识的方式协作训练基于他们本地数据的模型。迄今为止，VFL已成为在组织之间安全学习模型的事实解决方案，允许共享知识而不损害任何个体的隐私。尽管VFL系统的发展蓬勃发展，但我们发现参与者的某些输入，即敌对控制输入（Adversarial Dominating Inputs，ADIs），可以支配联合推理，朝着敌对方的意愿方向，并迫使其他（受害）参与者做出微不足道的贡献，失去通常在联邦学习场景中提供的奖励。我们通过首先证明典型VFL系统中存在ADIs来对ADIs进行系统研究。然后，我们提出基于梯度的方法来合成各种格式的ADIs，并利用常见的VFL系统。我们进一步启动灰盒模糊测试，由“受害”参与者的显著性分数指导，扰动敌对控制的输入，并以隐私保护的方式系统地探索VFL攻击面。我们对合成ADIs的关键参数和设置的影响进行深入研究。我们的研究揭示了新的VFL攻击机会，促进在出现漏洞之前识别未知威胁并构建更安全的VFL系统。

3. BayBFed: Bayesian Backdoor Defense for Federated Learning<br>
联邦学习（FL）是一种新兴技术，允许参与者在不与他人共享私有数据的情况下共同训练机器学习模型。然而，FL易受污染攻击（如后门攻击）的影响。因此，最近提出了各种防御措施，这些措施主要利用全局模型的中间状态（即logit）或本地模型与全局模型的距离（即L2范数）来检测FL中的恶意后门。然而，由于这些方法直接操作客户端更新（或权重），它们的有效性取决于客户端数据分布或对手的攻击策略等因素。本文介绍了一种新型、更通用的后门防御框架BayBFed，该框架建议利用客户端更新的概率分布来检测FL中的恶意更新：BayBFed计算客户端更新的概率度量，以跟踪更新中的任何调整，并使用一种新颖的检测算法，可以利用这个概率度量来有效地检测和过滤出恶意更新。因此，它克服了以前方法的缺点，这些缺点由于直接使用客户端更新而引起；尽管如此，我们的概率度量将包括本地客户端训练策略的所有方面。BayBFed利用了两个贝叶斯非参数（BNP）扩展：（i）层次贝塔伯努利过程，用于给出客户端更新的概率度量，以及（ii）中国餐馆过程（CRP）的一种改进，我们称之为CRP-Jensen，它利用这个概率度量来检测和过滤出恶意更新。我们在五个基准数据集上广泛评估了我们的防御方法：CIFAR10、Reddit、IoT入侵检测、MNIST和FMNIST，并展示了它可以有效地检测和消除FL中的恶意更新，而不会恶化全局模型的良好性能。

4. ELSA: Secure Aggregation for Federated Learning with Malicious Actors<br>
Federated Learning（FL）是一种在训练数据集高度分布的情况下进行机器学习（ML）的越来越流行的方法。客户端在其数据集上进行本地训练，然后将更新聚合到全局模型中。现有的聚合协议要么效率低下，要么不考虑系统中恶意行为者的情况。这是使FL成为隐私敏感的ML应用程序的理想解决方案的主要障碍。我们提出了ELSA，一种安全的FL聚合协议，它打破了这个障碍——它高效并且在设计的核心处考虑了恶意行为者的存在。与Prio和Prio+的先前工作类似，ELSA提供了一种新颖的安全聚合协议，由两个服务器上的分布式信任组成，只要一个服务器是诚实的，就可以保持单个客户端更新的私密性，防御恶意客户端，并且是从头到尾高效的。与之前的工作相比，ELSA的区别主题在于，客户端充当这些相关性的不可信经销商，而不会影响协议的安全性，而不是服务器交互地生成加密相关性。这导致了一个更快的协议，同时实现了比先前工作更强的安全性。我们引入了新技术，即使服务器是恶意的，也可以以很小的额外成本（比半诚实服务器的情况下增加的通信还要少）保留隐私。我们的工作大大提高了具有类似安全保证的先前工作的端到端运行时间——对于我们考虑的模型，单个聚合器RoFL的提高高达305倍，分布式信任Prio的提高高达8倍。

## 人工智能&隐私保护
1. A Theory to Instruct Differentially-Private Learning via Clipping Bias Reduction<br>
我们研究了在差分隐私随机梯度下降（DP-SGD）中引入的偏差，其中采用了裁剪或归一化的每个样本梯度。作为确保有界敏感性的最流行但人为操作之一，梯度裁剪使得许多迭代优化方法的复合隐私分析成为可能，而无需对学习模型或输入数据做出额外的假设。尽管其适用性广泛，但梯度裁剪在系统地指导隐私或效用的改善方面也存在理论上的挑战。一般来说，在没有对全局梯度进行有界假设的情况下，经典的收敛分析不适用于裁剪的梯度下降。此外，由于对效用损失的理解有限，许多现有的DP-SGD改进方法都是启发式的，特别是在私有深度学习的应用中。在本文中，我们提供了有意义的理论分析，并通过详尽的实验结果验证了DP-SGD。我们指出，在先前的工作中低估了由梯度裁剪引起的偏差。对于通过DP-SGD进行通用非凸优化，我们展示了一个导致偏差的关键因素是要裁剪的随机梯度的采样噪声。因此，我们利用所开发的理论从各个角度建立了一系列减少采样噪声的改进。从优化角度出发，我们研究了方差减少技术，并提出了内外动量。在学习模型（神经网络）层面上，我们提出了几种技巧来增强网络内部归一化和批量裁剪，以仔细裁剪一批样本的梯度。对于数据预处理，我们提供了最近提出的通过数据归一化和（自我）增强的改进的理论证明。将这些系统改进结合起来，通过DP-SGD进行私有深度学习在许多任务中可以得到显着加强。例如，在计算机视觉应用中，我们使用（epsilon=8，delta=10^-5）的DP保证，成功地在CIFAR10和SVHN上训练了ResNet20，测试精度分别为76.0％和90.1％；在自然语言处理方面，使用（epsilon=4，delta=10^-5），我们成功地在IMDb数据上训练了一个循环神经网络，测试精度为77.5％。

2. Accuracy-Privacy Trade-off in Deep Ensemble: A Membership Inference Perspective<br>
深度集成学习已经被证明通过训练多个神经网络并平均它们的输出来提高准确性。集成学习也被建议用于防御破坏隐私的成员推断攻击。在本文中，我们实证地展示了深度集成中准确性和隐私（以成员推断攻击为代价）之间的权衡。使用各种数据集和模型架构，我们展示了集成提高准确性时成员推断攻击的有效性增加的情况。我们分析了深度集成中各种因素的影响，并展示了权衡的根本原因。然后，我们评估了基于正则化和差分隐私的常见成员推断攻击防御措施。我们展示了这些防御措施虽然可以减轻成员推断攻击的有效性，但同时也会降低集成的准确性。我们展示了在更先进和最先进的集成技术中也存在类似的权衡，例如快照集成和多样化集成网络。最后，我们提出了一种简单而有效的深度集成防御措施来打破这种权衡，从而同时提高准确性和隐私。

3. Analyzing Leakage of Personally Identifiable Information in Language Models<br>
语言模型（LMs）已被证明通过句子级成员推断和重构攻击泄漏有关训练数据的信息。了解LMs泄漏个人身份信息（PII）的风险受到较少关注，这可以归因于虚假的假设，即数据集清理技术如擦洗足以防止PII泄漏。擦洗技术减少但不能防止PII泄漏的风险：在实践中，擦洗是不完美的，必须权衡最小化披露和保留数据集的效用之间的平衡。另一方面，不清楚算法防御，如差分隐私，旨在保证句子或用户级隐私，能够防止PII披露的程度。在这项工作中，我们引入了三种PII泄漏类型的严格基于游戏的定义，通过只使用LM的API访问进行黑盒提取、推断和重构攻击。我们在三个领域中评估了带有和不带有防御的GPT-2模型的攻击：案例法、医疗保健和电子邮件。我们的主要贡献是：（i）新颖的攻击可以提取多达10倍的PII序列，比现有攻击更有效；（ii）显示句子级差分隐私减少了PII披露的风险，但仍泄漏约3%的PII序列；（iii）记录级成员推断和PII重构之间的微妙联系。

4. Continual Observation under User-level Differential Privacy<br>
Dwork等人在差分隐私的持续观察方面的基础工作中提出了两种隐私模型：事件级别的DP和用户级别的DP。后者提供了更强的隐私保护，因为它允许用户贡献任意数量的项目。在事件级别DP下，他们的机制与所有保持联合的函数的静态设置下的最优效用界相匹配，直到对数多项式因子。不幸的是，与事件级别DP的强结果相比，他们的用户级别DP机制具有较弱的效用保证，并对数据施加许多限制。在本文中，我们采用特定实例的方法，设计了在用户级别DP下多个基本函数的持续观测机制。我们的机制不需要任何先验数据限制，同时提供了随着数据难度的增加而逐渐降低的效用保证。对于计数和求和函数，我们的机制是下邻域最优的，与静态设置相匹配，直到对数多项式因子。对于其他函数，它们不匹配静态情况，但我们证明这是不可避免的，这是差分隐私下持续观察的第一个分离结果。

5. D-DAE: Defense-Penetrating Model Extraction Attacks<br>
最近的研究表明，机器学习模型容易受到模型提取攻击的威胁，攻击者可以通过查询受害模型来构建一个几乎达到相同性能的替代模型。为了防止这种攻击，一系列方法已被提出，以在返回结果之前破坏查询结果，大大降低现有模型提取攻击的性能。在本文中，我们首次尝试开发一种防御-渗透模型提取攻击框架，命名为D-DAE，旨在打破基于破坏的防御。D-DAE的关键是设计两个模块，即破坏检测和破坏恢复，它们可以与通用模型提取攻击集成。更具体地说，在从受害模型获取查询结果之后，破坏检测模块推断出捍卫者采用的防御机制。我们设计了一种基于元学习的破坏检测算法，用于学习破坏和未破坏查询结果分布之间的基本差异。即使我们无法访问受害模型的原始训练数据集，该算法也具有很好的泛化性质。在检测到防御机制后，破坏恢复模块试图使用精心设计的生成模型从破坏的查询结果中恢复出干净的查询结果。我们在MNIST、FashionMNIST、CIFAR-10、GTSRB和ImageNette数据集上进行了广泛的评估，结果表明，在面对4种最先进的防御措施和多种防御措施组合时，D-DAE可以将现有模型提取攻击的替代模型准确率提高高达82.24%。我们还验证了D-DAE在穿透微软Azure和Face++托管的真实世界API中未知防御方面的有效性。

## 人工智能&安全威胁
1. AI-Guardian: Defeating Adversarial Attacks using Backdoors<br>
深度神经网络（DNNs）由于其越来越高的准确性已被广泛应用于许多领域。然而，它们也容易受到对抗性攻击，对于自动驾驶、远程诊断等安全关键应用构成严重威胁。现有解决方案在检测/预防此类攻击方面存在局限性，也会影响原始任务的性能。本文提出了AI-Guardian，一种新型方法，利用有意嵌入的后门来失败对抗性扰动，同时保持原始主任务的性能。我们使用五种流行的对抗性样例生成方法对AI-Guardian进行了广泛评估，实验结果表明其在打败对抗性攻击方面的有效性。具体来说，AI-Guardian将攻击成功率从97.3%降低到3.2%，超过最先进的工作30.9%，仅在干净数据准确率下降0.9%。此外，AI-Guardian在模型预测时间方面只引入了0.36%的开销，在大多数情况下几乎可以忽略不计。

2. Breaking Security-Critical Voice Authentication<br>
语音认证（VA）最近已成为许多安全关键操作的不可或缺的组成部分，例如银行交易和呼叫中心对话。自动说话者验证系统（ASVs）对欺骗攻击的脆弱性促进了对抗措施（CMs）的开发，其任务是区分真实和欺骗性言语。ASVs和CMs一起形成了今天的VA系统，并被宣传为不可攻破的访问控制机制。我们开发了第一种实际攻击欺骗对策的方法，并展示了恶意行为者如何有效地针对这些防御措施制作音频样本。以前针对VA的对抗性攻击主要是针对白盒场景设计的，这种场景假设了对系统内部的了解，或需要大量的查询和时间预算来发起针对特定目标的攻击。在攻击安全关键系统时，这些假设不成立。另一方面，我们的攻击针对所有欺骗对策共享的常见失败点，使得它实时，模型不可知，并且完全是黑盒，无需与目标交互来制作攻击样本。我们工作的关键信息是，CMs错误地学习区分基于易于识别和伪造的线索的欺骗和真实音频。我们的攻击效果足够微妙，可以保证这些对抗样本仍然可以绕过ASV并保留其原始文本内容。这些属性组合起来形成了一个强大的攻击，可以绕过安全关键的VA，在其最严格的形式下产生高达99％的成功率，仅需要6次尝试。最后，我们对CMs进行了第一个针对电话网络的有针对性攻击，绕过了几个已知的挑战，为各种潜在威胁提供了可能，考虑到语音生物识别在呼叫中心中的增加使用。我们的结果质疑了现代VA系统的安全性，并敦促用户重新考虑对它们的信任，考虑到攻击者绕过这些措施以获得他们最有价值的资源的真实威胁。

3. Deepfake Text Detection: Limitations and Opportunities<br>
近年来，语言生成模型的进步使得人们可以创建逼真的合成文本或深度伪造文本。以往的研究已经表明，深度伪造文本的滥用可能会误导内容消费者。因此，深度伪造文本检测，即区分人类生成和机器生成文本的任务变得越来越关键。已经提出了几种深度伪造文本检测的防御方法。然而，我们缺乏对它们在现实世界中适用性的全面理解。在本文中，我们从4个基于Transformer的工具驱动的在线服务中收集深度伪造文本，以评估防御措施在野外内容上的泛化能力。我们开发了几种低成本的对抗攻击，并研究了现有防御措施对自适应攻击者的鲁棒性。我们发现，许多防御措施在我们的评估场景下与其原始声称的性能相比，表现出显著的性能下降。我们的评估表明，利用文本内容中的语义信息是提高深度伪造文本检测方案的鲁棒性和泛化性能的有前途的方法。

4. DepthFake: Spoofing 3D Face Authentication with a 2D Photo<br>
人脸识别已被广泛应用于门禁控制，最新的3D人脸识别系统采用3D活体检测技术来应对照片重放攻击，即攻击者使用2D照片来绕过身份验证。本文分析了利用结构光深度摄像机的3D活体检测系统的安全性，发现了一种针对3D人脸识别系统的新的攻击方式。我们提出了DepthFake攻击，可以仅使用一张2D照片欺骗3D人脸识别。为了实现这一目标，DepthFake首先从目标受害者的2D照片中估算出其面部的3D深度信息。然后，DepthFake投射嵌入面部深度信息的精心制作的散射图案，以赋予2D照片3D认证属性。我们克服了一系列实际挑战，例如从2D照片中的深度估算误差，基于结构光的深度图像伪造，为面部对齐RGB图像和深度图像，并在实验室环境中实现了DepthFake。我们在三个商业化人脸识别系统（即腾讯云、百度云和3DiVi）和一个商业化门禁设备上验证了DepthFake。50个用户的结果表明，DepthFake在现实世界中的深度攻击成功率为79.4％，RGB-D攻击成功率为59.4％。

5. Disguising Attacks with Explanation-Aware Backdoors<br>
可解释的机器学习在分析和理解基于学习的系统方面具有巨大潜力。然而，这些方法可以被操纵以提供不可靠的解释，从而产生强大而隐蔽的对手。在本文中，我们展示了如何完全掩盖机器学习模型的对抗操作。与神经后门类似，我们在触发器存在时修改模型的预测，但同时欺骗了后续进行分析的解释方法。这使得对手可以隐藏触发器的存在，或者将解释指向输入的完全不同部分，引入一个红鱼。我们对图像领域中的梯度和传播解释方法的这些解释感知后门的不同表现进行了分析，然后继续对恶意软件分类进行了红鱼攻击。





