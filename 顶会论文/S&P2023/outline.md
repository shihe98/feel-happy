# 2023 S&P 关注内容
## 联邦学习
1. 3DFed: Adaptive and Extensible Framework for Covert Backdoor Attack in Federated Learning<br>
Federated Learning（FL）是一种分布式机器学习模式，它在个体设备上本地训练数据集，但容易受到后门模型污染攻击。攻击者可以通过破坏或冒充这些设备，上传制作的恶意模型更新来操纵全局模型，在攻击者指定的触发器下实现后门行为。然而，现有的后门攻击需要更多的关于受害FL系统的信息，而且它们通常是专门为优化单一目标而设计的，这在现代FL系统中变得无效，因为这些系统往往采用深度防御来从不同角度检测后门模型。受到这些问题的启发，本文提出了3DFed，这是一个自适应、可扩展和多层次的框架，用于在黑盒设置下启动隐蔽的FL后门攻击。3DFed具有三个避免模块，可以伪装后门模型：约束损失的后门训练、噪声掩蔽和诱饵模型。通过将指示器植入后门模型中，3DFed可以从全局模型中获取先前时期的攻击反馈，并动态调整这些后门避免模块的超参数。通过广泛的实验结果，我们展示了当它的所有组件一起工作时，3DFed可以逃避所有最先进的FL后门防御，包括Deepsight、Foolsgold、FLAME、FL-Detector和RFLBAT。未来还可以将新的避免模块纳入3DFed中，因为它是一个可扩展的框架。

2. ADI: Adversarial Dominating Inputs in Vertical Federated Learning Systems<br>
垂直联邦学习（Vertical Federated Learning，VFL）系统近来成为一个突出的概念，用于处理分布在许多个体源中的数据，而无需将其集中。多个参与者以隐私意识的方式协作训练基于他们本地数据的模型。迄今为止，VFL已成为在组织之间安全学习模型的事实解决方案，允许共享知识而不损害任何个体的隐私。尽管VFL系统的发展蓬勃发展，但我们发现参与者的某些输入，即敌对控制输入（Adversarial Dominating Inputs，ADIs），可以支配联合推理，朝着敌对方的意愿方向，并迫使其他（受害）参与者做出微不足道的贡献，失去通常在联邦学习场景中提供的奖励。我们通过首先证明典型VFL系统中存在ADIs来对ADIs进行系统研究。然后，我们提出基于梯度的方法来合成各种格式的ADIs，并利用常见的VFL系统。我们进一步启动灰盒模糊测试，由“受害”参与者的显著性分数指导，扰动敌对控制的输入，并以隐私保护的方式系统地探索VFL攻击面。我们对合成ADIs的关键参数和设置的影响进行深入研究。我们的研究揭示了新的VFL攻击机会，促进在出现漏洞之前识别未知威胁并构建更安全的VFL系统。

3. BayBFed: Bayesian Backdoor Defense for Federated Learning<br>
联邦学习（FL）是一种新兴技术，允许参与者在不与他人共享私有数据的情况下共同训练机器学习模型。然而，FL易受污染攻击（如后门攻击）的影响。因此，最近提出了各种防御措施，这些措施主要利用全局模型的中间状态（即logit）或本地模型与全局模型的距离（即L2范数）来检测FL中的恶意后门。然而，由于这些方法直接操作客户端更新（或权重），它们的有效性取决于客户端数据分布或对手的攻击策略等因素。本文介绍了一种新型、更通用的后门防御框架BayBFed，该框架建议利用客户端更新的概率分布来检测FL中的恶意更新：BayBFed计算客户端更新的概率度量，以跟踪更新中的任何调整，并使用一种新颖的检测算法，可以利用这个概率度量来有效地检测和过滤出恶意更新。因此，它克服了以前方法的缺点，这些缺点由于直接使用客户端更新而引起；尽管如此，我们的概率度量将包括本地客户端训练策略的所有方面。BayBFed利用了两个贝叶斯非参数（BNP）扩展：（i）层次贝塔伯努利过程，用于给出客户端更新的概率度量，以及（ii）中国餐馆过程（CRP）的一种改进，我们称之为CRP-Jensen，它利用这个概率度量来检测和过滤出恶意更新。我们在五个基准数据集上广泛评估了我们的防御方法：CIFAR10、Reddit、IoT入侵检测、MNIST和FMNIST，并展示了它可以有效地检测和消除FL中的恶意更新，而不会恶化全局模型的良好性能。

4. ELSA: Secure Aggregation for Federated Learning with Malicious Actors<br>
Federated Learning（FL）是一种在训练数据集高度分布的情况下进行机器学习（ML）的越来越流行的方法。客户端在其数据集上进行本地训练，然后将更新聚合到全局模型中。现有的聚合协议要么效率低下，要么不考虑系统中恶意行为者的情况。这是使FL成为隐私敏感的ML应用程序的理想解决方案的主要障碍。我们提出了ELSA，一种安全的FL聚合协议，它打破了这个障碍——它高效并且在设计的核心处考虑了恶意行为者的存在。与Prio和Prio+的先前工作类似，ELSA提供了一种新颖的安全聚合协议，由两个服务器上的分布式信任组成，只要一个服务器是诚实的，就可以保持单个客户端更新的私密性，防御恶意客户端，并且是从头到尾高效的。与之前的工作相比，ELSA的区别主题在于，客户端充当这些相关性的不可信经销商，而不会影响协议的安全性，而不是服务器交互地生成加密相关性。这导致了一个更快的协议，同时实现了比先前工作更强的安全性。我们引入了新技术，即使服务器是恶意的，也可以以很小的额外成本（比半诚实服务器的情况下增加的通信还要少）保留隐私。我们的工作大大提高了具有类似安全保证的先前工作的端到端运行时间——对于我们考虑的模型，单个聚合器RoFL的提高高达305倍，分布式信任Prio的提高高达8倍。

5. FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information<br>
联邦学习容易遭受中毒攻击，即恶意客户端通过向服务器发送恶意模型更新来污染全局模型。现有的防御方法主要集中在通过强健的联邦学习方法防止少数恶意客户端污染全局模型，以及在存在大量恶意客户端时检测恶意客户端。然而，如何在检测到恶意客户端后从中毒攻击中恢复全局模型仍然是一个开放性挑战。一个朴素的解决方法是移除检测到的恶意客户端，并使用剩余客户端重新从头开始训练全局模型。然而，这种从头开始训练的恢复方法会产生大量的计算和通信成本，这对于资源受限的客户端如智能手机和物联网设备可能是无法承受的。在本文中，我们提出了FedRecover，一种可以在客户端承受的小计算和通信成本下从中毒攻击中恢复精确全局模型的方法。我们的关键思想是服务器估计客户端的模型更新，而不是在恢复过程中要求客户端计算和通信这些更新。具体来说，在训练被污染的全局模型之前，服务器存储历史信息，包括每轮的全局模型和客户端的模型更新。在恢复过程中，服务器使用其存储的历史信息估计每轮客户端的模型更新。此外，我们进一步优化了FedRecover，通过预热、定期校正、异常修复和最终调整策略来恢复更精确的全局模型，其中服务器要求客户端计算和通信其准确的模型更新。从理论上讲，我们证明了在某些假设下，FedRecover恢复的全局模型接近或与从头开始训练恢复的全局模型相同。从实证角度来看，我们在四个数据集、三种联邦学习方法以及无目标和有目标中毒攻击（例如后门攻击）上的评估表明，FedRecover既准确又高效。

6. Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning<br>
本文介绍了Flamingo，这是一个用于在大量客户端之间进行安全聚合数据的系统。在安全聚合中，服务器将客户端的私有输入相加，获得结果，而不了解除最终总和之外的任何个体输入信息。Flamingo专注于联邦学习中发现的多轮设置，其中执行多个连续的模型权重求和（平均）以获得良好的模型。以前的协议，例如Bell等人（CCS’20）设计了单轮协议，并通过多次重复协议来适应联邦学习设置。Flamingo消除了以前协议每轮设置的需要，并具有新的轻量级丢弃容错协议，以确保如果客户端在求和过程中离开，则服务器仍然可以获得有意义的结果。此外，Flamingo引入了一种新的本地选择所谓的客户端邻域的方法，这是由Bell等人引入的。这些技术帮助Flamingo减少了客户端与服务器之间的交互次数，从而显着减少了完整训练会话的端到端运行时间。我们实现和评估了Flamingo，并展示它可以在（扩展）MNIST和CIFAR-100数据集上安全地训练神经网络，与非私有联邦学习系统相比，模型收敛而不会损失准确性。

## 人工智能&隐私保护
1. A Theory to Instruct Differentially-Private Learning via Clipping Bias Reduction<br>
我们研究了在差分隐私随机梯度下降（DP-SGD）中引入的偏差，其中采用了裁剪或归一化的每个样本梯度。作为确保有界敏感性的最流行但人为操作之一，梯度裁剪使得许多迭代优化方法的复合隐私分析成为可能，而无需对学习模型或输入数据做出额外的假设。尽管其适用性广泛，但梯度裁剪在系统地指导隐私或效用的改善方面也存在理论上的挑战。一般来说，在没有对全局梯度进行有界假设的情况下，经典的收敛分析不适用于裁剪的梯度下降。此外，由于对效用损失的理解有限，许多现有的DP-SGD改进方法都是启发式的，特别是在私有深度学习的应用中。在本文中，我们提供了有意义的理论分析，并通过详尽的实验结果验证了DP-SGD。我们指出，在先前的工作中低估了由梯度裁剪引起的偏差。对于通过DP-SGD进行通用非凸优化，我们展示了一个导致偏差的关键因素是要裁剪的随机梯度的采样噪声。因此，我们利用所开发的理论从各个角度建立了一系列减少采样噪声的改进。从优化角度出发，我们研究了方差减少技术，并提出了内外动量。在学习模型（神经网络）层面上，我们提出了几种技巧来增强网络内部归一化和批量裁剪，以仔细裁剪一批样本的梯度。对于数据预处理，我们提供了最近提出的通过数据归一化和（自我）增强的改进的理论证明。将这些系统改进结合起来，通过DP-SGD进行私有深度学习在许多任务中可以得到显着加强。例如，在计算机视觉应用中，我们使用（epsilon=8，delta=10^-5）的DP保证，成功地在CIFAR10和SVHN上训练了ResNet20，测试精度分别为76.0％和90.1％；在自然语言处理方面，使用（epsilon=4，delta=10^-5），我们成功地在IMDb数据上训练了一个循环神经网络，测试精度为77.5％。

2. Accuracy-Privacy Trade-off in Deep Ensemble: A Membership Inference Perspective<br>
深度集成学习已经被证明通过训练多个神经网络并平均它们的输出来提高准确性。集成学习也被建议用于防御破坏隐私的成员推断攻击。在本文中，我们实证地展示了深度集成中准确性和隐私（以成员推断攻击为代价）之间的权衡。使用各种数据集和模型架构，我们展示了集成提高准确性时成员推断攻击的有效性增加的情况。我们分析了深度集成中各种因素的影响，并展示了权衡的根本原因。然后，我们评估了基于正则化和差分隐私的常见成员推断攻击防御措施。我们展示了这些防御措施虽然可以减轻成员推断攻击的有效性，但同时也会降低集成的准确性。我们展示了在更先进和最先进的集成技术中也存在类似的权衡，例如快照集成和多样化集成网络。最后，我们提出了一种简单而有效的深度集成防御措施来打破这种权衡，从而同时提高准确性和隐私。

3. Analyzing Leakage of Personally Identifiable Information in Language Models<br>
语言模型（LMs）已被证明通过句子级成员推断和重构攻击泄漏有关训练数据的信息。了解LMs泄漏个人身份信息（PII）的风险受到较少关注，这可以归因于虚假的假设，即数据集清理技术如擦洗足以防止PII泄漏。擦洗技术减少但不能防止PII泄漏的风险：在实践中，擦洗是不完美的，必须权衡最小化披露和保留数据集的效用之间的平衡。另一方面，不清楚算法防御，如差分隐私，旨在保证句子或用户级隐私，能够防止PII披露的程度。在这项工作中，我们引入了三种PII泄漏类型的严格基于游戏的定义，通过只使用LM的API访问进行黑盒提取、推断和重构攻击。我们在三个领域中评估了带有和不带有防御的GPT-2模型的攻击：案例法、医疗保健和电子邮件。我们的主要贡献是：（i）新颖的攻击可以提取多达10倍的PII序列，比现有攻击更有效；（ii）显示句子级差分隐私减少了PII披露的风险，但仍泄漏约3%的PII序列；（iii）记录级成员推断和PII重构之间的微妙联系。

4. Continual Observation under User-level Differential Privacy<br>
Dwork等人在差分隐私的持续观察方面的基础工作中提出了两种隐私模型：事件级别的DP和用户级别的DP。后者提供了更强的隐私保护，因为它允许用户贡献任意数量的项目。在事件级别DP下，他们的机制与所有保持联合的函数的静态设置下的最优效用界相匹配，直到对数多项式因子。不幸的是，与事件级别DP的强结果相比，他们的用户级别DP机制具有较弱的效用保证，并对数据施加许多限制。在本文中，我们采用特定实例的方法，设计了在用户级别DP下多个基本函数的持续观测机制。我们的机制不需要任何先验数据限制，同时提供了随着数据难度的增加而逐渐降低的效用保证。对于计数和求和函数，我们的机制是下邻域最优的，与静态设置相匹配，直到对数多项式因子。对于其他函数，它们不匹配静态情况，但我们证明这是不可避免的，这是差分隐私下持续观察的第一个分离结果。

5. D-DAE: Defense-Penetrating Model Extraction Attacks<br>
最近的研究表明，机器学习模型容易受到模型提取攻击的威胁，攻击者可以通过查询受害模型来构建一个几乎达到相同性能的替代模型。为了防止这种攻击，一系列方法已被提出，以在返回结果之前破坏查询结果，大大降低现有模型提取攻击的性能。在本文中，我们首次尝试开发一种防御-渗透模型提取攻击框架，命名为D-DAE，旨在打破基于破坏的防御。D-DAE的关键是设计两个模块，即破坏检测和破坏恢复，它们可以与通用模型提取攻击集成。更具体地说，在从受害模型获取查询结果之后，破坏检测模块推断出捍卫者采用的防御机制。我们设计了一种基于元学习的破坏检测算法，用于学习破坏和未破坏查询结果分布之间的基本差异。即使我们无法访问受害模型的原始训练数据集，该算法也具有很好的泛化性质。在检测到防御机制后，破坏恢复模块试图使用精心设计的生成模型从破坏的查询结果中恢复出干净的查询结果。我们在MNIST、FashionMNIST、CIFAR-10、GTSRB和ImageNette数据集上进行了广泛的评估，结果表明，在面对4种最先进的防御措施和多种防御措施组合时，D-DAE可以将现有模型提取攻击的替代模型准确率提高高达82.24%。我们还验证了D-DAE在穿透微软Azure和Face++托管的真实世界API中未知防御方面的有效性。

6. On the (In)security of Peer-to-Peer Decentralized Machine Learning<br>
在这项工作中，我们进行了首次深入的隐私分析，研究了分布式学习——一种协作机器学习框架，旨在解决联邦学习的主要限制。我们引入了一套新的攻击方式，包括被动和主动分布式对手。我们证明，与分散学习提议者所声称的相反，分散学习并没有提供比联邦学习更多的安全优势。相反，它增加了攻击面，使系统中的任何用户都能够执行隐私攻击，如梯度反演，甚至获得对诚实用户的本地模型的完全控制。我们还表明，鉴于现有保护技术，隐私保护配置的分散学习需要全连接网络，失去了与联邦设置相比的任何实际优势，因此完全打败了分散方法的目标。

7. Private, Efficient, and Accurate: Protecting Models Trained by Multi-party Learning with Differential Privacy<br>
安全多方计算机器学习，简称多方学习（MPL），已成为利用保护隐私的多方数据的重要技术。虽然MPL为计算过程提供了严格的安全保障，但由MPL训练的模型仍然容易受到仅依赖于对模型的访问的攻击。差分隐私可以帮助抵御这种攻击，但差分隐私带来的精度损失和安全多方计算协议的巨大通信开销使得在隐私、效率和准确性之间平衡三方面非常具有挑战性。本文旨在通过提出一个解决方案（称为PEA，私有、高效、准确），解决上述问题。该方案包括一个安全差分隐私随机梯度下降（DPSGD）协议和两种优化方法。首先，我们提出一个安全的DPSGD协议，以在基于秘密共享的MPL框架中执行DPSGD，该算法是一种流行的差分隐私机器学习算法。其次，为了降低差分隐私噪声引起的精度损失和MPL的巨大通信开销，我们提出了两种优化方法来训练MPL模型：（1）数据无关特征提取方法，旨在简化训练模型结构；（2）基于本地数据的全局模型初始化方法，旨在加速模型训练的收敛。我们在两个开源MPL框架TF-Encrypted和Queqiao中实现了PEA。在各种数据集上的实验结果证明了PEA的效率和有效性。例如，当ε=2时，我们可以在LAN环境下，在7分钟内训练出一个准确率为88％的差分隐私分类模型，用于CIFAR-10。该结果显著优于CryptGPU的结果，后者是一种最先进的MPL框架：使用相同的准确性，在CIFAR-10上训练一个非私有深度神经网络模型需要耗费超过16小时。
## 人工智能&安全威胁
1. AI-Guardian: Defeating Adversarial Attacks using Backdoors<br>
深度神经网络（DNNs）由于其越来越高的准确性已被广泛应用于许多领域。然而，它们也容易受到对抗性攻击，对于自动驾驶、远程诊断等安全关键应用构成严重威胁。现有解决方案在检测/预防此类攻击方面存在局限性，也会影响原始任务的性能。本文提出了AI-Guardian，一种新型方法，利用有意嵌入的后门来失败对抗性扰动，同时保持原始主任务的性能。我们使用五种流行的对抗性样例生成方法对AI-Guardian进行了广泛评估，实验结果表明其在打败对抗性攻击方面的有效性。具体来说，AI-Guardian将攻击成功率从97.3%降低到3.2%，超过最先进的工作30.9%，仅在干净数据准确率下降0.9%。此外，AI-Guardian在模型预测时间方面只引入了0.36%的开销，在大多数情况下几乎可以忽略不计。

2. Breaking Security-Critical Voice Authentication<br>
语音认证（VA）最近已成为许多安全关键操作的不可或缺的组成部分，例如银行交易和呼叫中心对话。自动说话者验证系统（ASVs）对欺骗攻击的脆弱性促进了对抗措施（CMs）的开发，其任务是区分真实和欺骗性言语。ASVs和CMs一起形成了今天的VA系统，并被宣传为不可攻破的访问控制机制。我们开发了第一种实际攻击欺骗对策的方法，并展示了恶意行为者如何有效地针对这些防御措施制作音频样本。以前针对VA的对抗性攻击主要是针对白盒场景设计的，这种场景假设了对系统内部的了解，或需要大量的查询和时间预算来发起针对特定目标的攻击。在攻击安全关键系统时，这些假设不成立。另一方面，我们的攻击针对所有欺骗对策共享的常见失败点，使得它实时，模型不可知，并且完全是黑盒，无需与目标交互来制作攻击样本。我们工作的关键信息是，CMs错误地学习区分基于易于识别和伪造的线索的欺骗和真实音频。我们的攻击效果足够微妙，可以保证这些对抗样本仍然可以绕过ASV并保留其原始文本内容。这些属性组合起来形成了一个强大的攻击，可以绕过安全关键的VA，在其最严格的形式下产生高达99％的成功率，仅需要6次尝试。最后，我们对CMs进行了第一个针对电话网络的有针对性攻击，绕过了几个已知的挑战，为各种潜在威胁提供了可能，考虑到语音生物识别在呼叫中心中的增加使用。我们的结果质疑了现代VA系统的安全性，并敦促用户重新考虑对它们的信任，考虑到攻击者绕过这些措施以获得他们最有价值的资源的真实威胁。

3. Deepfake Text Detection: Limitations and Opportunities<br>
近年来，语言生成模型的进步使得人们可以创建逼真的合成文本或深度伪造文本。以往的研究已经表明，深度伪造文本的滥用可能会误导内容消费者。因此，深度伪造文本检测，即区分人类生成和机器生成文本的任务变得越来越关键。已经提出了几种深度伪造文本检测的防御方法。然而，我们缺乏对它们在现实世界中适用性的全面理解。在本文中，我们从4个基于Transformer的工具驱动的在线服务中收集深度伪造文本，以评估防御措施在野外内容上的泛化能力。我们开发了几种低成本的对抗攻击，并研究了现有防御措施对自适应攻击者的鲁棒性。我们发现，许多防御措施在我们的评估场景下与其原始声称的性能相比，表现出显著的性能下降。我们的评估表明，利用文本内容中的语义信息是提高深度伪造文本检测方案的鲁棒性和泛化性能的有前途的方法。

4. DepthFake: Spoofing 3D Face Authentication with a 2D Photo<br>
人脸识别已被广泛应用于门禁控制，最新的3D人脸识别系统采用3D活体检测技术来应对照片重放攻击，即攻击者使用2D照片来绕过身份验证。本文分析了利用结构光深度摄像机的3D活体检测系统的安全性，发现了一种针对3D人脸识别系统的新的攻击方式。我们提出了DepthFake攻击，可以仅使用一张2D照片欺骗3D人脸识别。为了实现这一目标，DepthFake首先从目标受害者的2D照片中估算出其面部的3D深度信息。然后，DepthFake投射嵌入面部深度信息的精心制作的散射图案，以赋予2D照片3D认证属性。我们克服了一系列实际挑战，例如从2D照片中的深度估算误差，基于结构光的深度图像伪造，为面部对齐RGB图像和深度图像，并在实验室环境中实现了DepthFake。我们在三个商业化人脸识别系统（即腾讯云、百度云和3DiVi）和一个商业化门禁设备上验证了DepthFake。50个用户的结果表明，DepthFake在现实世界中的深度攻击成功率为79.4％，RGB-D攻击成功率为59.4％。

5. Disguising Attacks with Explanation-Aware Backdoors<br>
可解释的机器学习在分析和理解基于学习的系统方面具有巨大潜力。然而，这些方法可以被操纵以提供不可靠的解释，从而产生强大而隐蔽的对手。在本文中，我们展示了如何完全掩盖机器学习模型的对抗操作。与神经后门类似，我们在触发器存在时修改模型的预测，但同时欺骗了后续进行分析的解释方法。这使得对手可以隐藏触发器的存在，或者将解释指向输入的完全不同部分，引入一个红鱼。我们对图像领域中的梯度和传播解释方法的这些解释感知后门的不同表现进行了分析，然后继续对恶意软件分类进行了红鱼攻击。

6. ImU: Physical Impersonating Attack for Face Recognition System with Natural Style Changes<br>
本文介绍了一种新的针对人脸识别系统的物理冒充攻击。其旨在在不同条件和姿势下生成攻击者多张照片的一致风格变化。此外，这些风格变化需要能够通过化妆来实现，并能够诱导所需的错误分类。为了实现这一目标，我们开发了新的技术，将同一物理人的多张照片嵌入到StyleGAN潜空间中的向量中，使得嵌入的潜空间向量具有一些隐式相关性，以便于寻找一致的风格变化。我们的数字和物理评估结果表明，我们的方法可以允许外部攻击者成功冒充内部人员，并进行一致自然的变化。

7. Jigsaw Puzzle: Selective Backdoor Attack to Subvert Malware Classifiers<br>
恶意软件分类器在训练过程中容易受到利用，因为需要定期重新训练，使用从野外收集的样本。最近的研究已经证明了针对恶意软件分类器的后门攻击的可行性，但此类攻击的隐蔽性尚不明确。本文重点研究Android恶意软件分类器，在清洁标签设置下（即攻击者无法完全控制训练过程或毒化数据的标签），研究后门攻击。实证上，我们表明现有的针对恶意软件分类器的后门攻击仍然可以被最近的防御措施（如MNTD）检测到。为了提高隐蔽性，我们提出了一种新的攻击方法，拼图游戏（JP），基于这样一个关键观察：恶意软件作者几乎没有动机去保护其他作者的恶意软件，只会保护自己的恶意软件。因此，拼图游戏学习一种触发器来补充恶意软件作者样本的潜在模式，并仅在触发器和潜在模式在样本中被拼接在一起时激活后门。我们进一步关注使用从良性软件广泛收集的字节码小工具在问题空间中（例如软件代码）实现触发器。我们的评估证实，拼图游戏作为后门是有效的，仍然隐蔽于最先进的防御措施之下，并且是在从仅考虑特征空间攻击的情况下推理出的现实设置中的威胁。最后，我们探讨了改进后门防御的有前途的方法。

8. MagBackdoor: Beware of Your Loudspeaker as a Backdoor for Magnetic Injection Attacks<br>
这篇论文介绍了MagBackdoor，它是第一个通过音频系统的扬声器后门注入恶意命令的磁场攻击，可以破坏连接的语音交互系统。MagBackdoor专注于扬声器的磁性威胁，并偷偷地操纵它们的声音产生。因此，由于内部音频系统的密集排列，麦克风不可避免地会捕捉到受攻击扬声器生成的恶意声音。为了证明MagBackdoor的可行性，我们进行了全面的模拟和实验。该研究还对外部磁场激发扬声器声音产生的机制进行了建模，为MagBackdoor提供了理论指导。为了针对真实场景中的隐秘磁场攻击，我们自行设计了一个原型，可以发射由语音命令调制的磁场。我们实施了MagBackdoor，并在涉及16部智能手机、四台笔记本电脑、两台平板电脑和三台智能扬声器的各种智能设备上进行了评估，实现了平均95%的注入成功率，注入的声学信号质量高。

9. ObjectSeeker: Certifiably Robust Object Detection against Patch Hiding Attacks via Patch-agnostic Masking<br>
物体检测器被广泛应用于安全关键系统，如自动驾驶车辆，在面对补丁隐藏攻击时被发现存在漏洞。攻击者可以使用一个物理可实现的对抗性补丁来使物体检测器错过目标物体的检测，破坏物体检测应用的功能。本文中，我们提出了ObjectSeeker，以对抗补丁隐藏攻击进行可证明的强健物体检测。ObjectSeeker的关键洞见是补丁不可知掩蔽：我们的目标是在不知道补丁的形状、大小和位置的情况下，遮盖整个对抗性补丁。这种掩蔽操作可以中和对抗性效果，使任何基本的物体检测器能够安全地检测被遮盖的图像上的物体。值得注意的是，我们可以以可证明的方式评估ObjectSeeker的强健性：我们开发了一种认证过程，以正式确定ObjectSeeker是否能够检测某些物体，并在威胁模型内对抗任何白盒自适应攻击，实现可证明的强健性。我们的实验证明，在可证明的强健性方面，ObjectSeeker相对于之前的研究实现了显著的（约10%-40%的绝对值和约2-6倍的相对值）提升，同时具有高的清洁性能（与未防御模型相比下降约1%）。

10. On The Empirical Effectiveness of Unrealistic Adversarial Hardening Against Realistic Adversarial Attacks<br>
最近的研究引起了人们对现实世界中逼真敌对攻击的关注，这一领域的研究往往关注于机器学习系统的安全攻击和防御，但主要集中在不切实际的敌对样本上。我们的论文为更好地理解现实攻击下的敌对鲁棒性打下了基础，并做出了两个主要贡献。首先，我们对三个现实世界的用例（文本分类、僵尸网络检测、恶意软件检测）和七个数据集进行了研究，以评估不切实际的敌对样本是否能够用于保护模型免受现实世界中的攻击。我们的结果揭示了不同用例之间的差异，其中不切实际的样本可能与逼真的样本一样有效，或者只能提供有限的改进。其次，为了解释这些结果，我们分析了用现实和不切实际的攻击生成的敌对样本的潜在表征。我们揭示了区分哪些不切实际的样本可以用于有效硬化的模式。我们发布了我们的代码、数据集和模型，以支持未来探索如何缩小不切实际和现实敌对攻击之间差距的研究。

11. PublicCheck: Public Integrity Verification for Services of Run-time Deep Models<br>
现有的深度模型完整性验证方法都是针对私有验证设计的（即假设服务提供商是诚实的，具有白盒访问模型参数）。然而，私有验证方法不允许模型用户在运行时验证模型。相反，他们必须信任服务提供商，而服务提供商可能会篡改验证结果。相比之下，考虑到不诚实的服务提供商可能存在的公共验证方法可以使更广泛的用户受益。在本文中，我们提出了PublicCheck，一种实用的公共完整性验证解决方案，适用于运行时深度模型的服务。PublicCheck考虑了不诚实的服务提供商，克服了轻量级、提供防伪保护和具有平滑外观的指纹样本的公共验证挑战。为了捕捉和指纹化运行时模型的内在预测行为，PublicCheck生成了平滑转换和增强的囊肿样本，这些样本围绕模型的决策边界封闭，同时确保验证查询与正常查询不可区分。PublicCheck在目标模型的知识受限的情况下（例如没有梯度或模型参数的知识）也适用。PublicCheck的彻底评估展示了模型完整性违规检测（对各种模型完整性攻击和模型压缩攻击的100％检测准确性，使用不到10个黑盒API查询）的强大能力。PublicCheck还展示了生成大量囊肿样本以进行指纹化的平滑外观、可行性和效率。



