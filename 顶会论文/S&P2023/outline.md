# 2023 S&P 关注内容
## 联邦学习
1. 3DFed: Adaptive and Extensible Framework for Covert Backdoor Attack in Federated Learning<br>
Federated Learning（FL）是一种分布式机器学习模式，它在个体设备上本地训练数据集，但容易受到后门模型污染攻击。攻击者可以通过破坏或冒充这些设备，上传制作的恶意模型更新来操纵全局模型，在攻击者指定的触发器下实现后门行为。然而，现有的后门攻击需要更多的关于受害FL系统的信息，而且它们通常是专门为优化单一目标而设计的，这在现代FL系统中变得无效，因为这些系统往往采用深度防御来从不同角度检测后门模型。受到这些问题的启发，本文提出了3DFed，这是一个自适应、可扩展和多层次的框架，用于在黑盒设置下启动隐蔽的FL后门攻击。3DFed具有三个避免模块，可以伪装后门模型：约束损失的后门训练、噪声掩蔽和诱饵模型。通过将指示器植入后门模型中，3DFed可以从全局模型中获取先前时期的攻击反馈，并动态调整这些后门避免模块的超参数。通过广泛的实验结果，我们展示了当它的所有组件一起工作时，3DFed可以逃避所有最先进的FL后门防御，包括Deepsight、Foolsgold、FLAME、FL-Detector和RFLBAT。未来还可以将新的避免模块纳入3DFed中，因为它是一个可扩展的框架。**联邦学习自适应攻击框架**

2. ADI: Adversarial Dominating Inputs in Vertical Federated Learning Systems<br>
垂直联邦学习（Vertical Federated Learning，VFL）系统近来成为一个突出的概念，用于处理分布在许多个体源中的数据，而无需将其集中。多个参与者以隐私意识的方式协作训练基于他们本地数据的模型。迄今为止，VFL已成为在组织之间安全学习模型的事实解决方案，允许共享知识而不损害任何个体的隐私。尽管VFL系统的发展蓬勃发展，但我们发现参与者的某些输入，即敌对控制输入（Adversarial Dominating Inputs，ADIs），可以支配联合推理，朝着敌对方的意愿方向，并迫使其他（受害）参与者做出微不足道的贡献，失去通常在联邦学习场景中提供的奖励。我们通过首先证明典型VFL系统中存在ADIs来对ADIs进行系统研究。然后，我们提出基于梯度的方法来合成各种格式的ADIs，并利用常见的VFL系统。我们进一步启动灰盒模糊测试，由“受害”参与者的显著性分数指导，扰动敌对控制的输入，并以隐私保护的方式系统地探索VFL攻击面。我们对合成ADIs的关键参数和设置的影响进行深入研究。我们的研究揭示了新的VFL攻击机会，促进在出现漏洞之前识别未知威胁并构建更安全的VFL系统。
**VFL安全问题，对抗样本攻击**

4. BayBFed: Bayesian Backdoor Defense for Federated Learning<br>
联邦学习（FL）是一种新兴技术，允许参与者在不与他人共享私有数据的情况下共同训练机器学习模型。然而，FL易受污染攻击（如后门攻击）的影响。因此，最近提出了各种防御措施，这些措施主要利用全局模型的中间状态（即logit）或本地模型与全局模型的距离（即L2范数）来检测FL中的恶意后门。然而，由于这些方法直接操作客户端更新（或权重），它们的有效性取决于客户端数据分布或对手的攻击策略等因素。本文介绍了一种新型、更通用的后门防御框架BayBFed，该框架建议利用客户端更新的概率分布来检测FL中的恶意更新：BayBFed计算客户端更新的概率度量，以跟踪更新中的任何调整，并使用一种新颖的检测算法，可以利用这个概率度量来有效地检测和过滤出恶意更新。因此，它克服了以前方法的缺点，这些缺点由于直接使用客户端更新而引起；尽管如此，我们的概率度量将包括本地客户端训练策略的所有方面。BayBFed利用了两个贝叶斯非参数（BNP）扩展：（i）层次贝塔伯努利过程，用于给出客户端更新的概率度量，以及（ii）中国餐馆过程（CRP）的一种改进，我们称之为CRP-Jensen，它利用这个概率度量来检测和过滤出恶意更新。我们在五个基准数据集上广泛评估了我们的防御方法：CIFAR10、Reddit、IoT入侵检测、MNIST和FMNIST，并展示了它可以有效地检测和消除FL中的恶意更新，而不会恶化全局模型的良好性能。
**联邦学习后门防御**

5. ELSA: Secure Aggregation for Federated Learning with Malicious Actors<br>
Federated Learning（FL）是一种在训练数据集高度分布的情况下进行机器学习（ML）的越来越流行的方法。客户端在其数据集上进行本地训练，然后将更新聚合到全局模型中。现有的聚合协议要么效率低下，要么不考虑系统中恶意行为者的情况。这是使FL成为隐私敏感的ML应用程序的理想解决方案的主要障碍。我们提出了ELSA，一种安全的FL聚合协议，它打破了这个障碍——它高效并且在设计的核心处考虑了恶意行为者的存在。与Prio和Prio+的先前工作类似，ELSA提供了一种新颖的安全聚合协议，由两个服务器上的分布式信任组成，只要一个服务器是诚实的，就可以保持单个客户端更新的私密性，防御恶意客户端，并且是从头到尾高效的。与之前的工作相比，ELSA的区别主题在于，客户端充当这些相关性的不可信经销商，而不会影响协议的安全性，而不是服务器交互地生成加密相关性。这导致了一个更快的协议，同时实现了比先前工作更强的安全性。我们引入了新技术，即使服务器是恶意的，也可以以很小的额外成本（比半诚实服务器的情况下增加的通信还要少）保留隐私。我们的工作大大提高了具有类似安全保证的先前工作的端到端运行时间——对于我们考虑的模型，单个聚合器RoFL的提高高达305倍，分布式信任Prio的提高高达8倍。
**联邦学习隐私强化，恶意服务器也可以保证隐私，效率保证**

6. FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information<br>
联邦学习容易遭受中毒攻击，即恶意客户端通过向服务器发送恶意模型更新来污染全局模型。现有的防御方法主要集中在通过强健的联邦学习方法防止少数恶意客户端污染全局模型，以及在存在大量恶意客户端时检测恶意客户端。然而，如何在检测到恶意客户端后从中毒攻击中恢复全局模型仍然是一个开放性挑战。一个朴素的解决方法是移除检测到的恶意客户端，并使用剩余客户端重新从头开始训练全局模型。然而，这种从头开始训练的恢复方法会产生大量的计算和通信成本，这对于资源受限的客户端如智能手机和物联网设备可能是无法承受的。在本文中，我们提出了FedRecover，一种可以在客户端承受的小计算和通信成本下从中毒攻击中恢复精确全局模型的方法。我们的关键思想是服务器估计客户端的模型更新，而不是在恢复过程中要求客户端计算和通信这些更新。具体来说，在训练被污染的全局模型之前，服务器存储历史信息，包括每轮的全局模型和客户端的模型更新。在恢复过程中，服务器使用其存储的历史信息估计每轮客户端的模型更新。此外，我们进一步优化了FedRecover，通过预热、定期校正、异常修复和最终调整策略来恢复更精确的全局模型，其中服务器要求客户端计算和通信其准确的模型更新。从理论上讲，我们证明了在某些假设下，FedRecover恢复的全局模型接近或与从头开始训练恢复的全局模型相同。从实证角度来看，我们在四个数据集、三种联邦学习方法以及无目标和有目标中毒攻击（例如后门攻击）上的评估表明，FedRecover既准确又高效。
**联邦学习检测后恢复数据，类似遗忘学习**

7. Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning<br>
本文介绍了Flamingo，这是一个用于在大量客户端之间进行安全聚合数据的系统。在安全聚合中，服务器将客户端的私有输入相加，获得结果，而不了解除最终总和之外的任何个体输入信息。Flamingo专注于联邦学习中发现的多轮设置，其中执行多个连续的模型权重求和（平均）以获得良好的模型。以前的协议，例如Bell等人（CCS’20）设计了单轮协议，并通过多次重复协议来适应联邦学习设置。Flamingo消除了以前协议每轮设置的需要，并具有新的轻量级丢弃容错协议，以确保如果客户端在求和过程中离开，则服务器仍然可以获得有意义的结果。此外，Flamingo引入了一种新的本地选择所谓的客户端邻域的方法，这是由Bell等人引入的。这些技术帮助Flamingo减少了客户端与服务器之间的交互次数，从而显着减少了完整训练会话的端到端运行时间。我们实现和评估了Flamingo，并展示它可以在（扩展）MNIST和CIFAR-100数据集上安全地训练神经网络，与非私有联邦学习系统相比，模型收敛而不会损失准确性。
**联邦学习安全聚合，轻量级容错，客户端邻域交互降低通信**

9. RoFL: Robustness of Secure Federated Learning<br>
近年来，尽管有许多攻击暴露了联邦学习(Federated Learning, FL)中的严重漏洞，但仍缺乏对这些攻击的启示和如何有效缓解的全面了解。在本研究中，我们揭示了现有（有针对性的）攻击的内部机理，提供了新的洞察力，解释了为什么这些攻击是可能的，以及为什么FL的稳健性的最终解决方案很具有挑战性。我们展示了机器学习算法需要记忆尾部数据对FL完整性的重要影响。这种现象在隐私的语境下已经被广泛研究，我们的分析揭示了其对机器学习完整性的影响。我们展示了某些类别的严重攻击可以通过强制约束客户端更新的范数边界等方式得到有效缓解。我们探讨如何在单服务器设置中高效地将这些约束纳入安全FL协议中。基于此，我们提出了RoFL，一种新的安全FL系统，它通过隐私保护输入验证扩展了安全聚合。具体而言，RoFL可以在高维加密模型更新中强制执行L_2和L_∞边界等约束。
**联邦学习隐私攻击本质，通过约束范数抵御攻击**

## 人工智能&隐私保护
1. A Theory to Instruct Differentially-Private Learning via Clipping Bias Reduction<br>
我们研究了在差分隐私随机梯度下降（DP-SGD）中引入的偏差，其中采用了裁剪或归一化的每个样本梯度。作为确保有界敏感性的最流行但人为操作之一，梯度裁剪使得许多迭代优化方法的复合隐私分析成为可能，而无需对学习模型或输入数据做出额外的假设。尽管其适用性广泛，但梯度裁剪在系统地指导隐私或效用的改善方面也存在理论上的挑战。一般来说，在没有对全局梯度进行有界假设的情况下，经典的收敛分析不适用于裁剪的梯度下降。此外，由于对效用损失的理解有限，许多现有的DP-SGD改进方法都是启发式的，特别是在私有深度学习的应用中。在本文中，我们提供了有意义的理论分析，并通过详尽的实验结果验证了DP-SGD。我们指出，在先前的工作中低估了由梯度裁剪引起的偏差。对于通过DP-SGD进行通用非凸优化，我们展示了一个导致偏差的关键因素是要裁剪的随机梯度的采样噪声。因此，我们利用所开发的理论从各个角度建立了一系列减少采样噪声的改进。从优化角度出发，我们研究了方差减少技术，并提出了内外动量。在学习模型（神经网络）层面上，我们提出了几种技巧来增强网络内部归一化和批量裁剪，以仔细裁剪一批样本的梯度。对于数据预处理，我们提供了最近提出的通过数据归一化和（自我）增强的改进的理论证明。将这些系统改进结合起来，通过DP-SGD进行私有深度学习在许多任务中可以得到显着加强。例如，在计算机视觉应用中，我们使用（epsilon=8，delta=10^-5）的DP保证，成功地在CIFAR10和SVHN上训练了ResNet20，测试精度分别为76.0％和90.1％；在自然语言处理方面，使用（epsilon=4，delta=10^-5），我们成功地在IMDb数据上训练了一个循环神经网络，测试精度为77.5％。

2. Accuracy-Privacy Trade-off in Deep Ensemble: A Membership Inference Perspective<br>
深度集成学习已经被证明通过训练多个神经网络并平均它们的输出来提高准确性。集成学习也被建议用于防御破坏隐私的成员推断攻击。在本文中，我们实证地展示了深度集成中准确性和隐私（以成员推断攻击为代价）之间的权衡。使用各种数据集和模型架构，我们展示了集成提高准确性时成员推断攻击的有效性增加的情况。我们分析了深度集成中各种因素的影响，并展示了权衡的根本原因。然后，我们评估了基于正则化和差分隐私的常见成员推断攻击防御措施。我们展示了这些防御措施虽然可以减轻成员推断攻击的有效性，但同时也会降低集成的准确性。我们展示了在更先进和最先进的集成技术中也存在类似的权衡，例如快照集成和多样化集成网络。最后，我们提出了一种简单而有效的深度集成防御措施来打破这种权衡，从而同时提高准确性和隐私。

3. Analyzing Leakage of Personally Identifiable Information in Language Models<br>
语言模型（LMs）已被证明通过句子级成员推断和重构攻击泄漏有关训练数据的信息。了解LMs泄漏个人身份信息（PII）的风险受到较少关注，这可以归因于虚假的假设，即数据集清理技术如擦洗足以防止PII泄漏。擦洗技术减少但不能防止PII泄漏的风险：在实践中，擦洗是不完美的，必须权衡最小化披露和保留数据集的效用之间的平衡。另一方面，不清楚算法防御，如差分隐私，旨在保证句子或用户级隐私，能够防止PII披露的程度。在这项工作中，我们引入了三种PII泄漏类型的严格基于游戏的定义，通过只使用LM的API访问进行黑盒提取、推断和重构攻击。我们在三个领域中评估了带有和不带有防御的GPT-2模型的攻击：案例法、医疗保健和电子邮件。我们的主要贡献是：（i）新颖的攻击可以提取多达10倍的PII序列，比现有攻击更有效；（ii）显示句子级差分隐私减少了PII披露的风险，但仍泄漏约3%的PII序列；（iii）记录级成员推断和PII重构之间的微妙联系。

4. Continual Observation under User-level Differential Privacy<br>
Dwork等人在差分隐私的持续观察方面的基础工作中提出了两种隐私模型：事件级别的DP和用户级别的DP。后者提供了更强的隐私保护，因为它允许用户贡献任意数量的项目。在事件级别DP下，他们的机制与所有保持联合的函数的静态设置下的最优效用界相匹配，直到对数多项式因子。不幸的是，与事件级别DP的强结果相比，他们的用户级别DP机制具有较弱的效用保证，并对数据施加许多限制。在本文中，我们采用特定实例的方法，设计了在用户级别DP下多个基本函数的持续观测机制。我们的机制不需要任何先验数据限制，同时提供了随着数据难度的增加而逐渐降低的效用保证。对于计数和求和函数，我们的机制是下邻域最优的，与静态设置相匹配，直到对数多项式因子。对于其他函数，它们不匹配静态情况，但我们证明这是不可避免的，这是差分隐私下持续观察的第一个分离结果。

5. D-DAE: Defense-Penetrating Model Extraction Attacks<br>
最近的研究表明，机器学习模型容易受到模型提取攻击的威胁，攻击者可以通过查询受害模型来构建一个几乎达到相同性能的替代模型。为了防止这种攻击，一系列方法已被提出，以在返回结果之前破坏查询结果，大大降低现有模型提取攻击的性能。在本文中，我们首次尝试开发一种防御-渗透模型提取攻击框架，命名为D-DAE，旨在打破基于破坏的防御。D-DAE的关键是设计两个模块，即破坏检测和破坏恢复，它们可以与通用模型提取攻击集成。更具体地说，在从受害模型获取查询结果之后，破坏检测模块推断出捍卫者采用的防御机制。我们设计了一种基于元学习的破坏检测算法，用于学习破坏和未破坏查询结果分布之间的基本差异。即使我们无法访问受害模型的原始训练数据集，该算法也具有很好的泛化性质。在检测到防御机制后，破坏恢复模块试图使用精心设计的生成模型从破坏的查询结果中恢复出干净的查询结果。我们在MNIST、FashionMNIST、CIFAR-10、GTSRB和ImageNette数据集上进行了广泛的评估，结果表明，在面对4种最先进的防御措施和多种防御措施组合时，D-DAE可以将现有模型提取攻击的替代模型准确率提高高达82.24%。我们还验证了D-DAE在穿透微软Azure和Face++托管的真实世界API中未知防御方面的有效性。

6. On the (In)security of Peer-to-Peer Decentralized Machine Learning<br>
在这项工作中，我们进行了首次深入的隐私分析，研究了分布式学习——一种协作机器学习框架，旨在解决联邦学习的主要限制。我们引入了一套新的攻击方式，包括被动和主动分布式对手。我们证明，与分散学习提议者所声称的相反，分散学习并没有提供比联邦学习更多的安全优势。相反，它增加了攻击面，使系统中的任何用户都能够执行隐私攻击，如梯度反演，甚至获得对诚实用户的本地模型的完全控制。我们还表明，鉴于现有保护技术，隐私保护配置的分散学习需要全连接网络，失去了与联邦设置相比的任何实际优势，因此完全打败了分散方法的目标。

7. Private, Efficient, and Accurate: Protecting Models Trained by Multi-party Learning with Differential Privacy<br>
安全多方计算机器学习，简称多方学习（MPL），已成为利用保护隐私的多方数据的重要技术。虽然MPL为计算过程提供了严格的安全保障，但由MPL训练的模型仍然容易受到仅依赖于对模型的访问的攻击。差分隐私可以帮助抵御这种攻击，但差分隐私带来的精度损失和安全多方计算协议的巨大通信开销使得在隐私、效率和准确性之间平衡三方面非常具有挑战性。本文旨在通过提出一个解决方案（称为PEA，私有、高效、准确），解决上述问题。该方案包括一个安全差分隐私随机梯度下降（DPSGD）协议和两种优化方法。首先，我们提出一个安全的DPSGD协议，以在基于秘密共享的MPL框架中执行DPSGD，该算法是一种流行的差分隐私机器学习算法。其次，为了降低差分隐私噪声引起的精度损失和MPL的巨大通信开销，我们提出了两种优化方法来训练MPL模型：（1）数据无关特征提取方法，旨在简化训练模型结构；（2）基于本地数据的全局模型初始化方法，旨在加速模型训练的收敛。我们在两个开源MPL框架TF-Encrypted和Queqiao中实现了PEA。在各种数据集上的实验结果证明了PEA的效率和有效性。例如，当ε=2时，我们可以在LAN环境下，在7分钟内训练出一个准确率为88％的差分隐私分类模型，用于CIFAR-10。该结果显著优于CryptGPU的结果，后者是一种最先进的MPL框架：使用相同的准确性，在CIFAR-10上训练一个非私有深度神经网络模型需要耗费超过16小时。

8. SNAP: Efficient Extraction of Private Properties with Poisoning<br>
财产推论攻击允许攻击者从机器学习模型中提取训练数据集的全局属性。这种攻击对于共享数据集以训练机器学习模型的数据所有者具有隐私影响。已经提出了几种现有的用于对深度神经网络进行财产推论攻击的方法，但它们都依赖于攻击者训练大量的阴影模型，这会导致大量的计算开销。在本文中，我们考虑了攻击者可以污染一部分训练数据集并查询训练目标模型的财产推论攻击的设置。在我们的模型置信度污染理论分析的基础上，我们设计了一种高效的财产推论攻击SNAP，它获得了比Mahloujifar等人的最新基于污染的财产推论攻击更高的攻击成功率，并且需要更少的污染量。例如，在人口普查数据集上，SNAP的成功率比Mahloujifar等人高34％，同时速度快了56.5倍。我们还将我们的攻击扩展到推断在训练期间是否存在某个属性，并有效地估计感兴趣属性的确切比例。我们评估了来自四个数据集的多种比例属性的攻击，并展示了SNAP的通用性和有效性。

9. Spectral-DP: Differentially Private Deep Learning through Spectral Perturbation and Filtering<br>
差分隐私是深度学习算法中广泛接受的隐私度量标准，实现差分隐私依赖于一种称为差分隐私随机梯度下降（DP-SGD）的噪声训练方法。DP-SGD需要对密集神经网络中的每个梯度进行直接噪声添加，隐私在显著的效用成本下得到保证。在这项工作中，我们提出了Spectral-DP，一种新的差分隐私学习方法，它将谱域梯度扰动与谱滤波相结合，以更低的噪声尺度和更好的效用实现所需的隐私保证。我们基于Spectral-DP开发了差分隐私深度学习方法，用于包含卷积和全连接层的架构。特别是对于全连接层，我们将基于块循环的空间重构与Spectral-DP相结合，以实现更好的效用。通过全面的实验，我们研究并提供了在基准数据集上实现Spectral-DP深度学习的指南。与最先进的基于DP-SGD的方法相比，Spectral-DP在从头开始训练和迁移学习设置中表现出均匀更好的效用性能。

## 人工智能&安全威胁
1. AI-Guardian: Defeating Adversarial Attacks using Backdoors<br>
深度神经网络（DNNs）由于其越来越高的准确性已被广泛应用于许多领域。然而，它们也容易受到对抗性攻击，对于自动驾驶、远程诊断等安全关键应用构成严重威胁。现有解决方案在检测/预防此类攻击方面存在局限性，也会影响原始任务的性能。本文提出了AI-Guardian，一种新型方法，利用有意嵌入的后门来失败对抗性扰动，同时保持原始主任务的性能。我们使用五种流行的对抗性样例生成方法对AI-Guardian进行了广泛评估，实验结果表明其在打败对抗性攻击方面的有效性。具体来说，AI-Guardian将攻击成功率从97.3%降低到3.2%，超过最先进的工作30.9%，仅在干净数据准确率下降0.9%。此外，AI-Guardian在模型预测时间方面只引入了0.36%的开销，在大多数情况下几乎可以忽略不计。

2. Breaking Security-Critical Voice Authentication<br>
语音认证（VA）最近已成为许多安全关键操作的不可或缺的组成部分，例如银行交易和呼叫中心对话。自动说话者验证系统（ASVs）对欺骗攻击的脆弱性促进了对抗措施（CMs）的开发，其任务是区分真实和欺骗性言语。ASVs和CMs一起形成了今天的VA系统，并被宣传为不可攻破的访问控制机制。我们开发了第一种实际攻击欺骗对策的方法，并展示了恶意行为者如何有效地针对这些防御措施制作音频样本。以前针对VA的对抗性攻击主要是针对白盒场景设计的，这种场景假设了对系统内部的了解，或需要大量的查询和时间预算来发起针对特定目标的攻击。在攻击安全关键系统时，这些假设不成立。另一方面，我们的攻击针对所有欺骗对策共享的常见失败点，使得它实时，模型不可知，并且完全是黑盒，无需与目标交互来制作攻击样本。我们工作的关键信息是，CMs错误地学习区分基于易于识别和伪造的线索的欺骗和真实音频。我们的攻击效果足够微妙，可以保证这些对抗样本仍然可以绕过ASV并保留其原始文本内容。这些属性组合起来形成了一个强大的攻击，可以绕过安全关键的VA，在其最严格的形式下产生高达99％的成功率，仅需要6次尝试。最后，我们对CMs进行了第一个针对电话网络的有针对性攻击，绕过了几个已知的挑战，为各种潜在威胁提供了可能，考虑到语音生物识别在呼叫中心中的增加使用。我们的结果质疑了现代VA系统的安全性，并敦促用户重新考虑对它们的信任，考虑到攻击者绕过这些措施以获得他们最有价值的资源的真实威胁。

3. Deepfake Text Detection: Limitations and Opportunities<br>
近年来，语言生成模型的进步使得人们可以创建逼真的合成文本或深度伪造文本。以往的研究已经表明，深度伪造文本的滥用可能会误导内容消费者。因此，深度伪造文本检测，即区分人类生成和机器生成文本的任务变得越来越关键。已经提出了几种深度伪造文本检测的防御方法。然而，我们缺乏对它们在现实世界中适用性的全面理解。在本文中，我们从4个基于Transformer的工具驱动的在线服务中收集深度伪造文本，以评估防御措施在野外内容上的泛化能力。我们开发了几种低成本的对抗攻击，并研究了现有防御措施对自适应攻击者的鲁棒性。我们发现，许多防御措施在我们的评估场景下与其原始声称的性能相比，表现出显著的性能下降。我们的评估表明，利用文本内容中的语义信息是提高深度伪造文本检测方案的鲁棒性和泛化性能的有前途的方法。

4. DepthFake: Spoofing 3D Face Authentication with a 2D Photo<br>
人脸识别已被广泛应用于门禁控制，最新的3D人脸识别系统采用3D活体检测技术来应对照片重放攻击，即攻击者使用2D照片来绕过身份验证。本文分析了利用结构光深度摄像机的3D活体检测系统的安全性，发现了一种针对3D人脸识别系统的新的攻击方式。我们提出了DepthFake攻击，可以仅使用一张2D照片欺骗3D人脸识别。为了实现这一目标，DepthFake首先从目标受害者的2D照片中估算出其面部的3D深度信息。然后，DepthFake投射嵌入面部深度信息的精心制作的散射图案，以赋予2D照片3D认证属性。我们克服了一系列实际挑战，例如从2D照片中的深度估算误差，基于结构光的深度图像伪造，为面部对齐RGB图像和深度图像，并在实验室环境中实现了DepthFake。我们在三个商业化人脸识别系统（即腾讯云、百度云和3DiVi）和一个商业化门禁设备上验证了DepthFake。50个用户的结果表明，DepthFake在现实世界中的深度攻击成功率为79.4％，RGB-D攻击成功率为59.4％。

5. Disguising Attacks with Explanation-Aware Backdoors<br>
可解释的机器学习在分析和理解基于学习的系统方面具有巨大潜力。然而，这些方法可以被操纵以提供不可靠的解释，从而产生强大而隐蔽的对手。在本文中，我们展示了如何完全掩盖机器学习模型的对抗操作。与神经后门类似，我们在触发器存在时修改模型的预测，但同时欺骗了后续进行分析的解释方法。这使得对手可以隐藏触发器的存在，或者将解释指向输入的完全不同部分，引入一个红鱼。我们对图像领域中的梯度和传播解释方法的这些解释感知后门的不同表现进行了分析，然后继续对恶意软件分类进行了红鱼攻击。

6. ImU: Physical Impersonating Attack for Face Recognition System with Natural Style Changes<br>
本文介绍了一种新的针对人脸识别系统的物理冒充攻击。其旨在在不同条件和姿势下生成攻击者多张照片的一致风格变化。此外，这些风格变化需要能够通过化妆来实现，并能够诱导所需的错误分类。为了实现这一目标，我们开发了新的技术，将同一物理人的多张照片嵌入到StyleGAN潜空间中的向量中，使得嵌入的潜空间向量具有一些隐式相关性，以便于寻找一致的风格变化。我们的数字和物理评估结果表明，我们的方法可以允许外部攻击者成功冒充内部人员，并进行一致自然的变化。

7. Jigsaw Puzzle: Selective Backdoor Attack to Subvert Malware Classifiers<br>
恶意软件分类器在训练过程中容易受到利用，因为需要定期重新训练，使用从野外收集的样本。最近的研究已经证明了针对恶意软件分类器的后门攻击的可行性，但此类攻击的隐蔽性尚不明确。本文重点研究Android恶意软件分类器，在清洁标签设置下（即攻击者无法完全控制训练过程或毒化数据的标签），研究后门攻击。实证上，我们表明现有的针对恶意软件分类器的后门攻击仍然可以被最近的防御措施（如MNTD）检测到。为了提高隐蔽性，我们提出了一种新的攻击方法，拼图游戏（JP），基于这样一个关键观察：恶意软件作者几乎没有动机去保护其他作者的恶意软件，只会保护自己的恶意软件。因此，拼图游戏学习一种触发器来补充恶意软件作者样本的潜在模式，并仅在触发器和潜在模式在样本中被拼接在一起时激活后门。我们进一步关注使用从良性软件广泛收集的字节码小工具在问题空间中（例如软件代码）实现触发器。我们的评估证实，拼图游戏作为后门是有效的，仍然隐蔽于最先进的防御措施之下，并且是在从仅考虑特征空间攻击的情况下推理出的现实设置中的威胁。最后，我们探讨了改进后门防御的有前途的方法。

8. MagBackdoor: Beware of Your Loudspeaker as a Backdoor for Magnetic Injection Attacks<br>
这篇论文介绍了MagBackdoor，它是第一个通过音频系统的扬声器后门注入恶意命令的磁场攻击，可以破坏连接的语音交互系统。MagBackdoor专注于扬声器的磁性威胁，并偷偷地操纵它们的声音产生。因此，由于内部音频系统的密集排列，麦克风不可避免地会捕捉到受攻击扬声器生成的恶意声音。为了证明MagBackdoor的可行性，我们进行了全面的模拟和实验。该研究还对外部磁场激发扬声器声音产生的机制进行了建模，为MagBackdoor提供了理论指导。为了针对真实场景中的隐秘磁场攻击，我们自行设计了一个原型，可以发射由语音命令调制的磁场。我们实施了MagBackdoor，并在涉及16部智能手机、四台笔记本电脑、两台平板电脑和三台智能扬声器的各种智能设备上进行了评估，实现了平均95%的注入成功率，注入的声学信号质量高。

9. ObjectSeeker: Certifiably Robust Object Detection against Patch Hiding Attacks via Patch-agnostic Masking<br>
物体检测器被广泛应用于安全关键系统，如自动驾驶车辆，在面对补丁隐藏攻击时被发现存在漏洞。攻击者可以使用一个物理可实现的对抗性补丁来使物体检测器错过目标物体的检测，破坏物体检测应用的功能。本文中，我们提出了ObjectSeeker，以对抗补丁隐藏攻击进行可证明的强健物体检测。ObjectSeeker的关键洞见是补丁不可知掩蔽：我们的目标是在不知道补丁的形状、大小和位置的情况下，遮盖整个对抗性补丁。这种掩蔽操作可以中和对抗性效果，使任何基本的物体检测器能够安全地检测被遮盖的图像上的物体。值得注意的是，我们可以以可证明的方式评估ObjectSeeker的强健性：我们开发了一种认证过程，以正式确定ObjectSeeker是否能够检测某些物体，并在威胁模型内对抗任何白盒自适应攻击，实现可证明的强健性。我们的实验证明，在可证明的强健性方面，ObjectSeeker相对于之前的研究实现了显著的（约10%-40%的绝对值和约2-6倍的相对值）提升，同时具有高的清洁性能（与未防御模型相比下降约1%）。

10. On The Empirical Effectiveness of Unrealistic Adversarial Hardening Against Realistic Adversarial Attacks<br>
最近的研究引起了人们对现实世界中逼真敌对攻击的关注，这一领域的研究往往关注于机器学习系统的安全攻击和防御，但主要集中在不切实际的敌对样本上。我们的论文为更好地理解现实攻击下的敌对鲁棒性打下了基础，并做出了两个主要贡献。首先，我们对三个现实世界的用例（文本分类、僵尸网络检测、恶意软件检测）和七个数据集进行了研究，以评估不切实际的敌对样本是否能够用于保护模型免受现实世界中的攻击。我们的结果揭示了不同用例之间的差异，其中不切实际的样本可能与逼真的样本一样有效，或者只能提供有限的改进。其次，为了解释这些结果，我们分析了用现实和不切实际的攻击生成的敌对样本的潜在表征。我们揭示了区分哪些不切实际的样本可以用于有效硬化的模式。我们发布了我们的代码、数据集和模型，以支持未来探索如何缩小不切实际和现实敌对攻击之间差距的研究。

11. PublicCheck: Public Integrity Verification for Services of Run-time Deep Models<br>
现有的深度模型完整性验证方法都是针对私有验证设计的（即假设服务提供商是诚实的，具有白盒访问模型参数）。然而，私有验证方法不允许模型用户在运行时验证模型。相反，他们必须信任服务提供商，而服务提供商可能会篡改验证结果。相比之下，考虑到不诚实的服务提供商可能存在的公共验证方法可以使更广泛的用户受益。在本文中，我们提出了PublicCheck，一种实用的公共完整性验证解决方案，适用于运行时深度模型的服务。PublicCheck考虑了不诚实的服务提供商，克服了轻量级、提供防伪保护和具有平滑外观的指纹样本的公共验证挑战。为了捕捉和指纹化运行时模型的内在预测行为，PublicCheck生成了平滑转换和增强的囊肿样本，这些样本围绕模型的决策边界封闭，同时确保验证查询与正常查询不可区分。PublicCheck在目标模型的知识受限的情况下（例如没有梯度或模型参数的知识）也适用。PublicCheck的彻底评估展示了模型完整性违规检测（对各种模型完整性攻击和模型压缩攻击的100％检测准确性，使用不到10个黑盒API查询）的强大能力。PublicCheck还展示了生成大量囊肿样本以进行指纹化的平滑外观、可行性和效率。

12. RAB: Provable Robustness Against Backdoor Attacks<br>
最近的研究表明，深度神经网络（DNN）容易受到对抗攻击，包括规避和后门（毒化）攻击。在防御方面，已经有大量的工作致力于提高对规避攻击的经验和可证明的鲁棒性；然而，对于后门攻击的可证明鲁棒性仍然很少被探究。在本文中，我们专注于通过随机平滑技术证明机器学习模型对于一般威胁模型的鲁棒性，特别是后门攻击。我们首先提供了一个统一的框架，并展示如何将其具体化以证明对于规避攻击和后门攻击的鲁棒性。然后，我们提出了第一个稳健训练过程RAB，以平滑训练过的模型，并证明其对于后门攻击的鲁棒性。我们在理论上证明了使用RAB训练的机器学习模型的鲁棒性界限，并证明了我们的鲁棒性界限是紧密的。此外，我们在理论上表明，对于简单模型如K近邻分类器，可以有效地训练鲁棒平滑模型，并提出了一种精确平滑训练算法，消除了对于这种模型从噪声分布中采样的需要。在实证方面，我们对MNIST、CIFAR-10和ImageNette数据集上的不同机器学习（ML）模型，如DNN、支持向量机和K-NN模型进行了全面的实验，并提供了第一个针对后门攻击的认证鲁棒性基准。此外，我们还在一个垃圾邮件数据集上评估了K-NN模型，以展示所提出的精确算法的优势。理论分析和对不同ML模型和数据集的全面评估，为进一步针对一般训练时间攻击的鲁棒学习策略提供了启示。

13. REDEEM MYSELF: Purifying Backdoors in Deep Learning Models using Self Attention Distillation<br>
最近的研究揭示了深度神经网络对于后门攻击的易受攻击性，其中后门模型在触发器的激活下会进行有针对性或无针对性的错误分类。一系列净化方法（如精细修剪、神经关注转移、MCR [69]）已被提出以消除模型中的后门。然而，它们要么无法降低更高级后门攻击的攻击成功率，要么大大降低模型对于干净样本的预测能力。在本文中，我们提出了一种新的净化防御框架，称为 SAGE，它利用自我关注蒸馏来净化模型中的后门。与传统的关注转移机制需要教师模型来监督蒸馏过程不同，SAGE可以利用少量干净样本实现自我净化。为了增强防御性能，我们进一步提出了一种动态学习率调整策略，仔细跟踪干净样本的预测准确性来指导学习率调整。我们将SAGE的防御性能与6种最先进的防御方法在4个数据集上对抗8种后门攻击进行比较。结果表明，SAGE可以将攻击成功率降低多达90％，同时对于干净样本的预测准确性减少不到3％。我们将在出版时开源我们的代码。

14. Selective Amnesia: On Efficient, High-Fidelity and Blind Suppression of Backdoor Effects in Trojaned Machine Learning Models<br>
深度神经网络（DNN）的广泛应用以及其越来越复杂的结构和供应链使后门攻击的风险变得比以往任何时候都更加现实。在这种攻击中，对手通过污染DNN模型的训练数据或操纵其训练过程，秘密注入一个隐蔽的后门任务，同时与主要任务一起，以便有策略地误分类携带触发器的输入。在干净的数据上取消后门模型的学习是困难的。防御这种攻击，特别是从受感染的模型中消除后门效应，被认为是困难的。为此，既需要先前的研究来恢复触发器（这很难实现），也需要尝试在其主要任务上微调模型，但当干净数据很少时，这种方法的效果会变差。在本文中，我们提出了一种简单但出乎意料地有效的技术，以在受感染的模型上诱导“选择性遗忘”。我们的方法称为SEAM，受到了连续学习中长期存在的灾难性遗忘（CF）问题的启发。我们的想法是在随机标记的干净数据上重新训练给定的DNN模型，以诱导CF，导致主要和后门任务的突然遗忘；然后我们通过在正确标记的干净数据上重新训练随机模型来恢复主要任务。我们通过将遗忘过程建模为连续学习并使用神经切向核来近似DNN以量化CF，对SEAM进行了分析。我们的分析表明，我们的随机标记方法实际上在触发输入不存在的情况下最大化了未知后门中的CF，并且还保留了网络中的一些特征提取，以启用主要任务的快速恢复。我们还在图像处理和自然语言处理任务中对SEAM进行了评估，在数据污染和训练操纵攻击下，在数千个模型上进行了测试，这些模型要么是在流行的图像数据集上训练的，要么是由TrojAI竞赛提供的。我们的实验表明，SEAM远远优于最先进的取消学习技术，在几分钟内就实现了高保真度（衡量主要任务精度与后门精度之间的差距），而且速度快了约30倍（使用MNIST数据集从头训练模型），只需要少量干净数据（TrojAI模型的训练数据的0.1%）。

15. SoK: Certified Robustness for Deep Neural Networks<br>
深度神经网络（DNN）的巨大进步已经导致在各种任务上达到了最先进的性能。然而，最近的研究表明DNN容易受到对抗性攻击，这在将这些模型部署到自动驾驶等安全关键应用程序时引起了极大的关注。针对对抗性攻击已经提出了不同的防御方法，包括：a）经验防御，这些方法通常可以被再次自适应攻击而不提供强健性认证；b）具有认证鲁棒性的方法，包括提供在某些条件下对任何攻击提供鲁棒性准确性的鲁棒性验证和相应的鲁棒性训练方法。在本文中，我们系统化了具有认证鲁棒性的方法以及相关的实践和理论影响和发现。我们还提供了现有鲁棒性验证和训练方法在不同数据集上的第一个全面基准。具体而言，我们：1）为鲁棒性验证和训练方法提供分类法，并总结代表性算法的方法，2）揭示这些方法的特点、优点、局限性和基本联系，3）讨论DNN具有认证鲁棒性的当前研究进展、理论障碍、主要挑战和未来方向，以及4）提供开源统一平台，评估20多种代表性具有认证鲁棒性的方法。

16. StyleFool: Fooling Video Classification Systems via Style Transfer<br>
视频分类系统容易受到对抗性攻击，这可能会在视频验证中造成严重的安全问题。目前的黑盒攻击需要大量的查询才能成功，导致攻击过程中计算开销很高。另一方面，具有限制扰动的攻击对于去噪或对抗性训练等防御是无效的。在本文中，我们专注于无限制扰动，并提出了StyleFool，一种通过风格转移来欺骗视频分类系统的黑盒视频对抗性攻击。StyleFool首先利用颜色主题接近度来选择最佳的风格图像，这有助于避免风格化视频中的不自然细节。同时，在有目标的攻击中，目标类置信度也被考虑在内，通过将风格化视频向或甚至越过决策边界来影响分类器的输出分布。然后采用无梯度方法进一步优化对抗扰动。我们进行了广泛的实验，评估了StyleFool在两个标准数据集UCF-101和HMDB-51上的表现。实验结果表明，StyleFool在查询数量和对现有防御的鲁棒性方面优于最先进的对抗性攻击。此外，在无目标攻击中，50%的风格化视频不需要任何查询，因为它们已经可以欺骗视频分类模型。此外，我们通过用户研究评估了不可区分性，结果表明，StyleFool的对抗样本在人眼中看起来不可察觉，尽管存在无限制扰动。

17. TrojanModel: A Practical Trojan Attack against Automatic Speech Recognition Systems<br>
虽然深度学习技术在现代数字产品中取得了巨大成功，但研究人员表明深度学习模型容易受到特洛伊攻击。在特洛伊攻击中，攻击者会悄悄地修改深度学习模型，使得当输入中存在特定触发器时，模型会输出预定义的标签。本文提出了一种实用的特洛伊攻击——TrojanModel，针对自动语音识别（ASR）系统。ASR系统旨在将语音输入转录为文本，以便后续下游应用程序更容易处理。我们考虑了一种实用的攻击场景，即攻击者将特洛伊木马插入目标ASR系统的声学模型中。与现有研究使用易引起用户怀疑的噪声触发器不同，本文的研究重点是使用不引起怀疑的声音作为触发器，例如背景中播放的一段音乐。此外，TrojanModel不需要对目标模型进行重新训练。实验结果表明，TrojanModel可以在对目标模型性能几乎没有影响的情况下实现高攻击成功率。我们还演示了攻击在空中攻击场景中的有效性，在该场景中，音频通过物理扬声器播放并被麦克风接收。


